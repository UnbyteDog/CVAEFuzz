#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DBSCANèšç±»å™¨ä¸éšç©ºé—´å¯è§†åŒ–å·¥å…·
=============================

åŸºäºç”Ÿæˆçš„éšç©ºé—´ç‰¹å¾è¿›è¡ŒDBSCANå¯†åº¦èšç±»
å®ç°é™ç»´å¯è§†åŒ–ã€è´¨å¿ƒæå–å’Œç²¾é”è½½è·ç­›é€‰åŠŸèƒ½


"""

import os
import sys
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.neighbors import NearestNeighbors
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import logging
from scipy.spatial.distance import cdist
import argparse

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CVAEClusterer:
    """éšç©ºé—´èšç±»å™¨

    åŸºäºDBSCANç®—æ³•å¯¹ç”Ÿæˆçš„éšç©ºé—´å‘é‡è¿›è¡Œèšç±»åˆ†æ
    æ”¯æŒå¤šç§é™ç»´æ–¹æ³•å’Œç²¾é”è½½è·ç­›é€‰ç­–ç•¥
    """

    def __init__(self, embeddings: np.ndarray, payloads: List[str], metadata: List[Dict],
                 valid_mask: Optional[np.ndarray] = None, label_weight: float = 15.0):
        """åˆå§‹åŒ–èšç±»å™¨

        Args:
            embeddings: [N, latent_dim] éšç©ºé—´ç‰¹å¾çŸ©é˜µ
            payloads: å¯¹åº”çš„è½½è·æ–‡æœ¬åˆ—è¡¨
            metadata: è½½è·å…ƒæ•°æ®åˆ—è¡¨
            valid_mask: [N] æœ‰æ•ˆæ€§æ©ç ï¼Œæ ‡è®°å“ªäº›ç‰¹å¾æ˜¯æœ‰æ•ˆçš„
            label_weight: æ ‡ç­¾æƒé‡å› å­ï¼Œç”¨äºå¢å¼ºä¸åŒç±»å‹è½½è·çš„åˆ†ç¦»åº¦ (å»ºè®®15.0-20.0ï¼Œå¼ºåŠ›ç±»å‹éš”ç¦»)
        """
        self.embeddings = embeddings
        self.payloads = payloads
        self.metadata = metadata
        self.valid_mask = valid_mask if valid_mask is not None else np.ones(len(embeddings), dtype=bool)
        self.label_weight = label_weight

        # ç‰¹å¾å¢å¼ºï¼šæ ‡ç­¾æƒé‡æ³¨å…¥
        self.enhanced_embeddings = self._create_label_enhanced_embeddings()

        # ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
        assert len(embeddings) == len(payloads) == len(metadata), "è¾“å…¥æ•°æ®é•¿åº¦ä¸ä¸€è‡´"
        if valid_mask is not None:
            assert len(valid_mask) == len(embeddings), "æœ‰æ•ˆæ€§æ©ç é•¿åº¦ä¸åŒ¹é…"

        # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬ï¼ˆä½¿ç”¨å¢å¼ºç‰¹å¾ï¼‰
        self.valid_indices = np.where(self.valid_mask)[0]
        self.valid_embeddings = self.enhanced_embeddings[self.valid_mask]  # ä½¿ç”¨å¢å¼ºç‰¹å¾
        self.valid_payloads = [payloads[i] for i in self.valid_indices]
        self.valid_metadata = [metadata[i] for i in self.valid_indices]

        logger.info(f"å¼€å§‹èšç±»å™¨åˆå§‹åŒ–")
        logger.info(f"   æ€»æ ·æœ¬æ•°: {len(embeddings)}")
        logger.info(f"   æœ‰æ•ˆæ ·æœ¬: {len(self.valid_embeddings)}")
        logger.info(f"   éšç©ºé—´ç»´åº¦: {embeddings.shape[1]}")

        # èšç±»ç»“æœ
        self.clustering_results = {}
        self.reduced_embeddings = {}
        self.refined_payloads = []

    def _create_label_enhanced_embeddings(self) -> np.ndarray:
        """åˆ›å»ºæ ‡ç­¾å¢å¼ºçš„ç‰¹å¾å‘é‡

        å°†åŸå§‹32ç»´éšå‘é‡ä¸6ç»´å¸¦æƒé‡çš„One-hotæ ‡ç­¾å‘é‡æ‹¼æ¥ï¼Œ
        å½¢æˆ38ç»´å¤åˆç‰¹å¾å‘é‡ï¼Œå¼ºåˆ¶ä¸åŒç±»å‹è½½è·åœ¨ç©ºé—´ä¸Šåˆ†ç¦»ã€‚

        Returns:
            [N, 38] æ ‡ç­¾å¢å¼ºç‰¹å¾çŸ©é˜µ
        """
        logger.info(f"å¼€å§‹æ ‡ç­¾æƒé‡æ³¨å…¥ (æƒé‡å› å­: {self.label_weight})")

        # ä»metadataä¸­æå–æ ‡ç­¾å¹¶è½¬æ¢ä¸ºOne-hotç¼–ç 
        labels = []
        for meta in self.metadata:
            if 'label' in meta:
                label = int(meta['label'])
            else:
                # å¦‚æœæ²¡æœ‰labelå­—æ®µï¼Œå°è¯•ä»typeå­—æ®µæ¨æ–­
                attack_type = meta.get('type', 'SQLi')
                type_to_label = {
                    'SQLi': 0, 'XSS': 1, 'CMDi': 2,
                    'Overflow': 3, 'XXE': 4, 'SSI': 5
                }
                label = type_to_label.get(attack_type, 0)
            labels.append(label)

        labels = np.array(labels)

        # åˆ›å»ºOne-hotç¼–ç  (6ç»´)
        n_samples = len(labels)
        n_classes = 6
        one_hot_labels = np.zeros((n_samples, n_classes))
        one_hot_labels[np.arange(n_samples), labels] = 1

        # åº”ç”¨æ ‡ç­¾æƒé‡
        weighted_one_hot = one_hot_labels * self.label_weight

        # æ‹¼æ¥åŸå§‹ç‰¹å¾å’ŒåŠ æƒæ ‡ç­¾ç‰¹å¾
        enhanced_embeddings = np.hstack([self.embeddings, weighted_one_hot])

        logger.info(f"   åŸå§‹ç‰¹å¾ç»´åº¦: {self.embeddings.shape[1]}")
        logger.info(f"   æ ‡ç­¾ç‰¹å¾ç»´åº¦: {weighted_one_hot.shape[1]}")
        logger.info(f"   å¢å¼ºåç»´åº¦: {enhanced_embeddings.shape[1]}")

        return enhanced_embeddings

    def find_optimal_eps(self, k: int = 5, method: str = 'knee', embeddings: Optional[np.ndarray] = None) -> float:
        """ä½¿ç”¨K-è·ç¦»å›¾å¯»æ‰¾æœ€ä¼˜epså‚æ•°

        Args:
            k: Kè·ç¦»çš„kå€¼ï¼Œé€šå¸¸è®¾ä¸ºmin_samples-1
            method: å¯»æ‰¾æ–¹æ³• ('knee', 'percentile')
            embeddings: å¯é€‰çš„ç‰¹å¾çŸ©é˜µï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨valid_embeddings

        Returns:
            æœ€ä¼˜çš„epså€¼
        """
        logger.info(f"å¯»æ‰¾æœ€ä¼˜epså‚æ•° (k={k}, method={method})")

        # ä½¿ç”¨æŒ‡å®šçš„ç‰¹å¾çŸ©é˜µæˆ–é»˜è®¤çš„æœ‰æ•ˆç‰¹å¾
        target_embeddings = embeddings if embeddings is not None else self.valid_embeddings

        if len(target_embeddings) < k:
            logger.warning(f"æ ·æœ¬æ•°ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤eps=1.0")
            return 1.0

        # è®¡ç®—k-è·ç¦»
        nbrs = NearestNeighbors(n_neighbors=k+1).fit(target_embeddings)
        distances, _ = nbrs.kneighbors(target_embeddings)
        k_distances = distances[:, k]  # è·ç¦»ç¬¬kä¸ªæœ€è¿‘é‚»å±…çš„è·ç¦»

        # æ’åº
        sorted_distances = np.sort(k_distances)[::-1]

        if method == 'knee':
            # è‚˜éƒ¨æ³•ï¼šå¯»æ‰¾æ›²ç‡æœ€å¤§çš„ç‚¹
            # ç®€åŒ–çš„è‚˜éƒ¨æ£€æµ‹ï¼šå¯»æ‰¾äºŒé˜¶å·®åˆ†æœ€å¤§çš„ç‚¹
            second_diff = np.diff(sorted_distances, 2)
            knee_idx = np.argmax(second_diff) + 1
            optimal_eps = sorted_distances[knee_idx]

        elif method == 'percentile':
            # ç™¾åˆ†ä½æ•°æ³•ï¼šä½¿ç”¨95%åˆ†ä½æ•°
            optimal_eps = np.percentile(sorted_distances, 95)

        else:
            raise ValueError(f"æœªçŸ¥çš„epså¯»æ‰¾æ–¹æ³•: {method}")

        logger.info(f"æœ€ä¼˜epså€¼: {optimal_eps:.4f}")
        return optimal_eps

    def perform_clustering(self, eps: float = None, min_samples: int = None) -> Dict:
        """æ‰§è¡ŒDBSCANèšç±»

        Args:
            eps: é‚»åŸŸåŠå¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨å¯»æ‰¾
            min_samples: æ ¸å¿ƒç‚¹æœ€å°æ ·æœ¬æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™è®¾ä¸º2*latent_dim

        Returns:
            èšç±»ç»“æœå­—å…¸
        """
        logger.info(f"å¼€å§‹DBSCANèšç±»")

        # å…³é”®ä¿®å¤ï¼šé‡æ–°è®¾è®¡é¢„å¤„ç†ç®¡é“
        logger.info(f"å¼€å§‹æ­£ç¡®çš„é¢„å¤„ç†ç®¡é“ï¼ˆéšå‘é‡æ ‡å‡†åŒ–+æ ‡ç­¾æƒé‡ä¿ç•™ï¼‰")

        # ç¬¬1æ­¥ï¼šæ‹†åˆ†å¢å¼ºç‰¹å¾ä¸ºåŸå§‹éšå‘é‡å’ŒåŠ æƒæ ‡ç­¾å‘é‡
        original_embeddings = self.valid_embeddings[:, :32]  # å‰32åˆ—ï¼šåŸå§‹éšå‘é‡
        weighted_labels = self.valid_embeddings[:, 32:]      # å6åˆ—ï¼šåŠ æƒæ ‡ç­¾å‘é‡

        logger.info(f"   åŸå§‹éšå‘é‡ç»´åº¦: {original_embeddings.shape}")
        logger.info(f"   åŠ æƒæ ‡ç­¾å‘é‡ç»´åº¦: {weighted_labels.shape}")
        logger.info(f"   æ ‡ç­¾æƒé‡èŒƒå›´: [{np.min(weighted_labels):.1f}, {np.max(weighted_labels):.1f}]")

        # ç¬¬2æ­¥ï¼šä»…å¯¹åŸå§‹éšå‘é‡æ‰§è¡Œæ ‡å‡†åŒ–ï¼ˆç»ä¸è§¦ç¢°æ ‡ç­¾æƒé‡ï¼ï¼‰
        scaler = StandardScaler()
        scaled_embeddings = scaler.fit_transform(original_embeddings)

        # ä¿å­˜Scaleræ¨¡å‹ä»¥ä¾¿åç»­ä½¿ç”¨
        self.scaler_model = scaler

        logger.info(f"   éšå‘é‡æ ‡å‡†åŒ–å®Œæˆ:")
        logger.info(f"   æ ‡å‡†åŒ–å‰å‡å€¼: {np.mean(original_embeddings, axis=0)[:3]}...")
        logger.info(f"   æ ‡å‡†åŒ–å‰æ ‡å‡†å·®: {np.std(original_embeddings, axis=0)[:3]}...")
        logger.info(f"   æ ‡å‡†åŒ–åå‡å€¼: {np.mean(scaled_embeddings, axis=0)[:3]}...")
        logger.info(f"   æ ‡å‡†åŒ–åæ ‡å‡†å·®: {np.std(scaled_embeddings, axis=0)[:3]}...")

        # ç¬¬3æ­¥ï¼šå°†æ ‡å‡†åŒ–éšå‘é‡ä¸åŸå§‹æƒé‡æ ‡ç­¾é‡æ–°æ‹¼æ¥ï¼ˆä¿æŒæ ‡ç­¾æƒé‡æ•ˆæœï¼ï¼‰
        embeddings_processed = np.hstack([scaled_embeddings, weighted_labels])

        logger.info(f"   æ‹¼æ¥åç‰¹å¾ç»´åº¦: {embeddings_processed.shape}")
        logger.info(f"   æ ‡ç­¾æƒé‡ä¿æŒå®Œæ•´: {np.mean(embeddings_processed[:, 32:], axis=0)}")

        # ç¬¬4æ­¥ï¼šPCAé™ç»´é¢„å¤„ç†ï¼ˆä»38ç»´é™åˆ°12ç»´ï¼Œä¿ç•™æ›´å¤šæ ‡ç­¾ä¸»å¯¼ç»“æ„ï¼‰
        logger.info(f"å¼€å§‹PCAé™ç»´é¢„å¤„ç†ï¼ˆä¿ç•™æ ‡ç­¾ä¸»å¯¼ç»“æ„ï¼‰")
        pca_components = 12
        pca = PCA(n_components=pca_components, random_state=42)
        embeddings_for_clustering = pca.fit_transform(embeddings_processed)

        # ä¿å­˜PCAæ¨¡å‹ä»¥ä¾¿åç»­ä½¿ç”¨
        self.pca_model = pca

        logger.info(f"   PCAé™ç»´: 38ç»´ -> {pca_components}ç»´")
        logger.info(f"   æ–¹å·®è§£é‡Šæ¯”ä¾‹: {np.sum(pca.explained_variance_ratio_):.3f}")
        logger.info(f"   å‰5ä¸ªæˆåˆ†è§£é‡Šæ¯”ä¾‹: {pca.explained_variance_ratio_[:5]}")
        logger.info(f"   æ ‡ç­¾ç‰¹å¾åœ¨PCAä¸­çš„å½±å“åŠ›å¾—åˆ°ä¿ç•™ï¼")

        # å‚æ•°è®¾ç½®ï¼ˆåŸºäºé™ç»´åçš„ç©ºé—´ï¼‰
        if eps is None:
            eps = self.find_optimal_eps(k=min_samples or 3, embeddings=embeddings_for_clustering)
        if min_samples is None:
            min_samples = 3  # å›ºå®šä¸ºè¾ƒå°å€¼ï¼Œé€‚åˆä½ç»´ç©ºé—´

        logger.info(f"èšç±»å‚æ•°: eps={eps:.4f}, min_samples={min_samples}")

        # åœ¨12ç»´PCAç©ºé—´ä¸Šæ‰§è¡ŒDBSCANï¼ˆæ ‡ç­¾æƒé‡ä¸»å¯¼çš„ç©ºé—´ï¼‰
        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')
        cluster_labels = dbscan.fit_predict(embeddings_for_clustering)

        # åˆ†æèšç±»ç»“æœ
        unique_labels = set(cluster_labels)
        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
        n_noise = list(cluster_labels).count(-1)
        noise_ratio = n_noise/len(cluster_labels)*100

        logger.info(f"ğŸ¯ èšç±»ç»“æœåˆ†æ:")
        logger.info(f"   ç°‡æ•°é‡: {n_clusters}")
        logger.info(f"   å™ªå£°ç‚¹: {n_noise} ({noise_ratio:.1f}%)")
        logger.info(f"   æœ‰æ•ˆèšç±»ç‡: {100-noise_ratio:.1f}%")

        # å‹å¥½æç¤º
        if n_clusters == 0:
            logger.warning(f"æœªæ£€æµ‹åˆ°ä»»ä½•èšç±»ï¼å»ºè®®ï¼š")
            logger.warning(f"  1. å¢å¤§epså‚æ•°ï¼ˆå½“å‰: {eps:.3f}ï¼‰")
            logger.warning(f"  2. å‡å°min_sampleså‚æ•°ï¼ˆå½“å‰: {min_samples}ï¼‰")
            logger.warning(f"  3. æ£€æŸ¥æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§")

        # è®¡ç®—è½®å»“ç³»æ•°ï¼ˆä»…å½“ç°‡æ•°>1ä¸”å™ªå£°ç‚¹ä¸è¿‡å¤šæ—¶ï¼‰
        silhouette_avg = None
        if n_clusters > 1 and n_noise < len(cluster_labels) * 0.5:
            try:
                silhouette_avg = silhouette_score(
                    embeddings_for_clustering[cluster_labels != -1],  # ä½¿ç”¨é™ç»´åçš„ç‰¹å¾
                    cluster_labels[cluster_labels != -1]
                )
                logger.info(f"   è½®å»“ç³»æ•°: {silhouette_avg:.3f}")
            except Exception as e:
                logger.warning(f"è½®å»“ç³»æ•°è®¡ç®—å¤±è´¥: {e}")

        # ä¿å­˜èšç±»ç»“æœï¼ˆç¡®ä¿PythonåŸç”Ÿç±»å‹ï¼‰
        self.clustering_results = {
            'labels': cluster_labels,
            'n_clusters': int(n_clusters),
            'n_noise': int(n_noise),
            'eps': float(eps) if eps is not None else None,
            'min_samples': int(min_samples) if min_samples is not None else None,
            'silhouette_score': float(silhouette_avg) if silhouette_avg is not None else None,
            'cluster_info': {}
        }

        # åˆ†ææ¯ä¸ªç°‡çš„è¯¦ç»†ä¿¡æ¯
        for label in unique_labels:
            if label == -1:
                continue  # å™ªå£°ç‚¹ç¨åå¤„ç†

            cluster_mask = cluster_labels == label
            cluster_size = np.sum(cluster_mask)
            cluster_embeddings = embeddings_for_clustering[cluster_mask]  # ä½¿ç”¨é™ç»´åçš„ç‰¹å¾

            # è®¡ç®—ç°‡çš„è´¨å¿ƒï¼ˆåœ¨é™ç»´ç©ºé—´ä¸­ï¼‰
            centroid = np.mean(cluster_embeddings, axis=0)

            # è®¡ç®—ç°‡çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç¡®ä¿JSONåºåˆ—åŒ–å…¼å®¹æ€§ï¼‰
            cluster_info = {
                'label': int(label),
                'size': int(cluster_size),
                'centroid': [float(x) for x in centroid.tolist()],  # 12ç»´è´¨å¿ƒï¼Œç¡®ä¿floatç±»å‹
                'indices': [int(x) for x in self.valid_indices[cluster_mask].tolist()],  # ç¡®ä¿intç±»å‹
                'avg_distance_to_centroid': float(np.mean(cdist([centroid], cluster_embeddings)[0]))
            }

            self.clustering_results['cluster_info'][int(label)] = cluster_info
            logger.info(f"   ç°‡ {label}: {cluster_size} ä¸ªæ ·æœ¬")

        return self.clustering_results

    def reduce_dimensions(self, method: str = 'tsne', n_components: int = 2, **kwargs) -> np.ndarray:
        """é™ç»´å¤„ç†ç”¨äºå¯è§†åŒ–

        Args:
            method: é™ç»´æ–¹æ³• ('tsne', 'pca')
            n_components: é™ç»´åçš„ç»´åº¦
            **kwargs: é™ç»´ç®—æ³•çš„é¢å¤–å‚æ•°

        Returns:
            é™ç»´åçš„ç‰¹å¾çŸ©é˜µ
        """
        logger.info(f"å¼€å§‹é™ç»´å¤„ç† (æ–¹æ³•: {method})")

        if method == 'tsne':
            # t-SNEé™ç»´ï¼ˆå¸¦PCAä¼˜åŒ–ï¼‰
            n_samples = len(self.valid_embeddings)
            n_features = self.valid_embeddings.shape[1]

            # å½“æ ·æœ¬æ•°è¶…è¿‡2000æ—¶ï¼Œå…ˆç”¨PCAé™ç»´åŠ é€Ÿ
            if n_samples > 2000:
                pca_components = min(50, n_features, n_samples // 4)
                logger.info(f"æ ·æœ¬æ•°é‡è¾ƒå¤š({n_samples})ï¼Œå…ˆç”¨PCAé™ç»´åˆ°{pca_components}ç»´åŠ é€Ÿt-SNE")

                pca = PCA(n_components=pca_components, random_state=42)
                embeddings_for_tsne = pca.fit_transform(self.valid_embeddings)

                # æ˜¾ç¤ºPCAé¢„å¤„ç†ä¿¡æ¯
                explained_variance = pca.explained_variance_ratio_
                logger.info(f"   PCAé¢„å¤„ç†è§£é‡Šæ–¹å·®æ¯”ä¾‹: {sum(explained_variance[:10]):.3f}")
                logger.info(f"   ç´¯è®¡è§£é‡Šæ–¹å·®: {np.cumsum(explained_variance)[-1]:.3f}")
            else:
                embeddings_for_tsne = self.valid_embeddings

            perplexity = kwargs.get('perplexity', min(30, len(embeddings_for_tsne) - 1))
            n_iter = kwargs.get('n_iter', 1000)

            tsne = TSNE(
                n_components=n_components,
                perplexity=perplexity,
                max_iter=n_iter,
                random_state=42,
                verbose=1
            )
            reduced_embeddings = tsne.fit_transform(embeddings_for_tsne)

        elif method == 'pca':
            # PCAé™ç»´
            pca = PCA(n_components=n_components, random_state=42)
            reduced_embeddings = pca.fit_transform(self.valid_embeddings)

            # æ˜¾ç¤ºè§£é‡Šæ–¹å·®æ¯”ä¾‹
            explained_variance = pca.explained_variance_ratio_
            logger.info(f"   PCAè§£é‡Šæ–¹å·®æ¯”ä¾‹: {explained_variance}")
            logger.info(f"   ç´¯è®¡è§£é‡Šæ–¹å·®: {np.cumsum(explained_variance)[-1]:.3f}")

        else:
            raise ValueError(f"æœªçŸ¥çš„é™ç»´æ–¹æ³•: {method}")

        self.reduced_embeddings[method] = reduced_embeddings
        logger.info(f"é™ç»´å®Œæˆ: {reduced_embeddings.shape}")
        return reduced_embeddings

    def visualize_clusters(self, method: str = 'tsne', save_path: str = None,
                          figsize: Tuple[int, int] = (12, 8)) -> None:
        """å¯è§†åŒ–èšç±»ç»“æœ

        Args:
            method: ä½¿ç”¨çš„é™ç»´æ–¹æ³•
            save_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä¿å­˜
            figsize: å›¾åƒå¤§å°
        """
        logger.info(f"ğŸ¨ å¼€å§‹ç»˜åˆ¶èšç±»å¯è§†åŒ–å›¾")

        if not self.clustering_results:
            raise ValueError("è¯·å…ˆæ‰§è¡Œèšç±»åˆ†æ")

        if method not in self.reduced_embeddings:
            self.reduce_dimensions(method)

        reduced_embeddings = self.reduced_embeddings[method]
        cluster_labels = self.clustering_results['labels']

        # åˆ›å»ºé¢œè‰²æ˜ å°„
        unique_labels = np.unique(cluster_labels)
        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))

        # åˆ›å»ºå›¾åƒ
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)

        # å·¦å›¾ï¼šæ‰€æœ‰ç‚¹ï¼ŒæŒ‰ç°‡ç€è‰²
        for i, label in enumerate(unique_labels):
            mask = cluster_labels == label
            if label == -1:
                # å™ªå£°ç‚¹ç”¨çº¢è‰²æ˜Ÿæ˜Ÿæ ‡è®°ï¼Œæ›´é†’ç›®
                ax1.scatter(
                    reduced_embeddings[mask, 0],
                    reduced_embeddings[mask, 1],
                    c='red',
                    marker='*',
                    s=80,  # å¢å¤§å°ºå¯¸
                    alpha=0.8,  # å¢åŠ é€æ˜åº¦
                    edgecolors='darkred',  # æ·»åŠ è¾¹ç¼˜
                    linewidths=1,
                    label=f'å™ªå£°ç‚¹ ({np.sum(mask)}ä¸ª)',
                    zorder=10  # ç¡®ä¿åœ¨æœ€ä¸Šå±‚
                )
                # æ·»åŠ å™ªå£°ç‚¹æ€»æ•°æ ‡æ³¨
                noise_count = np.sum(mask)
                if noise_count > 0:
                    # è®¡ç®—å™ªå£°ç‚¹çš„ä¸­å¿ƒä½ç½®
                    center_x = np.mean(reduced_embeddings[mask, 0])
                    center_y = np.mean(reduced_embeddings[mask, 1])
                    ax1.annotate(f'å™ªå£°ç‚¹: {noise_count}',
                               xy=(center_x, center_y),
                               xytext=(10, 10), textcoords='offset points',
                               bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                               fontsize=9, fontweight='bold')
            else:
                ax1.scatter(
                    reduced_embeddings[mask, 0],
                    reduced_embeddings[mask, 1],
                    c=[colors[i]],
                    s=50,
                    alpha=0.7,
                    label=f'ç°‡ {label} ({np.sum(mask)}ä¸ª)'
                )

        ax1.set_xlabel(f'{method.upper()} ç¬¬1ç»´')
        ax1.set_ylabel(f'{method.upper()} ç¬¬2ç»´')
        ax1.set_title(f'CVAEéšç©ºé—´èšç±»ç»“æœ ({method.upper()}é™ç»´)')
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax1.grid(True, alpha=0.3)

        # å³å›¾ï¼šç°‡åˆ†å¸ƒç»Ÿè®¡
        cluster_sizes = [np.sum(cluster_labels == label) for label in unique_labels]
        cluster_names = ['å™ªå£°ç‚¹' if label == -1 else f'ç°‡ {label}' for label in unique_labels]

        bars = ax2.bar(range(len(cluster_names)), cluster_sizes, color=colors)
        ax2.set_xlabel('ç°‡')
        ax2.set_ylabel('æ ·æœ¬æ•°é‡')
        ax2.set_title('ç°‡åˆ†å¸ƒç»Ÿè®¡')
        ax2.set_xticks(range(len(cluster_names)))
        ax2.set_xticklabels(cluster_names, rotation=45)

        # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
        for bar, size in zip(bars, cluster_sizes):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01*max(cluster_sizes),
                    f'{size}', ha='center', va='bottom')

        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"å¯è§†åŒ–å›¾å·²ä¿å­˜: {save_path}")

        plt.show()

    def select_refined_payloads(self, samples_per_cluster: int = 5, keep_all_noise: bool = True) -> List[Dict]:
        """ç­›é€‰ç²¾é”è½½è·

        Args:
            samples_per_cluster: æ¯ä¸ªç°‡ä¿ç•™çš„æ ·æœ¬æ•°ï¼ˆè´¨å¿ƒé™„è¿‘ï¼‰
            keep_all_noise: æ˜¯å¦ä¿ç•™æ‰€æœ‰å™ªå£°ç‚¹

        Returns:
            ç²¾é”è½½è·åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹ç­›é€‰ç²¾é”è½½è·")
        logger.info(f"   æ¯ç°‡ä¿ç•™æ ·æœ¬æ•°: {samples_per_cluster}")
        logger.info(f"   ä¿ç•™æ‰€æœ‰å™ªå£°ç‚¹: {keep_all_noise}")

        if not self.clustering_results:
            raise ValueError("è¯·å…ˆæ‰§è¡Œèšç±»åˆ†æ")

        refined_payloads = []
        cluster_labels = self.clustering_results['labels']

        # å¤„ç†æ¯ä¸ªç°‡
        for cluster_id, cluster_info in self.clustering_results['cluster_info'].items():
            cluster_indices = cluster_info['indices']
            centroid = np.array(cluster_info['centroid'])

            # ä¿®å¤ç´¢å¼•æ˜ å°„é—®é¢˜ï¼šç›´æ¥ä»cluster_infoè·å–indicesï¼Œè¿™äº›å·²ç»æ˜¯å…¨å±€ç´¢å¼•
            # è·å–å¯¹åº”çš„å¢å¼ºç‰¹å¾
            cluster_mask = np.isin(self.valid_indices, cluster_indices)
            cluster_enhanced_embeddings = self.valid_embeddings[cluster_mask]

            # åº”ç”¨æ­£ç¡®çš„é¢„å¤„ç†ç®¡é“ï¼ˆä¸perform_clusteringå®Œå…¨ä¸€è‡´ï¼‰
            if hasattr(self, 'scaler_model') and hasattr(self, 'pca_model'):
                # æ‹†åˆ†ç‰¹å¾ï¼šåŸå§‹éšå‘é‡ + åŠ æƒæ ‡ç­¾
                original_part = cluster_enhanced_embeddings[:, :32]
                label_part = cluster_enhanced_embeddings[:, 32:]

                # ä»…å¯¹éšå‘é‡éƒ¨åˆ†æ ‡å‡†åŒ–ï¼Œä¿æŒæ ‡ç­¾æƒé‡ä¸å˜
                scaled_original = self.scaler_model.transform(original_part)

                # é‡æ–°æ‹¼æ¥å¹¶åº”ç”¨PCA
                cluster_processed = np.hstack([scaled_original, label_part])
                cluster_embeddings_pca = self.pca_model.transform(cluster_processed)
            elif hasattr(self, 'pca_model'):
                # ä»…æœ‰PCAæ¨¡å‹çš„æƒ…å†µï¼ˆå‘åå…¼å®¹ï¼‰
                cluster_embeddings_pca = self.pca_model.transform(cluster_enhanced_embeddings)
            else:
                cluster_embeddings_pca = cluster_enhanced_embeddings

            # è®¡ç®—æ¯ä¸ªæ ·æœ¬åˆ°è´¨å¿ƒçš„è·ç¦»ï¼ˆè´¨å¿ƒå·²ç»åœ¨PCAç©ºé—´ä¸­ï¼‰
            distances = cdist([centroid], cluster_embeddings_pca)[0]

            # é€‰æ‹©è·ç¦»è´¨å¿ƒæœ€è¿‘çš„æ ·æœ¬
            n_select = min(samples_per_cluster, len(cluster_indices))
            selected_local_indices = np.argsort(distances)[:n_select]

            # ç¡®ä¿ç´¢å¼•æ˜ å°„æ­£ç¡®
            valid_cluster_indices = self.valid_indices[cluster_mask]

            for i, local_idx in enumerate(selected_local_indices):
                global_idx = int(valid_cluster_indices[local_idx])
                refined_payload = {
                    'id': int(global_idx),
                    'payload': self.payloads[global_idx],
                    'metadata': self.metadata[global_idx],
                    'cluster_id': int(cluster_id),
                    'distance_to_centroid': float(distances[local_idx]),
                    'selection_reason': 'centroid_close'
                }
                refined_payloads.append(refined_payload)

            logger.info(f"   ç°‡ {cluster_id}: é€‰æ‹© {n_select} ä¸ªè´¨å¿ƒæ ·æœ¬")

        # å¤„ç†å™ªå£°ç‚¹
        if keep_all_noise:
            noise_mask = cluster_labels == -1
            noise_indices = self.valid_indices[noise_mask]

            for idx in noise_indices:
                global_idx = int(idx)
                refined_payload = {
                    'id': global_idx,
                    'payload': self.payloads[global_idx],
                    'metadata': self.metadata[global_idx],
                    'cluster_id': -1,
                    'distance_to_centroid': float('inf'),
                    'selection_reason': 'noise_outlier'
                }
                refined_payloads.append(refined_payload)

            logger.info(f"   å™ªå£°ç‚¹: ä¿ç•™ {len(noise_indices)} ä¸ªå¼‚å¸¸æ ·æœ¬")

        self.refined_payloads = refined_payloads

        # ç»Ÿè®¡ä¿¡æ¯
        total_selected = len(refined_payloads)
        total_original = len(self.valid_embeddings)
        reduction_ratio = (total_original - total_selected) / total_original

        logger.info(f"ç²¾é”è½½è·ç­›é€‰å®Œæˆï¼")
        logger.info(f"   åŸå§‹æ ·æœ¬: {total_original}")
        logger.info(f"   ç­›é€‰å: {total_selected}")
        logger.info(f"   å‹ç¼©æ¯”ä¾‹: {reduction_ratio*100:.1f}%")

        return refined_payloads

    def save_refined_payloads(self, output_path: str) -> None:
        """ä¿å­˜ç²¾é”è½½è·åˆ°æ–‡ä»¶

        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not self.refined_payloads:
            raise ValueError("è¯·å…ˆæ‰§è¡Œç²¾é”è½½è·ç­›é€‰")

        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ä¸ºtxtæ ¼å¼ï¼ˆä»…è½½è·æ–‡æœ¬ï¼‰
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in self.refined_payloads:
                f.write(item['payload'] + '\n')

        logger.info(f"ç²¾é”è½½è·å·²ä¿å­˜: {output_file}")
        logger.info(f"   æ€»æ•°: {len(self.refined_payloads)} ä¸ª")

    def save_clustering_results(self, output_dir: str) -> None:
        """ä¿å­˜å®Œæ•´çš„èšç±»åˆ†æç»“æœ

        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        def convert_numpy_types(obj):
            """é€’å½’è½¬æ¢NumPyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜èšç±»ç»“æœï¼ˆå¤„ç†numpyæ•°ç»„åºåˆ—åŒ–ï¼‰
        results_for_saving = convert_numpy_types(self.clustering_results)

        results_file = output_path / "clustering_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_for_saving, f, ensure_ascii=False, indent=2)
        logger.info(f"èšç±»ç»“æœå·²ä¿å­˜: {results_file}")

        # ä¿å­˜é™ç»´ç»“æœ
        for method, embeddings in self.reduced_embeddings.items():
            embeddings_file = output_path / f"reduced_embeddings_{method}.npy"
            np.save(embeddings_file, embeddings)
            logger.info(f"é™ç»´ç»“æœå·²ä¿å­˜: {embeddings_file}")

        # ä¿å­˜ç²¾é”è½½è·è¯¦ç»†ä¿¡æ¯
        if self.refined_payloads:
            refined_file = output_path / "refined_payloads.json"
            # ç¡®ä¿ç²¾é”è½½è·æ•°æ®ä¹Ÿæ˜¯JSONå¯åºåˆ—åŒ–çš„
            refined_payloads_for_saving = convert_numpy_types(self.refined_payloads)
            with open(refined_file, 'w', encoding='utf-8') as f:
                json.dump(refined_payloads_for_saving, f, ensure_ascii=False, indent=2)
            logger.info(f"ç²¾é”è½½è·è¯¦æƒ…å·²ä¿å­˜: {refined_file}")

        logger.info(f"æ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")


# ä½¿ç”¨ç¤ºä¾‹ï¼š
#
# # åˆå§‹åŒ–èšç±»å™¨ï¼ˆå·²ä¼˜åŒ–ï¼šæ ‡ç­¾æƒé‡æ³¨å…¥ + PCAé™ç»´é¢„å¤„ç†ï¼‰
# clusterer = CVAEClusterer(
#     embeddings=latent_features,      # [N, 32] åŸå§‹éšç©ºé—´ç‰¹å¾
#     payloads=payloads,               # [N] è½½è·æ–‡æœ¬åˆ—è¡¨
#     metadata=metadata,               # [N] åŒ…å«labelå­—æ®µçš„å…ƒæ•°æ®
#     label_weight=8.0                 # æ ‡ç­¾æƒé‡ï¼ˆæ›´é«˜æƒé‡å¼ºåˆ¶åˆ†ç¦»ä¸åŒç±»å‹ï¼‰
# )
#
# # æ‰§è¡Œä¼˜åŒ–åçš„èšç±»ï¼ˆè‡ªåŠ¨ï¼šæ ‡ç­¾å¢å¼º -> PCAé™ç»´ -> DBSCANèšç±»ï¼‰
# results = clusterer.perform_clustering()
#
# # ç­›é€‰ç²¾é”è½½è·ï¼ˆæ¯ä¸ªç°‡ä¿ç•™è´¨å¿ƒé™„è¿‘5ä¸ªæ ·æœ¬ + æ‰€æœ‰å™ªå£°ç‚¹ï¼‰
# refined = clusterer.select_refined_payloads(samples_per_cluster=5, keep_all_noise=True)
#
# print(f"èšç±»æ•ˆæœ: {results['n_clusters']}ä¸ªç°‡ï¼Œå™ªå£°æ¯”ä¾‹{results['n_noise']/len(payloads)*100:.1f}%")



def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œè°ƒç”¨"""
    parser = argparse.ArgumentParser(
        description="CVAEéšç©ºé—´èšç±»å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹ï¼š
  python clusterer.py --embeddings Data/generated/latent_embeddings.npy
                     --payloads Data/generated/raw_payloads.txt
                     --metadata Data/generated/payload_metadata.json
        """
    )

    parser.add_argument(
        '--embeddings',
        type=str,
        required=True,
        help='éšç©ºé—´ç‰¹å¾æ–‡ä»¶è·¯å¾„ (.npy)'
    )

    parser.add_argument(
        '--payloads',
        type=str,
        required=True,
        help='è½½è·æ–‡ä»¶è·¯å¾„ (.txt)'
    )

    parser.add_argument(
        '--metadata',
        type=str,
        required=True,
        help='è½½è·å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„ (.json)'
    )

    parser.add_argument(
        '--valid-mask',
        type=str,
        help='æœ‰æ•ˆæ€§æ©ç æ–‡ä»¶è·¯å¾„ (.npy)'
    )

    parser.add_argument(
        '--eps',
        type=float,
        help='DBSCANçš„epså‚æ•°ï¼ˆè‡ªåŠ¨å¯»æ‰¾å¦‚æœæœªæŒ‡å®šï¼‰'
    )

    parser.add_argument(
        '--min-samples',
        type=int,
        help='DBSCANçš„min_sampleså‚æ•°'
    )

    parser.add_argument(
        '--samples-per-cluster',
        type=int,
        default=5,
        help='æ¯ä¸ªç°‡ä¿ç•™çš„æ ·æœ¬æ•° (é»˜è®¤: 5)'
    )

    parser.add_argument(
        '--keep-noise',
        action='store_true',
        default=False,
        help='ä¿ç•™æ‰€æœ‰å™ªå£°ç‚¹ (é»˜è®¤: False)'
    )

    parser.add_argument(
        '--cluster',
        action='store_true',
        default=True,
        help='æ‰§è¡Œèšç±»åˆ†æ (é»˜è®¤: True)'
    )

    parser.add_argument(
        '--reduction-method',
        type=str,
        default='tsne',
        choices=['tsne', 'pca'],
        help='é™ç»´æ–¹æ³• (é»˜è®¤: tsne)'
    )

    parser.add_argument(
        '--output-dir',
        type=str,
        default='Data/clustered',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: Data/clustered)'
    )

    parser.add_argument(
        '--visualize',
        action='store_true',
        help='ç”Ÿæˆå¯è§†åŒ–å›¾åƒ'
    )

    parser.add_argument(
        '--label-weight',
        type=float,
        default=8.0,
        help='æ ‡ç­¾æƒé‡å› å­ï¼Œç”¨äºå¢å¼ºä¸åŒç±»å‹è½½è·çš„åˆ†ç¦»åº¦ (é»˜è®¤: 8.0)'
    )

    args = parser.parse_args()

    print("=" * 80)
    print("CVAEéšç©ºé—´èšç±»å™¨")
    print("=" * 80)

    try:
        # åŠ è½½æ•°æ®
        logger.info("ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶...")

        # åŠ è½½éšç©ºé—´ç‰¹å¾
        embeddings = np.load(args.embeddings)
        logger.info(f"   éšç©ºé—´ç‰¹å¾: {embeddings.shape}")

        # åŠ è½½è½½è·
        with open(args.payloads, 'r', encoding='utf-8') as f:
            payloads = [line.strip() for line in f.readlines()]
        logger.info(f"   è½½è·æ•°é‡: {len(payloads)}")

        # åŠ è½½å…ƒæ•°æ®
        with open(args.metadata, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        logger.info(f"   å…ƒæ•°æ®æ•°é‡: {len(metadata)}")

        # åŠ è½½æœ‰æ•ˆæ€§æ©ç 
        valid_mask = None
        if args.valid_mask:
            valid_mask = np.load(args.valid_mask)
            logger.info(f"   æœ‰æ•ˆæ€§æ©ç : {valid_mask.shape}")

        # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
        min_len = min(len(embeddings), len(payloads), len(metadata))
        embeddings = embeddings[:min_len]
        payloads = payloads[:min_len]
        metadata = metadata[:min_len]
        if valid_mask is not None:
            valid_mask = valid_mask[:min_len]

        # åˆå§‹åŒ–èšç±»å™¨
        clusterer = CVAEClusterer(embeddings, payloads, metadata, valid_mask, label_weight=args.label_weight)

        # æ‰§è¡Œèšç±»
        clustering_results = clusterer.perform_clustering(
            eps=args.eps,
            min_samples=args.min_samples
        )

        # é™ç»´
        clusterer.reduce_dimensions(method=args.reduction_method)

        # å¯è§†åŒ–
        if args.visualize:
            viz_path = Path(args.output_dir) / "clustering_visualization.png"
            clusterer.visualize_clusters(
                method=args.reduction_method,
                save_path=str(viz_path)
            )

        # ç­›é€‰ç²¾é”è½½è·
        refined_payloads = clusterer.select_refined_payloads(
            samples_per_cluster=args.samples_per_cluster,
            keep_all_noise=args.keep_noise
        )

        # ä¿å­˜ç»“æœ
        clusterer.save_clustering_results(args.output_dir)

        # ä¿å­˜ç²¾é”è½½è·åˆ°æŒ‡å®šä½ç½®
        refined_path = Path(args.output_dir).parent / "fuzzing" / "refined_payloads.txt"
        clusterer.save_refined_payloads(str(refined_path))

        print("\n" + "=" * 80)
        print("èšç±»åˆ†æä»»åŠ¡å®Œæˆï¼")
        print("=" * 80)
        print(f"èšç±»ç»“æœ:")
        print(f"   ç°‡æ•°é‡: {clustering_results['n_clusters']}")
        print(f"   å™ªå£°ç‚¹: {clustering_results['n_noise']}")
        print(f"   ç²¾é”è½½è·: {len(refined_payloads)}")
        print(f"   å‹ç¼©æ¯”ä¾‹: {(1 - len(refined_payloads)/len(payloads))*100:.1f}%")

    except Exception as e:
        logger.error(f"âŒ èšç±»åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()