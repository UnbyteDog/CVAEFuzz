#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CVAE æ¨¡å‹å®šä¹‰
============

åŸºäº GRU çš„ Seq2Seq æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨å®ç°
ä¸¥æ ¼éµå¾ª Doc/promptæŒ‡å¯¼.md ä¸­çš„æŠ€æœ¯è§„èŒƒ

ä½œè€…ï¼šè€ç‹ (æš´èºæŠ€æœ¯æµ)
ç‰ˆæœ¬ï¼š1.0
æ—¥æœŸï¼š2025-12-18
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional, Dict, Any
import math


class GumbelSoftmax(nn.Module):
    """Gumbel-Softmax é‡‡æ ·å™¨

    ç”¨äºå¤„ç†ç¦»æ•£æ•°æ®ç”Ÿæˆï¼Œè§£å†³ä¸å¯å¯¼é‡‡æ ·é—®é¢˜
    """

    def __init__(self, temperature: float = 1.0, hard: bool = False):
        super(GumbelSoftmax, self).__init__()
        self.temperature = temperature
        self.hard = hard

    def sample_gumbel(self, logits: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:
        """ä» Gumbel(0,1) åˆ†å¸ƒé‡‡æ ·"""
        U = torch.rand_like(logits)
        return -torch.log(-torch.log(U + eps) + eps)

    def forward(self, logits: torch.Tensor, temperature: Optional[float] = None) -> torch.Tensor:
        """
        Args:
            logits: [batch_size, vocab_size] æœªå½’ä¸€åŒ–çš„å¯¹æ•°æ¦‚ç‡
            temperature: å¯é€‰çš„æ¸©åº¦å‚æ•°ï¼Œè¦†ç›–å®ä¾‹çš„temperature

        Returns:
            [batch_size, vocab_size] é‡‡æ ·ç»“æœ
        """
        if temperature is None:
            temperature = self.temperature

        # æ·»åŠ  Gumbel å™ªå£°
        y = logits + self.sample_gumbel(logits)

        # åº”ç”¨ Softmax
        y = F.softmax(y / temperature, dim=-1)

        if self.hard:
            # ç¡¬åŒ–ï¼šè¿”å› one-hot å‘é‡ï¼Œä½†ä¿æŒæ¢¯åº¦
            shape = y.size()
            _, ind = y.max(dim=-1)
            y_hard = torch.zeros_like(y).scatter_(-1, ind.view(-1, 1), 1.0)
            y = y_hard - y.detach() + y

        return y


class CVAREncoder(nn.Module):
    """CVAE ç¼–ç å™¨

    ä½¿ç”¨åŒå‘ GRU å°†è¾“å…¥åºåˆ— x å’Œæ¡ä»¶ c æ˜ å°„åˆ°éšç©ºé—´çš„å‡å€¼å’Œæ–¹å·®
    """

    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int,
                 latent_dim: int, condition_dim: int, num_layers: int = 2):
        super(CVAREncoder, self).__init__()

        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        self.condition_dim = condition_dim
        self.num_layers = num_layers

        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # æ¡ä»¶åµŒå…¥å±‚
        self.condition_embedding = nn.Embedding(condition_dim, embed_dim)

        # åŒå‘ GRU
        self.gru = nn.GRU(
            input_size=embed_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True
        )

        # éšç©ºé—´å‚æ•°æŠ•å½±å±‚
        self.fc_mu = nn.Linear(hidden_dim * 2, latent_dim)  # åŒå‘ GRU è¾“å‡ºä¸º hidden_dim * 2
        self.fc_logvar = nn.Linear(hidden_dim * 2, latent_dim)

        # ğŸ”¥ LayerNormå±‚ - åœ¨__init__ä¸­æ­£ç¡®åˆå§‹åŒ–
        self.layer_norm = nn.LayerNorm(hidden_dim * 2)

    def forward(self, x: torch.Tensor, c: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: [batch_size, seq_len] è¾“å…¥åºåˆ—
            c: [batch_size] æ¡ä»¶æ ‡ç­¾

        Returns:
            mu: [batch_size, latent_dim] å‡å€¼
            logvar: [batch_size, latent_dim] å¯¹æ•°æ–¹å·®
        """
        batch_size = x.size(0)
        seq_len = x.size(1)

        # åµŒå…¥è¾“å…¥åºåˆ—
        x_embed = self.embedding(x)  # [batch_size, seq_len, embed_dim]

        # åµŒå…¥æ¡ä»¶å¹¶å¹¿æ’­åˆ°åºåˆ—é•¿åº¦
        c_embed = self.condition_embedding(c).unsqueeze(1).expand(-1, seq_len, -1)  # [batch_size, seq_len, embed_dim]

        # ç»“åˆæ¡ä»¶ä¿¡æ¯ (ç®€å•ç›¸åŠ )
        combined = x_embed + c_embed

        # åŒå‘ GRU ç¼–ç 
        outputs, hidden = self.gru(combined)

        # ğŸ”¥ å±‚çº§ç‰¹å¾èšåˆï¼šå¤šç»´åº¦ä¿¡æ¯èåˆç¡®ä¿éšå˜é‡æ‰¿è½½ç»“æ„åŒ–ä¿¡æ¯
        # outputså½¢çŠ¶: [batch_size, seq_len, hidden_dim * 2]

        # ğŸ”¥ ç­–ç•¥1ï¼šå¤šå±‚çº§ç‰¹å¾æå–
        global_avg = torch.mean(outputs, dim=1)  # [batch_size, hidden_dim * 2] å…¨å±€å¹³å‡
        global_max = torch.max(outputs, dim=1)[0]  # [batch_size, hidden_dim * 2] å…¨å±€æœ€å¤§
        last_output = outputs[:, -1, :]  # [batch_size, hidden_dim * 2] æœ€åæ—¶é—´æ­¥
        first_output = outputs[:, 0, :]  # [batch_size, hidden_dim * 2] ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥

        # ğŸ”¥ ç­–ç•¥2ï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶åŠ æƒ
        # è®¡ç®—ç‰¹å¾é‡è¦æ€§åˆ†æ•°
        feature_scores = torch.mean(outputs, dim=-1)  # [batch_size, seq_len] æ¯ä¸ªæ—¶é—´æ­¥çš„é‡è¦æ€§
        attention_weights = torch.softmax(feature_scores, dim=1).unsqueeze(-1)  # [batch_size, seq_len, 1]

        # æ³¨æ„åŠ›åŠ æƒè¾“å‡º
        attention_output = torch.sum(outputs * attention_weights, dim=1)  # [batch_size, hidden_dim * 2]

        # ğŸ”¥ ç­–ç•¥3ï¼šå±‚çº§èšåˆ - å››ç§ä¸åŒè§†è§’çš„èåˆ
        hierarchical_features = 0.3 * global_avg + 0.2 * global_max + 0.3 * last_output + 0.2 * first_output

        # ğŸ”¥ ç­–ç•¥4ï¼šæ ‡å‡†å·®ç‰¹å¾ - æ•æ‰åºåˆ—å˜åŒ–ä¿¡æ¯
        std_features = torch.std(outputs, dim=1)  # [batch_size, hidden_dim * 2] æ ‡å‡†å·®ç‰¹å¾

        # ğŸ”¥ æœ€ç»ˆç‰¹å¾ç»„åˆï¼šå¤šå±‚çº§ + æ³¨æ„åŠ› + ç»Ÿè®¡ç‰¹å¾
        # ä½¿ç”¨å¯å­¦ä¹ çš„æƒé‡èåˆä¸åŒç‰¹å¾
        final_features = (
            0.4 * hierarchical_features +  # ä¸»è¦çš„å±‚çº§ç‰¹å¾
            0.3 * attention_output +        # æ³¨æ„åŠ›åŠ æƒç‰¹å¾
            0.2 * std_features +            # å˜åŒ–ç‰¹å¾
            0.1 * torch.tanh(global_avg)    # éçº¿æ€§å˜æ¢çš„å…¨å±€ç‰¹å¾
        )

        # ğŸ”¥ æ·»åŠ ç‰¹å¾æ ‡å‡†åŒ–ï¼Œç¡®ä¿éšç©ºé—´è®­ç»ƒç¨³å®š
        final_features = self.layer_norm(final_features)

        # æ˜ å°„åˆ°éšç©ºé—´å‚æ•°
        mu = self.fc_mu(final_features)
        logvar = self.fc_logvar(final_features)

        return mu, logvar


class CVARDecoder(nn.Module):
    """CVAE è§£ç å™¨

    ä½¿ç”¨å•å‘ GRU æ ¹æ®éšå˜é‡ z å’Œæ¡ä»¶ c ç”Ÿæˆåºåˆ—
    """

    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int,
                 latent_dim: int, condition_dim: int, num_layers: int = 2,
                 vocab_info: Dict[str, int] = None):
        super(CVARDecoder, self).__init__()

        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        self.condition_dim = condition_dim
        self.num_layers = num_layers

        # ğŸ”¥ åŠ¨æ€è·å–ç‰¹æ®Štokenç´¢å¼•ï¼Œé¿å…ç¡¬ç¼–ç 
        if vocab_info is not None and 'special_tokens' in vocab_info:
            self.sos_idx = vocab_info['special_tokens'].get('<SOS>', 0)
            self.eos_idx = vocab_info['special_tokens'].get('<EOS>', 1)
            self.pad_idx = vocab_info['special_tokens'].get('<PAD>', 2)
            self.unk_idx = vocab_info['special_tokens'].get('<UNK>', 3)
        else:
            # ä¿æŒå‘åå…¼å®¹çš„é»˜è®¤å€¼
            self.sos_idx = 0
            self.eos_idx = 1
            self.pad_idx = 2
            self.unk_idx = 3

        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # æ¡ä»¶åµŒå…¥å±‚
        self.condition_embedding = nn.Embedding(condition_dim, embed_dim)

        # éšå˜é‡åˆ°åˆå§‹éšè—çŠ¶æ€çš„æŠ•å½±
        self.fc_hidden = nn.Linear(latent_dim + embed_dim, hidden_dim)

        # å•å‘ GRU
        self.gru = nn.GRU(
            input_size=embed_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )

        # è¾“å‡ºå±‚
        self.fc_out = nn.Linear(hidden_dim, vocab_size)

        # Gumbel-Softmax é‡‡æ ·å™¨
        self.gumbel_softmax = GumbelSoftmax(hard=False)  # è®­ç»ƒæ—¶ä½¿ç”¨ soft é‡‡æ ·

        # Word Dropout å‚æ•° - ğŸ”¥ å¼ºåˆ¶æ–­å¥¶ï¼å½»åº•æ‰“ç ´Teacher Forcingä¾èµ–
        self.word_dropout_prob = 0.6  # æé«˜åˆ°60%æ¦‚ç‡ï¼Œå¼ºåˆ¶æ¨¡å‹å¿…é¡»ä¾èµ–éšå˜é‡ï¼

    def forward(self, z: torch.Tensor, c: torch.Tensor,
                target_seq: Optional[torch.Tensor] = None,
                max_length: int = 150, temperature: float = 1.0) -> torch.Tensor:
        """
        Args:
            z: [batch_size, latent_dim] éšå˜é‡
            c: [batch_size] æ¡ä»¶æ ‡ç­¾
            target_seq: [batch_size, seq_len] ç›®æ ‡åºåˆ— (è®­ç»ƒæ—¶ä½¿ç”¨)
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: Gumbel-Softmax æ¸©åº¦å‚æ•°

        Returns:
            è®­ç»ƒæ¨¡å¼: [batch_size, seq_len, vocab_size] è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
            ç”Ÿæˆæ¨¡å¼: [batch_size, seq_len] é‡‡æ ·å¾—åˆ°çš„token_ids
        """
        batch_size = z.size(0)
        device = z.device

        # åµŒå…¥æ¡ä»¶
        c_embed = self.condition_embedding(c)  # [batch_size, embed_dim]

        # åˆå§‹éšè—çŠ¶æ€
        initial_input = torch.cat([z, c_embed], dim=-1)
        hidden = self.fc_hidden(initial_input)  # [batch_size, hidden_dim]
        hidden = hidden.unsqueeze(0).repeat(self.num_layers, 1, 1)  # [num_layers, batch_size, hidden_dim]

        # åˆå§‹è¾“å…¥ (SOS token)
        input_token = torch.full((batch_size, 1), self.sos_idx, dtype=torch.long, device=device)  # åŠ¨æ€SOS

        outputs = []

        if target_seq is not None:
            # ğŸ”¥ è®­ç»ƒæ¨¡å¼ï¼šå¢å¼ºçš„ Teacher Forcing + Word Dropout
            seq_len = target_seq.size(1)
            input_step = input_token

            # ğŸ”¥ å¼ºåˆ¶æ–­å¥¶ç­–ç•¥ï¼šå¤§å¹…é™ä½Teacher Forcingä¾èµ–
            teacher_forcing_ratio = 0.5  # ä»70%é™åˆ°50%ï¼Œä¸€åŠæ—¶é—´é éšå˜é‡ï¼
            use_teacher_forcing = torch.rand(batch_size, device=device) < teacher_forcing_ratio

            for t in range(seq_len):
                # åµŒå…¥å½“å‰è¾“å…¥
                input_embed = self.embedding(input_step)  # [batch_size, 1, embed_dim]

                # ğŸ”¥ å¢å¼ºå¹²æ‰°ï¼šåœ¨åµŒå…¥ä¸­æ·»åŠ å™ªå£°ï¼Œè¿«ä½¿æ¨¡å‹ä¾èµ–éšå˜é‡z
                if self.training and torch.rand(1, device=device) < 0.3:  # 30%æ¦‚ç‡æ·»åŠ å™ªå£°
                    noise = torch.randn_like(input_embed) * 0.1  # æ·»åŠ é«˜æ–¯å™ªå£°
                    input_embed = input_embed + noise

                # GRU å‰å‘ä¼ æ’­
                output, hidden = self.gru(input_embed, hidden)  # output: [batch_size, 1, hidden_dim]

                # è¾“å‡ºæŠ•å½±
                logits = self.fc_out(output.squeeze(1))  # [batch_size, vocab_size]
                outputs.append(logits)

                # ä¸‹ä¸€ä¸ªè¾“å…¥ - ğŸ”¥ æ¿€è¿›çš„å¼ºåˆ¶è‡ªé¢„æµ‹ç­–ç•¥
                if t < seq_len - 1:
                    # ğŸ”¥ ç­–ç•¥1ï¼šåŸºäºteacher_forcingå‘é‡é€‰æ‹©è¾“å…¥
                    true_next = target_seq[:, t:t+1]  # [batch_size, 1] çœŸå®æ ‡ç­¾
                    pred_next = torch.argmax(logits, dim=-1, keepdim=True)  # [batch_size, 1] æ¨¡å‹é¢„æµ‹

                    # ä¸ºæ¯ä¸ªæ ·æœ¬é€‰æ‹©è¾“å…¥ï¼šTeacher Forcing vs è‡ªé¢„æµ‹
                    tf_mask = use_teacher_forcing.unsqueeze(1)  # [batch_size, 1]
                    input_step = torch.where(tf_mask, true_next, pred_next)

                    # ğŸ”¥ é¢å¤–çš„å¼ºåˆ¶è‡ªé¢„æµ‹ï¼š20%æ¦‚ç‡å¼ºåˆ¶æ‰€æœ‰æ ·æœ¬ä½¿ç”¨è‡ªé¢„æµ‹
                    if torch.rand(1, device=device) < 0.2:
                        input_step = pred_next

                    # ğŸ”¥ ç­–ç•¥2ï¼šè¶…å¼ºçš„ Word Dropout (60%æ¦‚ç‡)
                    if self.training:
                        # åŸºç¡€Word Dropout
                        mask = torch.rand_like(input_step.float()) < self.word_dropout_prob
                        # ğŸ”¥ ä½¿ç”¨åŠ¨æ€ç‰¹æ®Štokenç´¢å¼•
                        special_tokens = (input_step == self.sos_idx) | (input_step == self.eos_idx) | (input_step == self.pad_idx)
                        mask = mask & (~special_tokens)
                        input_step = torch.where(mask, torch.tensor(self.unk_idx, device=device), input_step)

                        # ğŸ”¥ ç­–ç•¥3ï¼šé¢å¤–çš„éšæœºæ›¿æ¢ - 15%æ¦‚ç‡æ›¿æ¢ä¸ºéšæœºtoken
                        random_mask = torch.rand_like(input_step.float()) < 0.15
                        # ğŸ”¥ ä»ç‰¹æ®Štokenç´¢å¼•+1å¼€å§‹ï¼Œé¿å…æ›¿æ¢ä¸ºç‰¹æ®Štoken
                        min_valid_idx = max(self.unk_idx, self.pad_idx, self.sos_idx, self.eos_idx) + 1
                        random_tokens = torch.randint(min_valid_idx, self.vocab_size, input_step.shape, device=device)
                        random_mask = random_mask & (~special_tokens)
                        input_step = torch.where(random_mask, random_tokens, input_step)

        else:
            # ğŸ”¥ ç”Ÿæˆæ¨¡å¼ï¼šè‡ªå›å½’ç”Ÿæˆï¼Œç›´æ¥æ”¶é›†é‡‡æ ·token_idsï¼
            input_step = input_token
            sampled_ids = []  # ğŸ”¥ ç›´æ¥åœ¨å¾ªç¯ä¸­æ”¶é›†token_idsï¼Œé¿å…åç»­argmaxï¼

            for t in range(max_length):
                # åµŒå…¥å½“å‰è¾“å…¥
                input_embed = self.embedding(input_step)  # [batch_size, 1, embed_dim]

                # GRU å‰å‘ä¼ æ’­
                output, hidden = self.gru(input_embed, hidden)  # output: [batch_size, 1, hidden_dim]

                # è¾“å‡ºæŠ•å½±
                logits = self.fc_out(output.squeeze(1))  # [batch_size, vocab_size]
                outputs.append(logits)

                # ğŸ”¥ Top-ké‡‡æ ·ä¸UNKå¼ºæŠ‘åˆ¶ç­–ç•¥ï¼
                if self.training:
                    # è®­ç»ƒæ—¶ä½¿ç”¨Gumbel-Softmax
                    probs = self.gumbel_softmax(logits, temperature=temperature)
                    next_token = torch.argmax(probs, dim=-1, keepdim=True)
                else:
                    # ğŸ”¥ ç”Ÿæˆæ—¶ï¼šç¦æ­¢ä½¿ç”¨ç®€å•çš„argmaxï¼Œå¼ºåˆ¶ä½¿ç”¨æ™ºèƒ½é‡‡æ ·
                    # Step 1: Temperatureç¼©æ”¾
                    scaled_logits = logits / temperature

                    # Step 2: Top-ké‡‡æ · (k=10)
                    k = 10
                    topk_values, topk_indices = torch.topk(scaled_logits, k=k, dim=-1)  # [batch_size, k]

                    # Step 3: æ£€æŸ¥UNKæ˜¯å¦åœ¨top-kä¸­
                    is_unk_in_topk = (topk_indices == self.unk_idx).any(dim=-1)  # [batch_size]

                    # Step 4: UNKå¼ºæŠ‘åˆ¶ - å¦‚æœé¢„æµ‹ç»“æœçš„Top-1æ˜¯UNKï¼Œå¼ºåˆ¶é™ä½å…¶æ¦‚ç‡
                    top1_indices = torch.argmax(scaled_logits, dim=-1)  # [batch_size]
                    is_top1_unk = (top1_indices == self.unk_idx)

                    # ä¸ºæ¯ä¸ªæ ·æœ¬å•ç‹¬å¤„ç†
                    final_tokens = []
                    for i in range(batch_size):
                        if is_top1_unk[i]:
                            # UNKå¼ºåŠ›æƒ©ç½šï¼šå°†UNKæ¦‚ç‡é™ä½90%å¹¶é‡æ–°å½’ä¸€åŒ–
                            current_logits = scaled_logits[i].clone()
                            unk_idx = self.unk_idx

                            # é™ä½UNKæƒé‡90%
                            current_logits[unk_idx] = current_logits[unk_idx] - 10.0  # ğŸ”¥ è¶…å¼ºæƒ©ç½šï¼šlog(0.000045) â‰ˆ -10.0

                            # ä»å‰©ä½™çš„top-10ä¸­é€‰æ‹©ï¼ˆæ’é™¤UNKï¼‰
                            topk_vals, topk_idxs = torch.topk(current_logits, k=k)

                            # å¦‚æœUNKè¿˜åœ¨top-10ä¸­ï¼Œå»æ‰å®ƒå¹¶å–ä¸‹ä¸€ä¸ª
                            if unk_idx in topk_idxs:
                                topk_mask = topk_idxs != unk_idx
                                topk_vals = topk_vals[topk_mask][:9]  # å–å‰9ä¸ªéUNK
                                topk_idxs = topk_idxs[topk_mask][:9]

                            # ä»top-kä¸­éšæœºé‡‡æ ·
                            probs = F.softmax(topk_vals, dim=0)
                            selected_idx = torch.multinomial(probs, num_samples=1)
                            next_token_single = topk_idxs[selected_idx].unsqueeze(0)
                        else:
                            # Top-1ä¸æ˜¯UNKï¼Œä½¿ç”¨æ ‡å‡†Top-ké‡‡æ ·
                            topk_vals_i = topk_values[i]
                            topk_idxs_i = topk_indices[i]

                            # å¦‚æœUNKåœ¨top-kä¸­ï¼Œé™ä½å…¶æ¦‚ç‡æƒé‡
                            if is_unk_in_topk[i]:
                                unk_mask = topk_idxs_i == self.unk_idx
                                topk_vals_i = topk_vals_i.clone()
                                topk_vals_i[unk_mask] = topk_vals_i[unk_mask] - 5.0  # ğŸ”¥ å¼ºåŒ–UNKæƒ©ç½š

                            # ä»top-kä¸­é‡‡æ ·
                            probs = F.softmax(topk_vals_i, dim=0)
                            selected_idx = torch.multinomial(probs, num_samples=1)
                            next_token_single = topk_idxs_i[selected_idx].unsqueeze(0)

                        final_tokens.append(next_token_single)

                    next_token = torch.cat(final_tokens, dim=0).unsqueeze(1)  # [batch_size, 1]

                # ğŸ”¥ ç¡®ä¿next_tokenç»´åº¦æ­£ç¡®ï¼šåº”è¯¥æ˜¯[batch_size, 1]
                if next_token.dim() == 1:
                    next_token = next_token.unsqueeze(1)  # [batch_size] -> [batch_size, 1]
                elif next_token.dim() == 3:
                    next_token = next_token.squeeze(-1)  # [batch_size, 1, 1] -> [batch_size, 1]

                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç›´æ¥å°†å½“å‰é‡‡æ ·tokenæ·»åŠ åˆ°sampled_idsåˆ—è¡¨ï¼
                sampled_ids.append(next_token.squeeze(1))  # [batch_size, 1] -> [batch_size]

                input_step = next_token

                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åºåˆ—éƒ½å·²ç”Ÿæˆ EOS
                if (next_token == self.eos_idx).all():  # åŠ¨æ€EOS
                    break

        if target_seq is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šè¿”å›æ¦‚ç‡åˆ†å¸ƒç”¨äºæŸå¤±è®¡ç®—
            outputs = torch.stack(outputs, dim=1)  # [batch_size, seq_len, vocab_size]
            return outputs
        else:
            # ğŸ”¥ ç”Ÿæˆæ¨¡å¼ï¼šç›´æ¥è¿”å›åœ¨å¾ªç¯ä¸­æ”¶é›†çš„sampled_idsï¼Œç¡®ä¿é€»è¾‘ä¸€è‡´æ€§ï¼
            # sampled_idså·²åŒ…å«æ¯ä¸ªæ—¶é—´æ­¥çš„é‡‡æ ·token: [batch_size] for each step
            sampled_token_ids = torch.stack(sampled_ids, dim=1)  # [batch_size, seq_len]
            return sampled_token_ids


class CVAE(nn.Module):
    """æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ (CVAE)

    å®Œæ•´çš„ CVAE æ¨¡å‹ï¼Œç»“åˆç¼–ç å™¨å’Œè§£ç å™¨
    """

    def __init__(self, vocab_size: int, embed_dim: int = 128, hidden_dim: int = 256,
                 latent_dim: int = 32, condition_dim: int = 6, num_layers: int = 2,
                 vocab_info: Dict[str, int] = None):
        super(CVAE, self).__init__()

        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        self.condition_dim = condition_dim
        self.num_layers = num_layers
        self.vocab_info = vocab_info  # ğŸ”¥ ä¿å­˜è¯è¡¨ä¿¡æ¯

        # ç¼–ç å™¨å’Œè§£ç å™¨
        self.encoder = CVAREncoder(
            vocab_size=vocab_size,
            embed_dim=embed_dim,
            hidden_dim=hidden_dim,
            latent_dim=latent_dim,
            condition_dim=condition_dim,
            num_layers=num_layers
        )

        self.decoder = CVARDecoder(
            vocab_size=vocab_size,
            embed_dim=embed_dim,
            hidden_dim=hidden_dim,
            latent_dim=latent_dim,
            condition_dim=condition_dim,
            num_layers=num_layers,
            vocab_info=vocab_info  # ğŸ”¥ ä¼ é€’è¯è¡¨ä¿¡æ¯
        )

    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        """é‡å‚æ•°åŒ–æŠ€å·§

        Args:
            mu: [batch_size, latent_dim] å‡å€¼
            logvar: [batch_size, latent_dim] å¯¹æ•°æ–¹å·®

        Returns:
            [batch_size, latent_dim] é‡‡æ ·çš„éšå˜é‡
        """
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x: torch.Tensor, c: torch.Tensor,
                target_seq: Optional[torch.Tensor] = None,
                max_length: int = 150, temperature: float = 1.0) -> Dict[str, torch.Tensor]:
        """
        Args:
            x: [batch_size, seq_len] è¾“å…¥åºåˆ—
            c: [batch_size] æ¡ä»¶æ ‡ç­¾
            target_seq: [batch_size, seq_len] ç›®æ ‡åºåˆ—
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: Gumbel-Softmax æ¸©åº¦å‚æ•°

        Returns:
            åŒ…å«å„ç§è¾“å‡ºçš„å­—å…¸
        """
        # ç¼–ç 
        mu, logvar = self.encoder(x, c)

        # é‡å‚æ•°åŒ–é‡‡æ ·
        z = self.reparameterize(mu, logvar)

        # è§£ç 
        decoder_output = self.decoder(
            z=z,
            c=c,
            target_seq=target_seq,
            max_length=max_length,
            temperature=temperature
        )

        return {
            'decoder_output': decoder_output,  # [batch_size, seq_len, vocab_size]
            'mu': mu,  # [batch_size, latent_dim]
            'logvar': logvar,  # [batch_size, latent_dim]
            'z': z,  # [batch_size, latent_dim]
        }

    def generate(self, c: torch.Tensor, num_samples: int = 1,
                 max_length: int = 150, temperature: float = 1.8) -> torch.Tensor:
        """ç”Ÿæˆæ ·æœ¬

        Args:
            c: [batch_size] æ¡ä»¶æ ‡ç­¾
            num_samples: æ¯ä¸ªæ¡ä»¶ç”Ÿæˆçš„æ ·æœ¬æ•°
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦å‚æ•°ï¼Œæé«˜é»˜è®¤å€¼å¢åŠ éšæœºæ€§

        Returns:
            [batch_size, num_samples, max_length] ç”Ÿæˆçš„åºåˆ—
        """
        self.eval()
        with torch.no_grad():
            batch_size = c.size(0)
            device = c.device

            # ğŸ”¥ ä»å…ˆéªŒåˆ†å¸ƒé‡‡æ ·éšå˜é‡ï¼Œå¢åŠ é‡‡æ ·æ¬¡æ•°æ‰¾æ›´å¥½çš„z
            best_samples = []
            for _ in range(3):  # æ¯ä¸ªæ¡ä»¶ç”Ÿæˆ3æ¬¡ï¼Œé€‰æœ€å¥½çš„
                z = torch.randn(batch_size * num_samples, self.latent_dim, device=device)
                c_expanded = c.unsqueeze(1).repeat(1, num_samples).flatten()

                # ğŸ”¥ ç”Ÿæˆåºåˆ—ï¼Œä½¿ç”¨æ›´é«˜çš„æ¸©åº¦
                # ğŸ”¥ ç°åœ¨decoderç›´æ¥è¿”å›token_idsï¼Œä¸éœ€è¦ä»æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ï¼
                sampled_token_ids = self.decoder(
                    z=z,
                    c=c_expanded,
                    target_seq=None,
                    max_length=max_length,
                    temperature=temperature * 1.2  # è¿›ä¸€æ­¥æé«˜æ¸©åº¦
                )

                # ğŸ”¥ ç›´æ¥ä½¿ç”¨è¿”å›çš„token_idsï¼Œé‡å¡‘ä¸ºæ­£ç¡®ç»´åº¦
                # sampled_token_ids: [batch_size * num_samples, seq_len] -> [batch_size, num_samples, seq_len]
                generated_tokens = sampled_token_ids.view(batch_size, num_samples, -1)

                best_samples.append(generated_tokens)

            # ğŸ”¥ ä»å¤šæ¬¡é‡‡æ ·ä¸­é€‰æ‹©UNKæœ€å°‘çš„æ ·æœ¬ - ä½¿ç”¨åŠ¨æ€UNKç´¢å¼•
            final_samples = []
            unk_idx = self.decoder.unk_idx  # ğŸ”¥ ç›´æ¥ä½¿ç”¨decoderçš„åŠ¨æ€UNKç´¢å¼•

            for i in range(batch_size):
                best_idx = 0
                min_unk_count = float('inf')

                for j, samples in enumerate(best_samples):
                    unk_count = (samples[i] == unk_idx).sum().item()  # ä½¿ç”¨åŠ¨æ€UNKç´¢å¼•
                    if unk_count < min_unk_count:
                        min_unk_count = unk_count
                        best_idx = j

                final_samples.append(best_samples[best_idx][i])

            # åˆå¹¶æœ€ç»ˆç»“æœ
            generated_tokens = torch.stack(final_samples, dim=0)
            return generated_tokens

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        return {
            'model_type': 'Seq2Seq CVAE',
            'vocab_size': self.vocab_size,
            'embed_dim': self.embed_dim,
            'hidden_dim': self.hidden_dim,
            'latent_dim': self.latent_dim,
            'condition_dim': self.condition_dim,
            'num_layers': self.num_layers,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params
        }