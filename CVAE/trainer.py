#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CVAE è®­ç»ƒå™¨
===========

å®ç°å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€æ¨¡å‹ä¿å­˜å’ŒæŒ‡æ ‡ç›‘æ§
æ”¯æŒé€šè¿‡ python main.py --train è§¦å‘

ä½œè€…ï¼šè€ç‹ (æš´èºæŠ€æœ¯æµ)
ç‰ˆæœ¬ï¼š1.0
æ—¥æœŸï¼š2025-12-18
"""

import torch
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
import time
import os
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import logging
from tqdm import tqdm

from cvae_model import CVAE
from training_utils import CVAELoss, CyclicalAnnealingSchedule, CVAEMetrics
from data_loader import create_data_loaders
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.logger import create_logger


class CVAETrainer:
    """CVAE è®­ç»ƒå™¨

    å®ç°å®Œæ•´çš„è®­ç»ƒæµç¨‹å’Œæ¨¡å‹ç®¡ç†
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Args:
            config: è®­ç»ƒé…ç½®å­—å…¸
        """
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # ğŸ”¥ åˆå§‹åŒ–è€ç‹ç‰Œè®­ç»ƒæ—¥å¿—è®°å½•å™¨
        log_dir = os.path.join(self.config.get('output_dir', 'CVAE/checkpoints'), 'logs')
        self.training_logger = create_logger(log_dir=log_dir, simple=False)

        # è®¾ç½®åŸæœ‰çš„åŸºç¡€æ—¥å¿—
        self._setup_logging()

        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.loss_fn = None
        self.kl_scheduler = None
        self.metrics = None
        self.scaler = None

        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.current_step = 0
        self.best_val_loss = float('inf')
        self.training_history = []

        # åˆå§‹åŒ–
        self._initialize()

        # ğŸ”¥ è®°å½•è®­ç»ƒé…ç½®åˆ°è¯¦ç»†æ—¥å¿—
        self.training_logger.log_training_config(self.config)

        print(f"ğŸ¯ CVAE è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“± è®¾å¤‡ï¼š{self.device}")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°ï¼š{sum(p.numel() for p in self.model.parameters()):,}")
        print(f"ğŸ“ è¯¦ç»†æ—¥å¿—ï¼š{self.training_logger.log_file}")

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_dir = Path(self.config['output_dir']) / 'logs'
        log_dir.mkdir(parents=True, exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / 'training.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def _initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        # åŠ è½½æ•°æ®
        self.train_loader, self.val_loader, self.vocab_info = create_data_loaders(
            data_path=self.config['data_path'],
            vocab_path=self.config['vocab_path'],
            batch_size=self.config['batch_size'],
            train_split=self.config['train_split'],
            oversample=self.config['oversample'],
            num_workers=self.config['num_workers'],
            random_state=self.config['random_state']
        )

        # ğŸ”¥ åˆ›å»ºæ¨¡å‹ï¼ˆä¼ é€’è¯è¡¨ä¿¡æ¯ï¼‰
        self.model = CVAE(
            vocab_size=self.vocab_info['vocab_size'],
            embed_dim=self.config['embed_dim'],
            hidden_dim=self.config['hidden_dim'],
            latent_dim=self.config['latent_dim'],
            condition_dim=self.config['condition_dim'],
            num_layers=self.config['num_layers'],
            vocab_info=self.vocab_info  # ğŸ”¥ ä¼ é€’å®Œæ•´è¯è¡¨ä¿¡æ¯
        ).to(self.device)

        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=self.config['epochs'],
            eta_min=self.config['learning_rate'] * 0.1
        )

        # æŸå¤±å‡½æ•°
        self.loss_fn = CVAELoss()

        # ğŸ”¥ KL é€€ç«è°ƒåº¦å™¨ï¼ˆå®Œå…¨é…ç½®é©±åŠ¨ç‰ˆæœ¬ï¼‰
        total_steps = len(self.train_loader) * self.config['epochs']
        steps_per_epoch = len(self.train_loader)

        # ğŸ”¥ æ„å»ºKLé€€ç«é…ç½®å­—å…¸ - ä»é…ç½®ä¸­è¯»å–ï¼Œä¸å†ç¡¬ç¼–ç ï¼
        kl_config = {
            'total_steps': total_steps,
            'n_cycles': self.config.get('kl_cycles', 1),  # ğŸ”¥ ä»é…ç½®è¯»å–ï¼Œé»˜è®¤1
            'ratio': self.config.get('kl_ratio', 0.6),  # ğŸ”¥ ä»é…ç½®è¯»å–ï¼Œé»˜è®¤0.6
            'beta_max': self.config.get('beta_max', 0.25),  # ğŸ”¥ ä»é…ç½®è¯»å–ï¼Œé»˜è®¤0.25
            'delay_epochs': self.config.get('delay_epochs', 20),  # ğŸ”¥ ä»é…ç½®è¯»å–ï¼Œé»˜è®¤20
            'steps_per_epoch': steps_per_epoch
        }

        self.kl_scheduler = CyclicalAnnealingSchedule(kl_config)

        # ğŸ”¥ æŒ‡æ ‡è®¡ç®—å™¨ï¼ˆä¼ é€’å®Œæ•´çš„ç‰¹æ®Štokenç´¢å¼•ï¼‰
        special_tokens = self.vocab_info['special_tokens']
        self.metrics = CVAEMetrics(
            vocab=self.vocab_info['char_to_idx'],
            pad_idx=special_tokens.get('<PAD>', 2),
            sos_idx=special_tokens.get('<SOS>', 0),
            eos_idx=special_tokens.get('<EOS>', 1),
            unk_idx=special_tokens.get('<UNK>', 3)  # ğŸ”¥ ä¼ é€’UNKç´¢å¼•
        )

        # ğŸ”¥ ä¿®å¤æ··åˆç²¾åº¦è®­ç»ƒçš„åºŸå¼ƒè­¦å‘Š
        if self.config.get('use_amp', True) and self.device.type == 'cuda':
            from torch.amp import GradScaler, autocast
            self.scaler = GradScaler('cuda')
            self.logger.info("ğŸ”¥ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
        else:
            self.scaler = None

    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ª epoch"""
        self.model.train()
        epoch_loss = 0.0
        epoch_recon_loss = 0.0
        epoch_kl_loss = 0.0
        epoch_recon_acc = 0.0
        epoch_validity = 0.0

        # è¿›åº¦æ¡ - è€ç‹æˆ‘åŠ ä¸Šå¹³æ»‘æ˜¾ç¤ºå’Œmininterval
        pbar = tqdm(self.train_loader, desc=f"Epoch {self.current_epoch + 1}",
                   mininterval=0.1, smoothing=0.1)

        try:
            for batch_idx, (sequences, labels) in enumerate(pbar):
                sequences = sequences.to(self.device)
                labels = labels.to(self.device)

                # æ¸…é›¶æ¢¯åº¦
                self.optimizer.zero_grad()

                # ğŸ”¥ ä¿®å¤autocastå¯¼å…¥å’Œå‰å‘ä¼ æ’­
                if self.scaler is not None:
                    from torch.amp import autocast
                    with autocast('cuda', enabled=self.scaler is not None):
                        # ä½¿ç”¨åºåˆ—è‡ªèº«ä½œä¸ºç›®æ ‡ (è®­ç»ƒæ—¶)
                        outputs = self.model(
                            x=sequences,
                            c=labels,
                            target_seq=sequences,
                            temperature=self.config.get('temperature', 1.0)
                        )

                        # è·å–å½“å‰ beta å€¼
                        beta = self.kl_scheduler.get_beta(self.current_step)

                        # è®¡ç®—æŸå¤±
                        loss_dict = self.loss_fn(
                            decoder_output=outputs['decoder_output'],
                            target=sequences,
                            mu=outputs['mu'],
                            logvar=outputs['logvar'],
                            beta=beta
                        )

                        loss = loss_dict['total_loss']
                else:
                    # ä¸ä½¿ç”¨æ··åˆç²¾åº¦æ—¶
                    # ä½¿ç”¨åºåˆ—è‡ªèº«ä½œä¸ºç›®æ ‡ (è®­ç»ƒæ—¶)
                    outputs = self.model(
                        x=sequences,
                        c=labels,
                        target_seq=sequences,
                        temperature=self.config.get('temperature', 1.0)
                    )

                    # è·å–å½“å‰ beta å€¼
                    beta = self.kl_scheduler.get_beta(self.current_step)

                    # è®¡ç®—æŸå¤±
                    loss_dict = self.loss_fn(
                        decoder_output=outputs['decoder_output'],
                        target=sequences,
                        mu=outputs['mu'],
                        logvar=outputs['logvar'],
                        beta=beta
                    )

                    loss = loss_dict['total_loss']

                # åå‘ä¼ æ’­
                if self.scaler is not None:
                    self.scaler.scale(loss).backward()
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    loss.backward()
                    self.optimizer.step()

                # æ›´æ–°ç»Ÿè®¡
                batch_size = sequences.size(0)
                epoch_loss += loss.item() * batch_size
                epoch_recon_loss += loss_dict['recon_loss'].item() * batch_size
                epoch_kl_loss += loss_dict['kl_loss'].item() * batch_size

                # è®¡ç®—æŒ‡æ ‡
                if batch_idx % self.config.get('metric_interval', 10) == 0:
                    with torch.no_grad():
                        metrics_dict = self.metrics.calculate_metrics(
                            decoder_output=outputs['decoder_output'],
                            target=sequences
                        )
                        epoch_recon_acc += metrics_dict['reconstruction_accuracy'] * batch_size
                        epoch_validity += metrics_dict['validity_rate'] * batch_size

                # ğŸ”¥ æ›´æ–°è¿›åº¦æ¡ï¼Œæ˜¾ç¤ºå…³é”®æŒ‡æ ‡
                pbar.set_postfix({
                    'Loss': f"{loss.item():.4f}",
                    'Beta': f"{beta:.3f}",
                    'Recon': f"{loss_dict['recon_loss'].item():.4f}",
                    'KL': f"{loss_dict['kl_loss'].item():.4f}"
                })

                self.current_step += 1

        except Exception as e:
            # ğŸ”¥ è®°å½•è®­ç»ƒé”™è¯¯åˆ°è€ç‹æ—¥å¿—
            error_msg = f"Epoch {self.current_epoch + 1} è®­ç»ƒå‡ºé”™: {str(e)}"
            self.training_logger.log_error(error_msg, e)
            self.logger.error(error_msg)
            raise e

        # è®¡ç®—å¹³å‡å€¼
        num_samples = len(self.train_loader.dataset)
        avg_loss = epoch_loss / num_samples
        avg_recon_loss = epoch_recon_loss / num_samples
        avg_kl_loss = epoch_kl_loss / num_samples
        avg_recon_acc = epoch_recon_acc / (len(self.train_loader) // self.config.get('metric_interval', 10) * self.config['batch_size'])
        avg_validity = epoch_validity / (len(self.train_loader) // self.config.get('metric_interval', 10) * self.config['batch_size'])

        return {
            'loss': avg_loss,
            'recon_loss': avg_recon_loss,
            'kl_loss': avg_kl_loss,
            'recon_accuracy': avg_recon_acc,
            'validity_rate': avg_validity,
            'learning_rate': self.optimizer.param_groups[0]['lr']
        }

    def validate(self) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹ - åŒ…å«Teacher Forcingå’ŒNon-Teacher-Forcingæµ‹è¯•"""
        self.model.eval()
        val_loss = 0.0
        val_recon_loss = 0.0
        val_kl_loss = 0.0
        val_recon_acc = 0.0
        val_validity = 0.0

        # ğŸ”¥ æ–°å¢ï¼šç”Ÿæˆæµ‹è¯•æŒ‡æ ‡
        val_gen_acc = 0.0
        val_gen_validity = 0.0

        with torch.no_grad():
            for sequences, labels in tqdm(self.val_loader, desc="Validation"):
                sequences = sequences.to(self.device)
                labels = labels.to(self.device)

                # === 1. Teacher Forcing éªŒè¯ï¼ˆé‡æ„æµ‹è¯•ï¼‰ ===
                tf_outputs = self.model(
                    x=sequences,
                    c=labels,
                    target_seq=sequences,  # Teacher Forcingæ¨¡å¼
                    temperature=self.config.get('temperature', 1.0)
                )

                # ä½¿ç”¨ beta = 1.0 è¿›è¡ŒéªŒè¯
                beta = 1.0
                tf_loss_dict = self.loss_fn(
                    decoder_output=tf_outputs['decoder_output'],
                    target=sequences,
                    mu=tf_outputs['mu'],
                    logvar=tf_outputs['logvar'],
                    beta=beta
                )

                # æ›´æ–°Teacher Forcingç»Ÿè®¡
                batch_size = sequences.size(0)
                val_loss += tf_loss_dict['total_loss'].item() * batch_size
                val_recon_loss += tf_loss_dict['recon_loss'].item() * batch_size
                val_kl_loss += tf_loss_dict['kl_loss'].item() * batch_size

                # è®¡ç®—Teacher ForcingæŒ‡æ ‡
                tf_metrics = self.metrics.calculate_metrics(
                    decoder_output=tf_outputs['decoder_output'],
                    target=sequences
                )
                val_recon_acc += tf_metrics['reconstruction_accuracy'] * batch_size
                val_validity += tf_metrics['validity_rate'] * batch_size

                # === 2. Non-Teacher-Forcing éªŒè¯ï¼ˆç”Ÿæˆæµ‹è¯•ï¼‰ ===
                gen_outputs = self.model(
                    x=sequences,
                    c=labels,
                    target_seq=None,  # ğŸ”¥ å…³é”®ï¼šä¸ç»™ç›®æ ‡åºåˆ—ï¼Œå¼ºåˆ¶è‡ªå›å½’ç”Ÿæˆ
                    max_length=sequences.size(1),
                    temperature=self.config.get('temperature', 1.0)
                )

                # ğŸ”¥ è®¡ç®—ç”ŸæˆæŒ‡æ ‡ï¼ˆçœŸå®è¯„ä¼°æ¨¡å¼ï¼Œä¸ä½¿ç”¨visual_modeï¼‰
                gen_metrics = self.metrics.calculate_metrics(
                    decoder_output=gen_outputs['decoder_output'],
                    target=sequences
                )
                val_gen_acc += gen_metrics['reconstruction_accuracy'] * batch_size
                val_gen_validity += gen_metrics['validity_rate'] * batch_size

        # è®¡ç®—å¹³å‡å€¼
        num_samples = len(self.val_loader.dataset)
        avg_loss = val_loss / num_samples
        avg_recon_loss = val_recon_loss / num_samples
        avg_kl_loss = val_kl_loss / num_samples
        avg_recon_acc = val_recon_acc / num_samples
        avg_validity = val_validity / num_samples
        avg_gen_acc = val_gen_acc / num_samples
        avg_gen_validity = val_gen_validity / num_samples

        # ğŸ”¥ è¿”å›æ‰©å±•çš„éªŒè¯æŒ‡æ ‡
        return {
            'val_loss': avg_loss,
            'val_recon_loss': avg_recon_loss,
            'val_kl_loss': avg_kl_loss,
            'val_recon_accuracy': avg_recon_acc,
            'val_validity_rate': avg_validity,
            # ğŸ”¥ æ–°å¢ï¼šç”Ÿæˆæµ‹è¯•æŒ‡æ ‡
            'val_gen_accuracy': avg_gen_acc,
            'val_gen_validity_rate': avg_gen_validity,
            'teacher_forcing_gap': avg_recon_acc - avg_gen_acc  # Teacher Forcingä¾èµ–åº¦æŒ‡æ ‡
        }

    def save_checkpoint(self, is_best: bool = False):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.current_epoch,
            'step': self.current_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config,
            'vocab_info': self.vocab_info,
            'training_history': self.training_history
        }

        if self.scaler is not None:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()

        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = Path(self.config['output_dir']) / 'checkpoint_latest.pth'
        torch.save(checkpoint, checkpoint_path)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = Path(self.config['output_dir']) / 'cvae.pth'
            torch.save(checkpoint, best_path)
            self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼š{best_path}")

    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.current_epoch = checkpoint['epoch']
        self.current_step = checkpoint['step']
        self.best_val_loss = checkpoint['best_val_loss']
        self.training_history = checkpoint.get('training_history', [])

        if self.scaler is not None and 'scaler_state_dict' in checkpoint:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])

        self.logger.info(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹ï¼š{checkpoint_path}")
        self.logger.info(f"ğŸ¯ æ¢å¤åˆ° epoch {self.current_epoch}, step {self.current_step}")

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        self.logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ CVAE æ¨¡å‹")
        self.logger.info(f"ğŸ“Š è®­ç»ƒé…ç½®ï¼š{self.config}")

        # ğŸ”¥ è®°å½•è®­ç»ƒå¼€å§‹åˆ°è€ç‹æ—¥å¿—
        self.training_logger.log_info("å¼€å§‹CVAEæ¨¡å‹è®­ç»ƒ")

        # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
        start_time = time.time()

        for epoch in range(self.current_epoch, self.config['epochs']):
            self.current_epoch = epoch

            # ğŸ”¥ è®°å½•epochå¼€å§‹
            self.training_logger.log_epoch_start(epoch, self.config['epochs'])

            # è®­ç»ƒä¸€ä¸ª epoch
            train_metrics = self.train_epoch()

            # éªŒè¯
            val_metrics = self.validate()

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()

            # æ›´æ–°æœ€ä½³éªŒè¯æŸå¤±
            val_loss = val_metrics['val_loss']
            is_best = val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = val_loss

            # ğŸ”¥ è®°å½•epochæŒ‡æ ‡åˆ°è€ç‹æ—¥å¿—
            combined_metrics = {
                **train_metrics,
                **val_metrics,
                'beta': self.kl_scheduler.get_beta(self.current_step),
                'lr': self.optimizer.param_groups[0]['lr']
            }
            self.training_logger.log_epoch_metrics(epoch, combined_metrics)

            # è®°å½•è®­ç»ƒå†å²
            epoch_record = {
                'epoch': epoch + 1,
                'train_metrics': train_metrics,
                'val_metrics': val_metrics,
                'beta': self.kl_scheduler.get_beta(self.current_step),
                'lr': self.optimizer.param_groups[0]['lr']
            }
            self.training_history.append(epoch_record)

            # ğŸ”¥ å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œè®°å½•åˆ°è€ç‹æ—¥å¿—
            if is_best:
                self.training_logger.log_best_model(epoch, val_metrics)

            # ğŸ”¥ ç”Ÿæˆè°ƒè¯•æ ·æœ¬å¹¶è®°å½•åˆ°è€ç‹æ—¥å¿—
            generation_samples = self.get_debug_samples_dict(num_samples=5, max_length=50, temperature=1.5)
            self.training_logger.log_generation_samples(epoch, generation_samples)

            # ğŸ”¥ æ‰“å°è®­ç»ƒä¿¡æ¯ï¼ˆåŒ…å«ç”Ÿæˆæµ‹è¯•æŒ‡æ ‡ï¼‰
            print(f"\nğŸ“ˆ Epoch {epoch + 1}/{self.config['epochs']} å®Œæˆ")
            print(f"è®­ç»ƒæŸå¤±: {train_metrics['loss']:.4f}, é‡æ„æŸå¤±: {train_metrics['recon_loss']:.4f}, KLæŸå¤±: {train_metrics['kl_loss']:.4f}")
            print(f"éªŒè¯æŸå¤±: {val_metrics['val_loss']:.4f}")
            print(f"ğŸ“Š Teacher Forcing: é‡æ„å‡†ç¡®ç‡={val_metrics['val_recon_accuracy']:.4f}, æœ‰æ•ˆç‡={val_metrics['val_validity_rate']:.4f}")
            print(f"ğŸ² çœŸå®ç”Ÿæˆ: å‡†ç¡®ç‡={val_metrics['val_gen_accuracy']:.4f}, æœ‰æ•ˆç‡={val_metrics['val_gen_validity_rate']:.4f}")
            print(f"âš ï¸  Teacher-Forcingä¾èµ–åº¦: {val_metrics['teacher_forcing_gap']:.4f} (è¶Šå°è¶Šå¥½)")
            print(f"å­¦ä¹ ç‡: {train_metrics['learning_rate']:.6f}, Beta: {epoch_record['beta']:.3f}")

            # ğŸ”¥ ç”Ÿæˆè°ƒè¯•æ ·æœ¬ï¼ˆæ¯ä¸ªepochç»“æŸæ—¶å¼ºåˆ¶æ‰“å°å‰5ä¸ªç”Ÿæˆæ ·æœ¬ï¼‰
            print(f"\nğŸ² ç”Ÿæˆè°ƒè¯•æ ·æœ¬ï¼ˆEpoch {epoch + 1}ï¼‰ï¼š")
            self.debug_generate_samples(num_samples=5, max_length=50, temperature=1.5)

            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(is_best)

            # æ—©åœæ£€æŸ¥
            if self.config.get('early_stopping', False):
                patience = self.config.get('patience', 10)
                if len(self.training_history) > patience:
                    recent_losses = [h['val_metrics']['val_loss'] for h in self.training_history[-patience:]]
                    if all(recent_losses[i] >= recent_losses[i-1] for i in range(1, len(recent_losses))):
                        self.logger.info(f"â° æ—©åœè§¦å‘ï¼Œåœ¨ epoch {epoch + 1}")
                        self.training_logger.log_info(f"æ—©åœè§¦å‘ï¼Œåœ¨ epoch {epoch + 1}")
                        break

        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        self.logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ€»æ—¶é—´ï¼š{total_time:.2f} ç§’")
        self.logger.info(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±ï¼š{self.best_val_loss:.4f}")

        # ğŸ”¥ è®°å½•è®­ç»ƒå®Œæˆåˆ°è€ç‹æ—¥å¿—
        self.training_logger.log_training_complete(self.config['epochs'])

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = Path(self.config['output_dir']) / 'cvae_final.pth'
        torch.save(self.model.state_dict(), final_path)
        self.logger.info(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ï¼š{final_path}")

        return self.training_history

    def generate_samples(self, num_samples: int = 10, max_length: int = 150) -> None:
        """ç”Ÿæˆæ ·æœ¬ç”¨äºæµ‹è¯•"""
        self.model.eval()

        # ä»éªŒè¯é›†ä¸­è·å–æ¡ä»¶æ ‡ç­¾
        all_labels = []
        for _, labels in self.val_loader:
            all_labels.extend(labels.tolist())
            if len(all_labels) >= num_samples:
                break

        all_labels = torch.tensor(all_labels[:num_samples]).to(self.device)

        with torch.no_grad():
            generated = self.model.generate(
                c=all_labels,
                num_samples=1,
                max_length=max_length,
                temperature=1.0
            )

        # è§£ç å¹¶æ‰“å°ç”Ÿæˆçš„æ ·æœ¬
        print("\nğŸ² ç”Ÿæˆçš„æ ·æœ¬ï¼š")
        class_names = ['SQLi', 'XSS', 'CMDi', 'Overflow', 'XXE', 'SSI']

        for i in range(num_samples):
            label_idx = int(all_labels[i])
            class_name = class_names[label_idx] if label_idx < len(class_names) else f"Class_{label_idx}"

            # è§£ç åºåˆ—
            sequence = generated[i, 0]  # [seq_len]
            decoded = self.metrics.decode_sequence(sequence)

            print(f"({i+1}) [{class_name}]: {decoded}")

    def debug_generate_samples(self, num_samples: int = 5, max_length: int = 50, temperature: float = 1.5):
        """ç”Ÿæˆè°ƒè¯•æ ·æœ¬ç”¨äºè§‚å¯Ÿæ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ

        å¼ºåˆ¶åœ¨æ¯ä¸ªepochç»“æŸåç”Ÿæˆæ ·æœ¬ï¼Œè§‚å¯Ÿæ˜¯å¦å‡ºç°é‡å¤ä¹±ç 

        Args:
            num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
        """
        self.model.eval()

        # ä½¿ç”¨å¤šç§æ”»å‡»ç±»å‹è¿›è¡Œç”Ÿæˆæµ‹è¯•
        test_labels = torch.tensor([0, 1, 2, 3, 4], dtype=torch.long, device=self.device)  # SQLi, XSS, CMDi, Overflow, XXE

        with torch.no_grad():
            for i in range(min(num_samples, len(test_labels))):
                label = test_labels[i:i+1]  # ä¿æŒ2Då½¢çŠ¶ [1, 1]

                # ğŸ”¥ ç”Ÿæˆæ ·æœ¬ï¼ˆä½¿ç”¨æ›´é«˜çš„æ¸©åº¦å‚æ•°å¢åŠ éšæœºæ€§ï¼‰
                generated = self.model.generate(
                    c=label,
                    num_samples=1,
                    max_length=max_length,
                    temperature=temperature
                )

                # è§£ç åºåˆ—ï¼ˆä½¿ç”¨visual_modeä¾¿äºè§‚å¯Ÿï¼‰
                sequence = generated[0, 0]  # [seq_len]
                decoded = self.metrics.decode_sequence(sequence, visual_mode=True)  # ğŸ”¥ è§†è§‰æ¨¡å¼

                # è·å–æ”»å‡»ç±»å‹åç§°
                class_names = ['SQLi', 'XSS', 'CMDi', 'Overflow', 'XXE', 'SSI']
                class_name = class_names[int(label.item())] if int(label.item()) < len(class_names) else f"Class_{int(label.item())}"

                # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤å­—ç¬¦æˆ–ä¹±ç 
                is_repetitive = len(set(decoded)) < 3 if len(decoded) > 5 else False
                is_empty = len(decoded.strip()) == 0

                status = ""
                if is_repetitive:
                    status = " [é‡å¤å­—ç¬¦]"
                elif is_empty:
                    status = " [ç©ºè¾“å‡º]"

                print(f"  æ ·æœ¬{i+1} [{class_name}]: '{decoded}'{status}")

    def get_debug_samples_dict(self, num_samples: int = 5, max_length: int = 50, temperature: float = 1.5) -> Dict[str, str]:
        """è·å–è°ƒè¯•æ ·æœ¬å­—å…¸ï¼Œç”¨äºæ—¥å¿—è®°å½•

        Args:
            num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦

        Returns:
            æ ·æœ¬å­—å…¸ {æ”»å‡»ç±»å‹: ç”Ÿæˆæ ·æœ¬}
        """
        self.model.eval()
        samples_dict = {}

        # ä½¿ç”¨å¤šç§æ”»å‡»ç±»å‹è¿›è¡Œç”Ÿæˆæµ‹è¯•
        test_labels = torch.tensor([0, 1, 2, 3, 4], dtype=torch.long, device=self.device)  # SQLi, XSS, CMDi, Overflow, XXE
        class_names = ['SQLi', 'XSS', 'CMDi', 'Overflow', 'XXE', 'SSI']

        with torch.no_grad():
            for i in range(min(num_samples, len(test_labels))):
                label = test_labels[i:i+1]  # ä¿æŒ2Då½¢çŠ¶ [1, 1]

                # ç”Ÿæˆæ ·æœ¬
                generated = self.model.generate(
                    c=label,
                    num_samples=1,
                    max_length=max_length,
                    temperature=temperature
                )

                # è§£ç åºåˆ—ï¼ˆä½¿ç”¨visual_modeä¾¿äºè§‚å¯Ÿï¼‰
                sequence = generated[0, 0]  # [seq_len]
                decoded = self.metrics.decode_sequence(sequence, visual_mode=True)  # è§†è§‰æ¨¡å¼

                # è·å–æ”»å‡»ç±»å‹åç§°
                class_name = class_names[int(label.item())] if int(label.item()) < len(class_names) else f"Class_{int(label.item())}"

                # è®°å½•åˆ°å­—å…¸
                samples_dict[class_name] = decoded

        return samples_dict


def create_default_config() -> Dict[str, Any]:
    """åˆ›å»ºé»˜è®¤è®­ç»ƒé…ç½®"""
    return {
        # æ•°æ®é…ç½®
        'data_path': 'Data/processed/processed_data.pt',
        'vocab_path': 'Data/processed/vocab.json',
        'output_dir': 'CVAE/checkpoints',

        # æ¨¡å‹é…ç½®
        'embed_dim': 128,
        'hidden_dim': 256,
        'latent_dim': 32,
        'condition_dim': 6,
        'num_layers': 2,

        # è®­ç»ƒé…ç½®
        'epochs': 50,
        'batch_size': 32,
        'learning_rate': 1e-3,
        'weight_decay': 1e-5,
        'train_split': 0.8,
        'oversample': True,
        'num_workers': 0,
        'random_state': 42,

        # ğŸ”¥ è®­ç»ƒç­–ç•¥é…ç½®ï¼ˆå•ä¸€ç¨³å®šå¢é•¿ç‰ˆæœ¬ï¼‰
        'use_amp': True,
        'temperature': 1.3,  # è¿›ä¸€æ­¥æé«˜temperatureå¢åŠ éšæœºæ€§
        'kl_cycles': 1,  # ğŸ”¥ å•ä¸€å‘¨æœŸï¼Œç¨³å®šå¢é•¿ä¸é‡ç½®ï¼
        'kl_ratio': 0.6,  # æ›´å¿«è¿›å…¥æœ‰æ•ˆçº¦æŸé˜¶æ®µ
        'beta_max': 0.25,  # ğŸ”¥ æé«˜çº¦æŸåŠ›
        'delay_epochs': 20,  # ğŸ”¥ å»¶è¿Ÿ20ä¸ªepoch
        'metric_interval': 10,

        # æ—©åœé…ç½®
        'early_stopping': True,
        'patience': 15
    }