#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CVAE è®­ç»ƒå·¥å…·
============

å®ç°æŸå¤±å‡½æ•°ã€KLé€€ç«ç­–ç•¥å’Œè®­ç»ƒæŒ‡æ ‡


"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, Tuple
import math


class CVAELoss(nn.Module):
    """CVAE æŸå¤±å‡½æ•°

    å®ç°ä¸¥æ ¼çš„æŸå¤±å‡½æ•°ï¼šL = L_Recon + Î² * L_KL
    L_Recon ä½¿ç”¨ CrossEntropy
    L_KL ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒè§£æè§£
    """

    def __init__(self):
        super(CVAELoss, self).__init__()
        self.cross_entropy = nn.CrossEntropyLoss(ignore_index=2)  # å¿½ç•¥ PAD token (index=2)

    def reconstruction_loss(self, decoder_output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        é‡æ„æŸå¤± (CrossEntropy)

        Args:
            decoder_output: [batch_size, seq_len, vocab_size] è§£ç å™¨è¾“å‡º
            target: [batch_size, seq_len] ç›®æ ‡åºåˆ—

        Returns:
            é‡æ„æŸå¤±å€¼
        """
        batch_size, seq_len, vocab_size = decoder_output.shape

        # é‡å¡‘ä¸ºé€‚åˆ CrossEntropyLoss çš„å½¢çŠ¶
        # decoder_output: [batch_size * seq_len, vocab_size]
        # target: [batch_size * seq_len]
        decoder_output_flat = decoder_output.view(-1, vocab_size)
        target_flat = target.view(-1)

        # è®¡ç®— CrossEntropy æŸå¤±
        recon_loss = self.cross_entropy(decoder_output_flat, target_flat)

        return recon_loss

    def kl_divergence_loss(self, mu: torch.Tensor, logvar: torch.Tensor, seq_len: float = 120.0) -> torch.Tensor:
        """
        ğŸ”¥ å½»åº•é‡Šæ”¾KLæ•£åº¦ - æ— ä»»ä½•æˆªæ–­ä¿æŠ¤

        å…¬å¼ï¼šL_KL = -0.5 * Î£(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)
        å…è®¸KLæŸå¤±è‡ªç”±æ³¢åŠ¨ï¼Œè®©ç¼–ç å™¨æ„Ÿå—çœŸå®å‹åŠ›

        Args:
            mu: [batch_size, latent_dim] å‡å€¼
            logvar: [batch_size, latent_dim] å¯¹æ•°æ–¹å·®
            seq_len: å¹³å‡åºåˆ—é•¿åº¦ï¼ˆæµ®ç‚¹æ•°ï¼‰ï¼Œç”¨äºå½’ä¸€åŒ–KLæŸå¤±é‡çº§

        Returns:
            å®Œå…¨è‡ªç”±çš„KLæ•£åº¦æŸå¤±å€¼
        """
        # ğŸ”¥ ä½¿ç”¨åŸå§‹çš„é«˜æ–¯åˆ†å¸ƒè§£æè§£å…¬å¼ï¼Œä¸åŠ ä»»ä½•é™åˆ¶
        # L_KL = -0.5 * sum(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)
        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)

        # å–batchå¹³å‡
        kl_loss = torch.mean(kl_loss)

        # ğŸ”¥ å½»åº•æ— ä¿æŠ¤çš„å½’ä¸€åŒ– - ç¡®ä¿ä½¿ç”¨æµ®ç‚¹æ•°é™¤æ³•
        seq_len_tensor = torch.tensor(seq_len, dtype=torch.float32, device=mu.device)
        kl_loss = kl_loss / seq_len_tensor

        # ğŸ”¥ å®Œå…¨ç§»é™¤æ‰€æœ‰é™åˆ¶ï¼è®©KLæŸå¤±å¯ä»¥è‡ªç”±å¢é•¿åˆ°ä»»ä½•å€¼ï¼
        # ä¸å†æœ‰torch.clampï¼Œä¸å†æœ‰ç¡¬ç¼–ç é™åˆ¶ï¼
        # å¦‚æœKLæŸå¤±è¾¾åˆ°10.0ã€20.0ç”šè‡³æ›´é«˜ï¼Œé‚£å°±è®©å®ƒè¾¾åˆ°ï¼

        # ğŸ”¥ è°ƒè¯•ä¿¡æ¯ï¼šç¡®ä¿KLæŸå¤±æ²¡æœ‰è¢«æˆªæ–­
        # print(f"DEBUG: Raw KL loss: {kl_loss.item():.6f}, mu_mean: {mu.mean().item():.6f}, logvar_mean: {logvar.mean().item():.6f}")

        return kl_loss

    def forward(self, decoder_output: torch.Tensor, target: torch.Tensor,
                mu: torch.Tensor, logvar: torch.Tensor, beta: float = 1.0) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æ€»æŸå¤±

        Args:
            decoder_output: [batch_size, seq_len, vocab_size] è§£ç å™¨è¾“å‡º
            target: [batch_size, seq_len] ç›®æ ‡åºåˆ—
            mu: [batch_size, latent_dim] å‡å€¼
            logvar: [batch_size, latent_dim] å¯¹æ•°æ–¹å·®
            beta: KL æŸå¤±æƒé‡

        Returns:
            åŒ…å«å„ç§æŸå¤±çš„å­—å…¸
        """
        # è®¡ç®—é‡æ„æŸå¤±
        recon_loss = self.reconstruction_loss(decoder_output, target)

        # ğŸ”¥ ç¡®ä¿ä½¿ç”¨æµ®ç‚¹æ•°åºåˆ—é•¿åº¦ - å½»åº•æ— ä¿æŠ¤KLè®¡ç®—
        seq_len = float(decoder_output.size(1))  # ç¡®ä¿æ˜¯æµ®ç‚¹æ•°ï¼

        # è®¡ç®— KL æ•£åº¦æŸå¤±ï¼ˆå®Œå…¨æ— ä¿æŠ¤ç‰ˆæœ¬ï¼‰
        kl_loss = self.kl_divergence_loss(mu, logvar, seq_len=seq_len)

        # æ€»æŸå¤±
        total_loss = recon_loss + beta * kl_loss

        return {
            'total_loss': total_loss,
            'recon_loss': recon_loss,
            'kl_loss': kl_loss,
            'beta': beta
        }


class CyclicalAnnealingSchedule:
    """ ğŸ”¥ é…ç½®é©±åŠ¨çš„KLé€€ç«ç­–ç•¥

    æ‰€æœ‰å‚æ•°é€šè¿‡é…ç½®å­—å…¸ä¼ å…¥ï¼Œç§»é™¤ç¡¬ç¼–ç é»˜è®¤å€¼
    æ”¯æŒå®Œå…¨è‡ªå®šä¹‰çš„KLé€€ç«ç­–ç•¥
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Args:
            config: åŒ…å«æ‰€æœ‰KLé€€ç«å‚æ•°çš„é…ç½®å­—å…¸
        """
        # ğŸ”¥ å¼ºåˆ¶è¦æ±‚é…ç½®å‚æ•°ï¼Œç§»é™¤ç¡¬ç¼–ç é»˜è®¤å€¼
        self.total_steps = config['total_steps']
        self.n_cycles = config['n_cycles']
        self.ratio = config['ratio']
        self.beta_max = config['beta_max']
        self.delay_epochs = config['delay_epochs']
        self.steps_per_epoch = config['steps_per_epoch']

        # è®¡ç®—å»¶è¿Ÿæ­¥æ•°
        self.delay_steps = self.delay_epochs * self.steps_per_epoch

        # è®¡ç®—å»¶è¿Ÿåçš„æœ‰æ•ˆæ­¥æ•°
        effective_steps = self.total_steps - self.delay_steps
        if effective_steps <= 0:
            self.steps_per_cycle = 1
            self.rise_steps = 1
        else:
            # è®¡ç®—æ¯ä¸ªå‘¨æœŸçš„æ­¥æ•°
            self.steps_per_cycle = effective_steps // self.n_cycles
            self.rise_steps = int(self.steps_per_cycle * self.ratio)

        # ğŸ”¥ æ—¥å¿—è®°å½•é…ç½®
        self._log_config()

    def _log_config(self):
        """è®°å½•KLé€€ç«é…ç½®"""
        import logging
        logger = logging.getLogger(__name__)
        logger.info(f"ğŸ”¥ KLé€€ç«é…ç½®:")
        logger.info(f"   æ€»æ­¥æ•°: {self.total_steps}")
        logger.info(f"   å‘¨æœŸæ•°: {self.n_cycles}")
        logger.info(f"   ä¸Šå‡æ¯”ä¾‹: {self.ratio}")
        logger.info(f"   æœ€å¤§Beta: {self.beta_max}")
        logger.info(f"   å»¶è¿Ÿè½®æ•°: {self.delay_epochs}")
        logger.info(f"   æ¯è½®æ­¥æ•°: {self.steps_per_epoch}")

    def get_beta(self, step: int) -> float:
        """
        è·å–å½“å‰æ­¥éª¤çš„ Î² å€¼ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        Args:
            step: å½“å‰è®­ç»ƒæ­¥æ•°

        Returns:
            å½“å‰ Î² å€¼ [0, beta_max]
        """
        if step >= self.total_steps:
            return self.beta_max

        # å»¶è¿Ÿé˜¶æ®µï¼šå‰delay_stepsæ­¥ä¿æŒbeta=0
        if step < self.delay_steps:
            return 0.0

        # å‡å»å»¶è¿Ÿæ­¥æ•°ï¼Œä½¿ç”¨æœ‰æ•ˆæ­¥æ•°è®¡ç®—
        effective_step = step - self.delay_steps
        effective_total = self.total_steps - self.delay_steps

        if effective_step < 0 or effective_total <= 0:
            return 0.0

        # è®¡ç®—å½“å‰åœ¨å“ªä¸ªå‘¨æœŸä¸­
        cycle = effective_step // self.steps_per_cycle
        step_in_cycle = effective_step % self.steps_per_cycle

        if step_in_cycle < self.rise_steps:
            # ä¸Šå‡é˜¶æ®µï¼šÎ² ä» 0 çº¿æ€§å¢é•¿åˆ° beta_max
            progress = step_in_cycle / self.rise_steps
            return self.beta_max * progress
        else:
            # å¹³å°é˜¶æ®µï¼šÎ² ä¿æŒä¸º beta_max
            return self.beta_max

    def get_beta_tensor(self, step: torch.Tensor) -> torch.Tensor:
        """
        è·å– Î² å€¼çš„å¼ é‡ç‰ˆæœ¬

        Args:
            step: å½“å‰æ­¥æ•°å¼ é‡

        Returns:
            Î² å€¼å¼ é‡
        """
        if isinstance(step, torch.Tensor):
            device = step.device
            step = step.item()
        else:
            device = torch.device('cpu')

        beta = self.get_beta(step)
        return torch.tensor(beta, device=device, dtype=torch.float32)


class CVAEMetrics:
    """CVAE è®­ç»ƒæŒ‡æ ‡è®¡ç®—

    è®¡ç®— Reconstruction Accuracy å’Œ Validity Rate
    """

    def __init__(self, vocab: Dict[str, int], pad_idx: int = 2, sos_idx: int = 0, eos_idx: int = 1, unk_idx: int = 3):
        """
        Args:
            vocab: è¯è¡¨å­—å…¸ {char: idx}
            pad_idx: PAD token ç´¢å¼•
            sos_idx: SOS token ç´¢å¼•
            eos_idx: EOS token ç´¢å¼•
            unk_idx: UNK token ç´¢å¼•ï¼ˆåŠ¨æ€è·å–ï¼‰
        """
        self.vocab = vocab
        self.idx_to_char = {idx: char for char, idx in vocab.items()}
        self.pad_idx = pad_idx
        self.sos_idx = sos_idx
        self.eos_idx = eos_idx
        self.unk_idx = unk_idx  # ğŸ”¥ åŠ¨æ€è·å–UNKç´¢å¼•

    def decode_sequence(self, sequence: torch.Tensor, visual_mode: bool = False) -> str:
        """
        ğŸ”¥ é‡æ„ï¼šå°†ç´¢å¼•åºåˆ—è§£ç ä¸ºå­—ç¬¦ä¸²ï¼ŒåŒºåˆ†çœŸå®è¯„ä¼°å’Œè§†è§‰è°ƒè¯•

        Args:
            sequence: [seq_len] ç´¢å¼•åºåˆ—
            visual_mode: æ˜¯å¦ä¸ºè§†è§‰æ¨¡å¼ï¼ˆå½±å“UNKå¤„ç†æ–¹å¼ï¼‰

        Returns:
            è§£ç åçš„å­—ç¬¦ä¸²
        """
        chars = []
        unk_count = 0
        total_chars = 0

        for idx in sequence:
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶è½¬æ¢ä¸ºæ•´æ•°ï¼Œè§£å†³TensoråŒ¹é…å¯¼è‡´çš„"å…¨é—®å·"æ˜¾ç¤ºBug
            idx_val = idx.item() if hasattr(idx, 'item') else idx

            if idx_val == self.eos_idx:
                break
            if idx_val not in [self.pad_idx, self.sos_idx]:
                total_chars += 1
                if idx_val == self.unk_idx:  # UNK token (ä½¿ç”¨åŠ¨æ€ç´¢å¼•)
                    unk_count += 1

                    if visual_mode:
                        # ğŸ”¥ è§†è§‰æ¨¡å¼ï¼šéšæœºæ›¿æ¢ä¸ºå¸¸è§å­—ç¬¦ï¼Œä¾¿äºè§‚å¯Ÿ
                        common_chars = ['a', 'e', 'i', 'o', 'u', '1', '0', '=', '\'', '"', ' ', '(', ')', '*', '+']
                        import random
                        if random.random() < 0.7:  # 70%æ¦‚ç‡æ›¿æ¢ä¸ºå¸¸è§å­—ç¬¦
                            chars.append(random.choice(common_chars))
                        else:
                            chars.append('?')  # 30%æ¦‚ç‡æ˜¾ç¤ºä¸º?
                    else:
                        # ğŸ”¥ è¯„ä¼°æ¨¡å¼ï¼šä¿æŒUNKä¸º?ï¼Œç¡®ä¿æŒ‡æ ‡å®¢è§‚æ€§
                        chars.append('?')  # ä¸¥æ ¼ä¿ç•™UNKæ ‡è¯†
                elif idx_val in self.idx_to_char:
                    chars.append(self.idx_to_char[idx_val])
                else:
                    chars.append('?')  # çœŸæ­£çš„æœªçŸ¥å­—ç¬¦

        # ğŸ”¥ å¦‚æœUNKå­—ç¬¦å æ¯”è¿‡é«˜ï¼Œè¿”å›å¤±è´¥æ ‡è®°ï¼ˆä»…åœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼‰
        if not visual_mode and total_chars > 0 and unk_count / total_chars > 0.8:
            return '[GENERATION_FAILED]'  # æ ‡è®°ç”Ÿæˆå¤±è´¥çš„æ ·æœ¬

        return ''.join(chars)

    def reconstruction_accuracy(self, decoder_output: torch.Tensor, target: torch.Tensor) -> float:
        """
        ğŸ”¥ ä¿®å¤ï¼šè®¡ç®—é‡æ„å‡†ç¡®ç‡ï¼Œå…¼å®¹2D/3Dè¾“å…¥å’Œåºåˆ—é•¿åº¦ä¸åŒ¹é…é—®é¢˜

        Args:
            decoder_output:
                - 3D: [batch_size, seq_len, vocab_size] è§£ç å™¨è¾“å‡ºlogits
                - 2D: [batch_size, seq_len] å·²ç»æ˜¯token_ids
            target: [batch_size, seq_len] ç›®æ ‡åºåˆ—

        Returns:
            é‡æ„å‡†ç¡®ç‡ (0-1)
        """
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥è¾“å…¥ç»´åº¦ï¼Œå…¼å®¹2D/3D
        if decoder_output.dim() == 3:
            # 3Dè¾“å…¥ï¼šlogitsï¼Œéœ€è¦argmax
            predicted = torch.argmax(decoder_output, dim=-1)  # [batch_size, seq_len]
        elif decoder_output.dim() == 2:
            # 2Dè¾“å…¥ï¼šå·²ç»æ˜¯token_ids
            predicted = decoder_output
        else:
            raise ValueError(f"decoder_outputç»´åº¦é”™è¯¯: {decoder_output.dim()}Dï¼ŒæœŸæœ›2Dæˆ–3D")

        # ğŸ”¥ å¤„ç†åºåˆ—é•¿åº¦ä¸åŒ¹é…é—®é¢˜
        pred_len = predicted.size(1) if predicted.dim() > 1 else predicted.size(0)
        target_len = target.size(1)

        if pred_len != target_len:
            # æˆªå–åˆ°æœ€å°é•¿åº¦
            min_len = min(pred_len, target_len)
            predicted = predicted[:, :min_len]  # [batch_size, min_len]
            target = target[:, :min_len]  # [batch_size, min_len]

        # è®¡ç®—æ¯ä¸ªä½ç½®çš„æ­£ç¡®æ€§
        correct = (predicted == target).float()

        # åˆ›å»ºæ©ç ï¼Œå¿½ç•¥ PAD token
        mask = (target != self.pad_idx).float()

        # è®¡ç®—å‡†ç¡®ç‡
        total_tokens = mask.sum()
        correct_tokens = (correct * mask).sum()

        if total_tokens == 0:
            return 1.0

        accuracy = (correct_tokens / total_tokens).item()
        return accuracy

    def validity_rate(self, decoder_output: torch.Tensor, target: torch.Tensor) -> float:
        """
        ğŸ”¥ ä¿®å¤ï¼šè®¡ç®—æœ‰æ•ˆè½½è·æ¯”ä¾‹ï¼Œå…¼å®¹2D/3Dè¾“å…¥

        ä¸€ä¸ªè½½è·è¢«è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„ï¼Œå¦‚æœï¼š
        1. åŒ…å«æœ‰æ•ˆçš„è¯­æ³•ç»“æ„
        2. é•¿åº¦åˆç† (è‡³å°‘3ä¸ªå­—ç¬¦ï¼Œå»é™¤ç‰¹æ®Štokenå)
        3. åŒ…å«å®é™…å†…å®¹ (ä¸åªæ˜¯ç‰¹æ®Štoken)

        Args:
            decoder_output:
                - 3D: [batch_size, seq_len, vocab_size] è§£ç å™¨è¾“å‡ºlogits
                - 2D: [batch_size, seq_len] å·²ç»æ˜¯token_ids
            target: [batch_size, seq_len] ç›®æ ‡åºåˆ—

        Returns:
            æœ‰æ•ˆè½½è·æ¯”ä¾‹ (0-1)
        """
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥è¾“å…¥ç»´åº¦ï¼Œå…¼å®¹2D/3D
        if decoder_output.dim() == 3:
            # 3Dè¾“å…¥ï¼šlogitsï¼Œéœ€è¦argmax
            predicted = torch.argmax(decoder_output, dim=-1)  # [batch_size, seq_len]
        elif decoder_output.dim() == 2:
            # 2Dè¾“å…¥ï¼šå·²ç»æ˜¯token_ids
            predicted = decoder_output
        else:
            raise ValueError(f"decoder_outputç»´åº¦é”™è¯¯: {decoder_output.dim()}Dï¼ŒæœŸæœ›2Dæˆ–3D")

        batch_size = predicted.size(0)
        valid_count = 0

        for i in range(batch_size):
            # è§£ç é¢„æµ‹åºåˆ— - ğŸ”¥ ä½¿ç”¨è¯„ä¼°æ¨¡å¼ï¼Œä¸ä½¿ç”¨visual_mode
            pred_seq = predicted[i]
            decoded = self.decode_sequence(pred_seq, visual_mode=False)  # å¼ºåˆ¶è¯„ä¼°æ¨¡å¼

            # æ£€æŸ¥æœ‰æ•ˆæ€§
            if self._is_valid_payload(decoded):
                valid_count += 1

        validity_rate = valid_count / batch_size
        return validity_rate

    def _is_valid_payload(self, payload: str) -> bool:
        """
        æ£€æŸ¥è½½è·æ˜¯å¦æœ‰æ•ˆ

        Args:
            payload: è§£ç åçš„è½½è·å­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        # é•¿åº¦æ£€æŸ¥
        if len(payload) < 3:
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å®é™…å†…å®¹
        if payload.strip() == '':
            return False

        # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯æœªçŸ¥å­—ç¬¦
        if payload.count('?') > len(payload) * 0.8:
            return False

        # æ£€æŸ¥åŸºæœ¬è¯­æ³•åˆç†æ€§ï¼ˆç®€å•æ£€æŸ¥ï¼‰
        # å¯¹äº SQLiï¼Œåº”è¯¥åŒ…å«ä¸€äº›å¸¸è§å­—ç¬¦
        sql_chars = ["'", '"', '=', '<', '>', ' ', ';', ',', '(', ')', '*', '/', '-']
        if any(char in payload for char in sql_chars):
            return True

        # å¯¹äº XSSï¼Œåº”è¯¥åŒ…å«æ ‡ç­¾ç›¸å…³å­—ç¬¦
        xss_chars = ['<', '>', '/', '\\', '&']
        if any(char in payload for char in xss_chars):
            return True

        # åŒ…å«å­—æ¯æ•°å­—ä¹Ÿç®—æœ‰æ•ˆ
        if any(char.isalnum() for char in payload):
            return True

        return False

    def calculate_metrics(self, decoder_output: torch.Tensor, target: torch.Tensor) -> Dict[str, float]:
        """
        è®¡ç®—æ‰€æœ‰æŒ‡æ ‡

        Args:
            decoder_output: [batch_size, seq_len, vocab_size] è§£ç å™¨è¾“å‡º
            target: [batch_size, seq_len] ç›®æ ‡åºåˆ—

        Returns:
            åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        """
        recon_acc = self.reconstruction_accuracy(decoder_output, target)
        validity = self.validity_rate(decoder_output, target)

        return {
            'reconstruction_accuracy': recon_acc,
            'validity_rate': validity
        }