#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CVAE æ•°æ®åŠ è½½å™¨
==============

å®ç°æ•°æ®åŠ è½½ã€è¿‡é‡‡æ ·å’Œæ‰¹å¤„ç†åŠŸèƒ½
æ”¯æŒä» processed_data.pt å’Œ vocab.json åŠ è½½æ•°æ®

ä½œè€…ï¼šè€ç‹ (æš´èºæŠ€æœ¯æµ)
ç‰ˆæœ¬ï¼š1.0
æ—¥æœŸï¼š2025-12-18
"""

import torch
from torch.utils.data import Dataset, DataLoader
import json
from pathlib import Path
from typing import Dict, Tuple, Optional, List
import numpy as np
import random


class CVADDataset(Dataset):
    """CVAE è®­ç»ƒæ•°æ®é›†

    æ”¯æŒè¿‡é‡‡æ ·æ¥ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    """

    def __init__(self, data_path: str, vocab_path: str,
                 oversample: bool = True, random_state: int = 42):
        """
        Args:
            data_path: processed_data.pt æ–‡ä»¶è·¯å¾„
            vocab_path: vocab.json æ–‡ä»¶è·¯å¾„
            oversample: æ˜¯å¦å¯ç”¨è¿‡é‡‡æ ·
            random_state: éšæœºç§å­
        """
        self.data_path = Path(data_path)
        self.vocab_path = Path(vocab_path)
        self.oversample = oversample
        self.random_state = random_state

        # è®¾ç½®éšæœºç§å­
        random.seed(random_state)
        np.random.seed(random_state)
        torch.manual_seed(random_state)

        # åŠ è½½æ•°æ®å’Œè¯è¡¨
        self.data, self.labels, self.vocab = self._load_data()
        self.original_length = len(self.data)

        # è¿‡é‡‡æ ·å¤„ç†
        if self.oversample:
            self.data, self.labels = self._apply_oversampling()

        print(f"ğŸ“Š æ•°æ®é›†åŠ è½½å®Œæˆï¼š")
        print(f"   åŸå§‹æ ·æœ¬æ•°ï¼š{self.original_length}")
        print(f"   è¿‡é‡‡æ ·åæ ·æœ¬æ•°ï¼š{len(self.data)}")
        print(f"   è¯è¡¨å¤§å°ï¼š{len(self.vocab['char_to_idx'])}")

        # ç»Ÿè®¡å„ç±»åˆ«åˆ†å¸ƒ
        self._print_class_distribution()

    def _load_data(self) -> Tuple[torch.Tensor, torch.Tensor, Dict]:
        """åŠ è½½é¢„å¤„ç†æ•°æ®å’Œè¯è¡¨"""
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®ï¼š{self.data_path}")

        # åŠ è½½å¼ é‡æ•°æ®
        data_tensor = torch.load(self.data_path, weights_only=False)
        print(f"âœ… æ•°æ®å¼ é‡åŠ è½½å®Œæˆï¼š{data_tensor.shape}")

        # åŠ è½½è¯è¡¨
        with open(self.vocab_path, 'r', encoding='utf-8') as f:
            vocab_data = json.load(f)

        print(f"âœ… è¯è¡¨åŠ è½½å®Œæˆï¼Œå¤§å°ï¼š{vocab_data['vocab_size']}")

        # åŠ è½½æ ‡ç­¾æ˜ å°„
        label_mapping_path = self.vocab_path.parent / 'label_mapping.json'
        if label_mapping_path.exists():
            with open(label_mapping_path, 'r', encoding='utf-8') as f:
                label_mapping = json.load(f)
            print(f"âœ… æ ‡ç­¾æ˜ å°„åŠ è½½å®Œæˆ")
        else:
            # ä½¿ç”¨é»˜è®¤æ ‡ç­¾æ˜ å°„
            label_mapping = {
                'SQLi': 0,
                'XSS': 1,
                'CMDi': 2,
                'Overflow': 3,
                'XXE': 4,
                'SSI': 5
            }
            print(f"âš ï¸ ä½¿ç”¨é»˜è®¤æ ‡ç­¾æ˜ å°„")

        # åŠ è½½æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        stats_path = self.vocab_path.parent / 'dataset_stats.json'
        if stats_path.exists():
            with open(stats_path, 'r', encoding='utf-8') as f:
                dataset_stats = json.load(f)

            # ä»ç»Ÿè®¡ä¿¡æ¯ä¸­è·å–å®é™…ç±»åˆ«åˆ†å¸ƒ
            attack_distribution = dataset_stats.get('attack_distribution', {})
            total_samples = dataset_stats.get('total_samples', data_tensor.shape[0])

            print(f"ğŸ“Š ä»ç»Ÿè®¡ä¿¡æ¯åŠ è½½ç±»åˆ«åˆ†å¸ƒï¼š{attack_distribution}")
        else:
            # ä½¿ç”¨é»˜è®¤åˆ†å¸ƒ
            attack_distribution = {
                'SQLi': 759,
                'XSS': 6711,
                'CMDi': 439,
                'Overflow': 49,
                'XXE': 105,
                'SSI': 18
            }
            total_samples = data_tensor.shape[0]
            print(f"âš ï¸ ä½¿ç”¨é»˜è®¤ç±»åˆ«åˆ†å¸ƒ")

        # åˆ›å»ºæ ‡ç­¾
        labels = []
        for attack_type, count in attack_distribution.items():
            if attack_type in label_mapping:
                class_id = label_mapping[attack_type]
                labels.extend([class_id] * count)
                print(f"   {attack_type}: {count} æ ·æœ¬ -> ç±»åˆ« {class_id}")

        # ç¡®ä¿æ ‡ç­¾æ•°é‡ä¸æ•°æ®æ ·æœ¬æ•°åŒ¹é…
        if len(labels) != total_samples:
            print(f"âš ï¸ æ ‡ç­¾æ•°é‡({len(labels)})ä¸æ•°æ®æ ·æœ¬æ•°({total_samples})ä¸åŒ¹é…ï¼Œè¿›è¡Œè°ƒæ•´")
            if len(labels) < total_samples:
                # è¡¥å……æœ€åä¸€ä¸ªç±»åˆ«çš„æ ‡ç­¾
                last_class_id = labels[-1] if labels else 0
                labels.extend([last_class_id] * (total_samples - len(labels)))
            else:
                # æˆªæ–­å¤šä½™çš„æ ‡ç­¾
                labels = labels[:total_samples]

        labels_tensor = torch.tensor(labels, dtype=torch.long)

        print(f"âœ… æ ‡ç­¾åˆ›å»ºå®Œæˆï¼š{len(labels)} ä¸ªæ ‡ç­¾")

        return data_tensor, labels_tensor, vocab_data

    def _apply_oversampling(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """åº”ç”¨è¿‡é‡‡æ ·æ¥å¹³è¡¡ç±»åˆ«åˆ†å¸ƒ"""
        print("ğŸ”„ æ­£åœ¨åº”ç”¨è¿‡é‡‡æ ·...")

        # ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
        unique_labels, counts = torch.unique(self.labels, return_counts=True)
        class_counts = {int(label): int(count) for label, count in zip(unique_labels, counts)}

        # æ‰¾åˆ°æœ€å¤šçš„ç±»åˆ«æ•°é‡
        max_count = max(class_counts.values())
        print(f"ğŸ“ˆ æœ€å¤§ç±»åˆ«æ ·æœ¬æ•°ï¼š{max_count}")

        # ä¸ºæ¯ä¸ªç±»åˆ«è¿‡é‡‡æ ·åˆ°æœ€å¤§æ•°é‡
        oversampled_data = []
        oversampled_labels = []

        for class_id in unique_labels:
            class_id = int(class_id)
            class_mask = (self.labels == class_id)
            class_data = self.data[class_mask]
            class_labels = self.labels[class_mask]

            current_count = len(class_data)
            needed_count = max_count - current_count

            if needed_count > 0:
                # éšæœºé‡‡æ ·ç°æœ‰æ ·æœ¬
                indices = torch.randint(0, current_count, (needed_count,))
                additional_data = class_data[indices]
                additional_labels = class_labels[indices]

                # åˆå¹¶åŸå§‹æ•°æ®å’Œè¿‡é‡‡æ ·æ•°æ®
                class_data = torch.cat([class_data, additional_data], dim=0)
                class_labels = torch.cat([class_labels, additional_labels], dim=0)

            oversampled_data.append(class_data)
            oversampled_labels.append(class_labels)

            print(f"   ç±»åˆ« {class_id}: {current_count} -> {len(class_data)} æ ·æœ¬")

        # åˆå¹¶æ‰€æœ‰ç±»åˆ«çš„æ•°æ®
        final_data = torch.cat(oversampled_data, dim=0)
        final_labels = torch.cat(oversampled_labels, dim=0)

        # éšæœºæ‰“ä¹±æ•°æ®
        permutation = torch.randperm(len(final_data))
        final_data = final_data[permutation]
        final_labels = final_labels[permutation]

        return final_data, final_labels

    def _print_class_distribution(self):
        """æ‰“å°ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡"""
        unique_labels, counts = torch.unique(self.labels, return_counts=True)
        total = len(self.labels)

        print("ğŸ“‹ ç±»åˆ«åˆ†å¸ƒï¼š")
        class_names = ['SQLi', 'XSS', 'CMDi', 'Overflow', 'XXE', 'SSI']

        for i, (label, count) in enumerate(zip(unique_labels, counts)):
            label_int = int(label)
            percentage = (count / total) * 100
            class_name = class_names[label_int] if label_int < len(class_names) else f"Class_{label_int}"
            print(f"   {class_name:>8}: {count:>6} ({percentage:>5.1f}%)")

    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.data)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è·å–å•ä¸ªæ ·æœ¬

        Args:
            idx: æ ·æœ¬ç´¢å¼•

        Returns:
            (sequence, label): åºåˆ—å’Œå¯¹åº”çš„æ ‡ç­¾
        """
        sequence = self.data[idx]
        label = self.labels[idx]

        return sequence, label

    def get_vocab_info(self) -> Dict:
        """è·å–è¯è¡¨ä¿¡æ¯"""
        return {
            'vocab_size': self.vocab['vocab_size'],
            'char_to_idx': self.vocab['char_to_idx'],
            'special_tokens': self.vocab['special_tokens'],
            'max_length': self.vocab.get('max_length', 150)
        }

    def get_class_weights(self) -> torch.Tensor:
        """
        è®¡ç®—ç±»åˆ«æƒé‡ï¼Œç”¨äºå¹³è¡¡æŸå¤±å‡½æ•°

        Returns:
            [num_classes] ç±»åˆ«æƒé‡
        """
        unique_labels, counts = torch.unique(self.labels, return_counts=True)
        total_samples = len(self.labels)
        num_classes = len(unique_labels)

        # è®¡ç®—æƒé‡ï¼š1 / (ç±»åˆ«é¢‘ç‡)
        weights = torch.zeros(num_classes)
        for label, count in zip(unique_labels, counts):
            weights[int(label)] = total_samples / (num_classes * count)

        # å½’ä¸€åŒ–æƒé‡
        weights = weights / weights.sum() * num_classes

        return weights


def create_data_loaders(data_path: str, vocab_path: str,
                       batch_size: int = 32, train_split: float = 0.8,
                       oversample: bool = True, num_workers: int = 0,
                       random_state: int = 42) -> Tuple[DataLoader, DataLoader, Dict]:
    """
    åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨

    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        vocab_path: è¯è¡¨æ–‡ä»¶è·¯å¾„
        batch_size: æ‰¹å¤§å°
        train_split: è®­ç»ƒé›†æ¯”ä¾‹
        oversample: æ˜¯å¦å¯ç”¨è¿‡é‡‡æ ·
        num_workers: æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°
        random_state: éšæœºç§å­

    Returns:
        (train_loader, val_loader, vocab_info): è®­ç»ƒåŠ è½½å™¨ã€éªŒè¯åŠ è½½å™¨å’Œè¯è¡¨ä¿¡æ¯
    """
    print(f"ğŸš€ å¼€å§‹åˆ›å»ºæ•°æ®åŠ è½½å™¨...")

    # åˆ›å»ºæ•°æ®é›†
    dataset = CVADDataset(
        data_path=data_path,
        vocab_path=vocab_path,
        oversample=oversample,
        random_state=random_state
    )

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    total_size = len(dataset)
    train_size = int(total_size * train_split)
    val_size = total_size - train_size

    # åˆ›å»ºéšæœºç´¢å¼•
    indices = torch.randperm(total_size)

    # åˆ’åˆ†ç´¢å¼•
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]

    # åˆ›å»ºå­æ•°æ®é›†
    train_dataset = torch.utils.data.Subset(dataset, train_indices)
    val_dataset = torch.utils.data.Subset(dataset, val_indices)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )

    # è·å–è¯è¡¨ä¿¡æ¯
    vocab_info = dataset.get_vocab_info()

    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼š")
    print(f"   è®­ç»ƒæ ·æœ¬æ•°ï¼š{len(train_dataset)}")
    print(f"   éªŒè¯æ ·æœ¬æ•°ï¼š{len(val_dataset)}")
    print(f"   æ‰¹å¤§å°ï¼š{batch_size}")
    print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°ï¼š{len(train_loader)}")
    print(f"   éªŒè¯æ‰¹æ¬¡æ•°ï¼š{len(val_loader)}")

    return train_loader, val_loader, vocab_info


def load_vocab(vocab_path: str) -> Dict:
    """
    åŠ è½½è¯è¡¨æ–‡ä»¶

    Args:
        vocab_path: è¯è¡¨æ–‡ä»¶è·¯å¾„

    Returns:
        è¯è¡¨å­—å…¸
    """
    with open(vocab_path, 'r', encoding='utf-8') as f:
        vocab = json.load(f)

    return vocab


def sample_from_dataset(data_path: str, vocab_path: str,
                       num_samples: int = 10, random_state: int = 42) -> List[Dict]:
    """
    ä»æ•°æ®é›†ä¸­éšæœºé‡‡æ ·æ ·æœ¬ç”¨äºè°ƒè¯•

    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        vocab_path: è¯è¡¨æ–‡ä»¶è·¯å¾„
        num_samples: é‡‡æ ·æ•°é‡
        random_state: éšæœºç§å­

    Returns:
        é‡‡æ ·çš„æ ·æœ¬åˆ—è¡¨
    """
    # åŠ è½½æ•°æ®
    data_tensor = torch.load(data_path, weights_only=False)
    vocab = load_vocab(vocab_path)

    # åˆ›å»ºç´¢å¼•æ˜ å°„
    idx_to_char = {int(k): v for k, v in vocab['idx_to_char'].items()}

    # éšæœºé‡‡æ ·
    torch.manual_seed(random_state)
    indices = torch.randperm(len(data_tensor))[:num_samples]

    samples = []
    for idx in indices:
        sequence = data_tensor[idx]
        decoded = []

        for token in sequence:
            if token == 1:  # EOS
                break
            if token not in [0, 2]:  # ä¸æ˜¯ SOS æˆ– PAD
                if int(token) in idx_to_char:
                    decoded.append(idx_to_char[int(token)])
                else:
                    decoded.append('?')

        payload = ''.join(decoded)
        samples.append({
            'index': int(idx),
            'sequence': sequence.tolist(),
            'payload': payload,
            'length': len(payload)
        })

    return samples