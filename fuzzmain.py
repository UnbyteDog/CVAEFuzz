#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CVDBFuzz - åŸºäºCVAEç”Ÿæˆä¸DBSCANä¼˜åŒ–çš„æ™ºèƒ½Webæ¨¡ç³Šæµ‹è¯•æ¡†æ¶ï¼ˆå…¨é˜¶æ®µï¼‰
====================================================================

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. é˜¶æ®µä¸€ï¼šæ•°æ®é¢„å¤„ç† - å­—ç¬¦çº§åˆ†è¯ã€åºåˆ—æ ‡å‡†åŒ–ã€è¯è¡¨æ„å»º
2. é˜¶æ®µäºŒï¼šCVAEè®­ç»ƒ - å­¦ä¹ æ”»å‡»è½½è·çš„éšå¼åˆ†å¸ƒ
3. é˜¶æ®µä¸‰ï¼šç”Ÿæˆä¸èšç±» - CVAEç”Ÿæˆè½½è· + DBSCANä¼˜åŒ–
4. é˜¶æ®µå››ï¼šé»‘ç›’æ¨¡ç³Šæµ‹è¯• - é€’å½’çˆ¬è™« + æ™ºèƒ½æ³¨å…¥

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # å‰ä¸‰é˜¶æ®µæµç¨‹
    python fuzzmain.py --preprocess --train --generate --cluster

    # ç¬¬å››é˜¶æ®µï¼šçº¯çˆ¬è™«æ¨¡å¼
    python fuzzmain.py --crawl --url http://example.com --depth 2

    # ç¬¬å››é˜¶æ®µï¼šå®Œæ•´æ‰«ææ¨¡å¼
    python fuzzmain.py --scan --url http://example.com --depth 2 --use-cache

    # åˆ†æ­¥æ‰§è¡Œ
    python fuzzmain.py --preprocess
    python fuzzmain.py --train --epochs 50
    python fuzzmain.py --generate --cluster --num-samples 10000
    python fuzzmain.py --analyze
"""

import sys
import os
import json
import argparse
import time
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é¢„å¤„ç†æ¨¡å—
try:
    sys.path.insert(0, os.path.join(project_root, "Data_processing"))
    from preprocessor import main as preprocess_main
except ImportError as e:
    print(f"[ERROR] å¯¼å…¥é¢„å¤„ç†æ¨¡å—å¤±è´¥ï¼š{e}")
    print("[INFO] è¯·ç¡®ä¿ Data_processing/preprocessor.py æ–‡ä»¶å­˜åœ¨")
    sys.exit(1)

# å¯¼å…¥CVAEç”Ÿæˆå™¨å’Œèšç±»å™¨æ¨¡å—
try:
    sys.path.insert(0, os.path.join(project_root, "CVAE"))
    from generator import CVAEGenerator
    sys.path.insert(0, os.path.join(project_root, "Clusterer"))
    from clusterer import CVAEClusterer
except ImportError as e:
    print(f"[ERROR] å¯¼å…¥ç”Ÿæˆ/èšç±»æ¨¡å—å¤±è´¥ï¼š{e}")
    print("[INFO] è¯·ç¡®ä¿ CVAE/generator.py å’Œ Clusterer/clusterer.py æ–‡ä»¶å­˜åœ¨")
    sys.exit(1)


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="CVDBFuzz - åŸºäºCVAEç”Ÿæˆä¸DBSCANä¼˜åŒ–çš„æ™ºèƒ½Webæ¨¡ç³Šæµ‹è¯•æ¡†æ¶ï¼ˆå…¨é˜¶æ®µï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å·¥ä½œæµç¨‹ç¤ºä¾‹ï¼š
ã€å‰ä¸‰é˜¶æ®µï¼šAIè½½è·ç”Ÿæˆã€‘
    1. æ•°æ®é¢„å¤„ç†ï¼š  python fuzzmain.py --preprocess
    2. æ¨¡å‹è®­ç»ƒï¼š    python fuzzmain.py --train --epochs 50
    3. ç”Ÿæˆä¸èšç±»ï¼š  python fuzzmain.py --generate --cluster --num-samples 5000

ã€ç¬¬å››é˜¶æ®µï¼šé»‘ç›’æ¨¡ç³Šæµ‹è¯•ã€‘
    4. çº¯çˆ¬è™«æ¨¡å¼ï¼š  python fuzzmain.py --crawl --url http://example.com --depth 2
    5. å®Œæ•´æ‰«æï¼š    python fuzzmain.py --scan --url http://example.com --use-cache

ã€BaseFuzzæ™ºèƒ½æ¨¡ç³Šæµ‹è¯•ã€‘
    6. ä»URLæ‰«æï¼š  python fuzzmain.py --fuzz --url http://example.com
    7. ä»ç¼“å­˜æ‰«æï¼š  python fuzzmain.py --fuzz --file Data/cache/example.com/spider_cache.json
    8. è‡ªå®šä¹‰æ¨¡å¼ï¼š  python fuzzmain.py --fuzz --url http://example.com --mode common --threads 20

ã€å®Œæ•´æµç¨‹ã€‘
    # AIè½½è·ç”Ÿæˆ + BaseFuzzæµ‹è¯•
    python fuzzmain.py --preprocess --train --generate --cluster
    python fuzzmain.py --fuzz --url http://target.com --mode cvae

å‚æ•°è¯´æ˜ï¼š
    [é˜¶æ®µä¸€] --preprocess:   æ‰§è¡Œæ•°æ®é¢„å¤„ç†æ¨¡å—
    [é˜¶æ®µäºŒ] --train:        æ‰§è¡ŒCVAEæ¨¡å‹è®­ç»ƒ
    [é˜¶æ®µä¸‰] --generate:     æ‰§è¡Œè½½è·ç”Ÿæˆ
            --cluster:       æ‰§è¡ŒDBSCANèšç±»ä¼˜åŒ–
    [é˜¶æ®µå››] --crawl:        çº¯çˆ¬è™«æ¨¡å¼ï¼Œå‘ç°å¹¶ä¿å­˜ä»»åŠ¡ç¼“å­˜
            --scan:         å®Œæ•´æ‰«ææ¨¡å¼ï¼ˆçˆ¬è™« + æ¨¡ç³Šæµ‹è¯•ï¼‰
            --url:          ç›®æ ‡åŸºç¡€URLï¼ˆé˜¶æ®µå››å¿…å¡«ï¼‰
            --depth:        çˆ¬è™«é€’å½’æ·±åº¦ï¼ˆé»˜è®¤2ï¼‰
            --use-cache:    å¤ç”¨å·²æœ‰çˆ¬è™«ç¼“å­˜
            --cookie:       å…¨å±€è®¤è¯Cookie

    [BaseFuzz] --fuzz:        ä½¿ç”¨BaseFuzzæ™ºèƒ½å¼•æ“æ‰§è¡Œæ¨¡ç³Šæµ‹è¯•
            --file:         åŠ è½½ç°æœ‰çš„çˆ¬è™«JSONç¼“å­˜æ–‡ä»¶
            --engine:       é€‰æ‹©å¼•æ“ç±»å‹ (ä»…baseå¯ç”¨ï¼Œå…¶ä»–å¾…å®ç°)
            --mode:         è½½è·æ¨¡å¼ (common=ä¸“å®¶å­—å…¸, cvae=AIç”Ÿæˆ, é»˜è®¤: cvae)
            --threads:      å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 10)
        """
    )

    # ========== é˜¶æ®µä¸€ï¼šæ•°æ®é¢„å¤„ç†å‚æ•° ==========
    preprocess_group = parser.add_argument_group('é˜¶æ®µä¸€ï¼šæ•°æ®é¢„å¤„ç†å‚æ•°')
    preprocess_group.add_argument(
        '--preprocess',
        action='store_true',
        help='æ‰§è¡Œæ•°æ®é¢„å¤„ç†ï¼ˆå­—ç¬¦çº§åˆ†è¯ã€åºåˆ—æ ‡å‡†åŒ–ã€è¯è¡¨æ„å»ºï¼‰'
    )
    preprocess_group.add_argument(
        '--data-dir',
        type=str,
        default='Data/payload/train',
        help='è®­ç»ƒæ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: Data/payload/train)'
    )
    preprocess_group.add_argument(
        '--output-dir',
        type=str,
        default='Data/processed',
        help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: Data/processed)'
    )
    preprocess_group.add_argument(
        '--max-length',
        type=int,
        default=150,
        help='æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 150)'
    )
    preprocess_group.add_argument(
        '--vocab-size',
        type=int,
        default=256,
        help='è¯è¡¨å¤§å° (é»˜è®¤: 256)'
    )

    # ========== é˜¶æ®µäºŒï¼šCVAEè®­ç»ƒå‚æ•° ==========
    train_group = parser.add_argument_group('é˜¶æ®µäºŒï¼šCVAEæ¨¡å‹è®­ç»ƒå‚æ•°')
    train_group.add_argument(
        '--train',
        action='store_true',
        help='è®­ç»ƒCVAEæ¨¡å‹'
    )
    train_group.add_argument(
        '--epochs',
        type=int,
        default=50,
        help='è®­ç»ƒè½®æ•° (é»˜è®¤: 50)'
    )
    train_group.add_argument(
        '--batch-size',
        type=int,
        default=32,
        help='æ‰¹å¤§å° (é»˜è®¤: 32)'
    )
    train_group.add_argument(
        '--learning-rate',
        type=float,
        default=1e-3,
        help='å­¦ä¹ ç‡ (é»˜è®¤: 1e-3)'
    )
    train_group.add_argument(
        '--embed-dim',
        type=int,
        default=128,
        help='è¯åµŒå…¥ç»´åº¦ (é»˜è®¤: 128)'
    )
    train_group.add_argument(
        '--hidden-dim',
        type=int,
        default=256,
        help='GRUéšè—å±‚ç»´åº¦ (é»˜è®¤: 256)'
    )
    train_group.add_argument(
        '--latent-dim',
        type=int,
        default=32,
        help='éšç©ºé—´ç»´åº¦ (é»˜è®¤: 32)'
    )
    train_group.add_argument(
        '--kl-cycles',
        type=int,
        default=1,
        help='KLé€€ç«å‘¨æœŸæ•° (é»˜è®¤: 1ï¼Œå•ä¸€ç¨³å®šå¢é•¿)'
    )
    train_group.add_argument(
        '--beta-max',
        type=float,
        default=0.25,
        help='KLé€€ç«æœ€å¤§Betaå€¼ (é»˜è®¤: 0.25ï¼Œå¼ºçº¦æŸåŠ›)'
    )
    train_group.add_argument(
        '--delay-epochs',
        type=int,
        default=20,
        help='KLé€€ç«å»¶è¿Ÿepochæ•° (é»˜è®¤: 20ï¼Œå…ˆå­¦å¥½é‡æ„)'
    )
    train_group.add_argument(
        '--num-layers',
        type=int,
        default=2,
        help='GRUå±‚æ•° (é»˜è®¤: 2ï¼ŒåŒå±‚GRUå¢å¼ºè¡¨è¾¾èƒ½åŠ›)'
    )
    train_group.add_argument(
        '--condition-dim',
        type=int,
        default=6,
        help='æ”»å‡»ç±»å‹æ ‡ç­¾ç»´åº¦ (é»˜è®¤: 6ï¼Œæ”¯æŒSQLi/XSS/CMDiç­‰6ç±»)'
    )
    train_group.add_argument(
        '--weight-decay',
        type=float,
        default=1e-5,
        help='L2æ­£åˆ™åŒ–æƒé‡è¡°å‡ç³»æ•° (é»˜è®¤: 1e-5)'
    )
    train_group.add_argument(
        '--train-split',
        type=float,
        default=0.8,
        help='è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤: 0.8ï¼Œ20%%ä½œä¸ºéªŒè¯é›†)'
    )
    train_group.add_argument(
        '--no-oversample',
        action='store_false',
        dest='oversample',
        help='ç¦ç”¨ç±»åˆ«è¿‡é‡‡æ · (é»˜è®¤å¯ç”¨ï¼Œå¹³è¡¡æ ·æœ¬åˆ†å¸ƒ)'
    )

    # ========== é˜¶æ®µä¸‰ï¼šç”Ÿæˆä¸èšç±»å‚æ•° ==========
    generate_group = parser.add_argument_group('é˜¶æ®µä¸‰ï¼šç”Ÿæˆä¸èšç±»å‚æ•°')
    generate_group.add_argument(
        '--generate',
        action='store_true',
        help='ä½¿ç”¨è®­ç»ƒå¥½çš„CVAEç”Ÿæˆè½½è·'
    )
    generate_group.add_argument(
        '--cluster',
        action='store_true',
        help='ä½¿ç”¨DBSCANå¯¹ç”Ÿæˆçš„è½½è·è¿›è¡Œèšç±»ä¼˜åŒ–'
    )
    generate_group.add_argument(
        '--temperature',
        type=float,
        default=1.8,
        help='ç”Ÿæˆæ¸©åº¦å‚æ•° (é»˜è®¤: 1.8ï¼Œå¢åŠ éšæœºæ€§)'
    )
    generate_group.add_argument(
        '--num-samples',
        type=int,
        default=5000,
        help='æ¯ç§æ”»å‡»ç±»å‹ç”Ÿæˆæ ·æœ¬æ•°é‡ (é»˜è®¤: 5000)'
    )
    generate_group.add_argument(
        '--attack-type',
        type=str,
        default='ALL',
        help='æ”»å‡»è½½è·ç±»å‹ (é»˜è®¤: ALLï¼Œæ”¯æŒ: SQLi, XSS, CMDi, Overflow, XXE, SSI, ALL æˆ–é€—å·åˆ†éš”çš„ç»„åˆ)'
    )
    generate_group.add_argument(
        '--generation-batch-size',
        type=int,
        default=500,
        help='ç”Ÿæˆæ‰¹å¤„ç†å¤§å° (é»˜è®¤: 500)'
    )
    generate_group.add_argument(
        '--eps',
        type=float,
        help='DBSCANçš„epså‚æ•°ï¼ˆè‡ªåŠ¨å¯»æ‰¾å¦‚æœæœªæŒ‡å®šï¼‰'
    )
    generate_group.add_argument(
        '--min-samples',
        type=int,
        help='DBSCANçš„min_sampleså‚æ•°'
    )
    generate_group.add_argument(
        '--samples-per-cluster',
        type=int,
        default=5,
        help='æ¯ä¸ªç°‡ä¿ç•™çš„æ ·æœ¬æ•° (é»˜è®¤: 5)'
    )
    generate_group.add_argument(
        '--reduction-method',
        type=str,
        default='tsne',
        choices=['tsne', 'pca'],
        help='é™ç»´æ–¹æ³• (é»˜è®¤: tsne)'
    )
    generate_group.add_argument(
        '--visualize',
        action='store_true',
        help='ç”Ÿæˆèšç±»å¯è§†åŒ–å›¾åƒ'
    )
    generate_group.add_argument(
        '--keep-noise',
        action='store_true',
        help='ä¿ç•™æ‰€æœ‰å™ªå£°ç‚¹'
    )

    # ========== æ•ˆæœåˆ†æå‚æ•° ==========
    analyze_group = parser.add_argument_group('æ•ˆæœåˆ†æå‚æ•°')
    analyze_group.add_argument(
        '--analyze',
        action='store_true',
        help='æ‰§è¡ŒAIè½½è·æ•ˆæœè¯„ä¼°åˆ†æï¼ˆTODO: å¾…å®ç°ï¼‰'
    )

    # ========== çˆ¬è™«ä¸ç›®æ ‡ç”Ÿæˆå‚æ•° ==========
    scan_group = parser.add_argument_group('çˆ¬è™«ä¸ç›®æ ‡ç”Ÿæˆå‚æ•°')
    scan_group.add_argument(
        '--crawl',
        action='store_true',
        help='çº¯çˆ¬è™«æ¨¡å¼ï¼šå‘ç°å¹¶ä¿å­˜ä»»åŠ¡ç¼“å­˜åé€€å‡º'
    )
    scan_group.add_argument(
        '--scan',
        action='store_true',
        help='å®Œæ•´æ‰«ææ¨¡å¼ï¼šçˆ¬è™« + æ¨¡ç³Šæµ‹è¯•'
    )
    scan_group.add_argument(
        '--url',
        type=str,
        help='ç›®æ ‡åŸºç¡€URLï¼ˆå¿…å¡«ï¼‰'
    )
    scan_group.add_argument(
        '--params',
        type=str,
        help='æ‰‹åŠ¨æŒ‡å®šæµ‹è¯•å‚æ•°ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œæ ¼å¼: --params name1,name2 æˆ– name1=value1,name2 ï¼ˆvalueè¡¨ç¤ºå›ºå®šå€¼ï¼ŒFuzzè¡¨ç¤ºæµ‹è¯•ï¼‰'
    )
    scan_group.add_argument(
        '--method',
        type=str,
        choices=['GET', 'POST'],
        default='GET',
        help='è¯·æ±‚æ–¹æ³•: GET(é»˜è®¤) æˆ– POST'
    )
    scan_group.add_argument(
        '--depth',
        type=int,
        default=2,
        help='çˆ¬è™«é€’å½’æ·±åº¦ (é»˜è®¤: 2)'
    )
    scan_group.add_argument(
        '--use-cache',
        action='store_true',
        help='å¤ç”¨å·²æœ‰çš„çˆ¬è™«ç¼“å­˜'
    )
    scan_group.add_argument(
        '--cookie',
        type=str,
        help='å…¨å±€è®¤è¯Cookie'
    )
    scan_group.add_argument(
        '--headers',
        type=str,
        default='',
        help='æŒ‡å®šè¦æµ‹è¯•çš„HTTPå¤´ï¼ˆé€—å·åˆ†éš”ï¼‰\nç¤ºä¾‹: --headers "User-Agent,Referer,X-Forwarded-For"\né»˜è®¤: ç©ºï¼ˆä¸æµ‹è¯•HTTPå¤´ï¼‰'
    )
    scan_group.add_argument(
        '--timeout',
        type=int,
        default=10,
        help='è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’) (é»˜è®¤: 10)'
    )

    # ========== BaseFuzzå¼•æ“å‚æ•° ==========
    basefuzz_group = parser.add_argument_group('BaseFuzzå¼•æ“å‚æ•°')
    basefuzz_group.add_argument(
        '--fuzz',
        action='store_true',
        help='ä½¿ç”¨BaseFuzzå¼•æ“æ‰§è¡Œæ¨¡ç³Šæµ‹è¯•'
    )
    basefuzz_group.add_argument(
        '--engine',
        type=str,
        default='base',
        choices=['base'],  # ç›®å‰åªå®ç°baseå¼•æ“
        help='é€‰æ‹©å¼•æ“ç±»å‹ (é»˜è®¤: baseï¼Œå…¶ä»–å¼•æ“å¾…å®ç°)'
    )
    basefuzz_group.add_argument(
        '--mode',
        type=str,
        default='cvae',
        choices=['common', 'cvae'],
        help='è½½è·æ¨¡å¼: common=ä¸“å®¶å­—å…¸, cvae=AIç”Ÿæˆ (é»˜è®¤: cvae)'
    )
    basefuzz_group.add_argument(
        '--threads',
        type=int,
        default=10,
        help='å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 10)'
    )
    basefuzz_group.add_argument(
        '--file',
        type=str,
        help='åŠ è½½ç°æœ‰çš„çˆ¬è™«JSONç¼“å­˜æ–‡ä»¶ï¼ˆBaseFuzzæ¨¡å¼ï¼‰'
    )

    # ========== é€šç”¨å‚æ•° ==========
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡ºä¿¡æ¯'
    )
    parser.add_argument(
        '--version',
        action='version',
        version='CVDBFuzz v4.0 - å…¨é˜¶æ®µå®Œæ•´ç‰ˆ'
    )

    return parser.parse_args()


def validate_arguments(args):
    """éªŒè¯å‘½ä»¤è¡Œå‚æ•°çš„æœ‰æ•ˆæ€§"""
    errors = []

    # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    if args.preprocess:
        data_path = Path(args.data_dir)
        if not data_path.exists():
            errors.append(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼š{data_path}")
        elif not data_path.is_dir():
            errors.append(f"æ•°æ®è·¯å¾„ä¸æ˜¯ç›®å½•ï¼š{data_path}")

    # æ£€æŸ¥æ“ä½œç»„åˆçš„æœ‰æ•ˆæ€§
    operations = [args.preprocess, args.train, args.generate, args.cluster, args.analyze, args.crawl, args.scan, args.fuzz]
    active_operations = sum(operations)

    if active_operations == 0:
        errors.append("å¿…é¡»æŒ‡å®šè‡³å°‘ä¸€ä¸ªæ“ä½œï¼š--preprocess, --train, --generate, --cluster, --analyze, --crawl, --scan, --fuzz")

    # ========== æ–°å¢ï¼šanalyzeåŠŸèƒ½å¾…å®ç° ==========
    if args.analyze:
        print("[WARNING] --analyze åŠŸèƒ½å°šæœªå®ç°ï¼Œå°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­æ¨å‡º")
        print("[INFO] è®¡åˆ’åŒ…å«ï¼šé‡æ„å‡†ç¡®ç‡ã€æœ‰æ•ˆæ ·æœ¬ç‡ã€èšç±»è´¨é‡è¯„ä¼°ç­‰")
        # æ³¨æ„ï¼šè¿™é‡Œä¸è¿”å›ï¼Œè®©ä¸»æµç¨‹ç»§ç»­ï¼Œmain()å‡½æ•°ä¸­ä¼šè·³è¿‡analyzeçš„æ‰§è¡Œ

    # å¦‚æœæŒ‡å®šäº†clusterä½†æ²¡æœ‰generateï¼Œç»™å‡ºè­¦å‘Š
    if args.cluster and not args.generate:
        print("[WARNING] --cluster é€šå¸¸éœ€è¦ä¸ --generate ä¸€èµ·ä½¿ç”¨")

    # ========== æ–°å¢ï¼šç¬¬å››é˜¶æ®µå‚æ•°éªŒè¯ ==========
    if args.crawl or args.scan:
        if not args.url:
            errors.append("--crawl å’Œ --scan æ¨¡å¼å¿…é¡»æŒ‡å®š --url å‚æ•°")

        # æ·±åº¦éªŒè¯
        if args.depth < 0 or args.depth > 10:
            errors.append("--depth å¿…é¡»åœ¨ 0-10 ä¹‹é—´")

        # å¦‚æœåŒæ—¶æŒ‡å®šäº†--crawlå’Œ--scanï¼Œç»™å‡ºè­¦å‘Š
        if args.crawl and args.scan:
            print("[WARNING] --crawl å’Œ --scan åŒæ—¶æŒ‡å®šï¼Œå°†åªæ‰§è¡Œ --scan æ¨¡å¼")

    # ========== æ–°å¢ï¼šBaseFuzzå‚æ•°éªŒè¯ ==========
    if args.fuzz:
        # BaseFuzzå¿…é¡»æŒ‡å®š --url æˆ– --file ä¹‹ä¸€
        if not args.url and not args.file:
            errors.append("--fuzz æ¨¡å¼å¿…é¡»æŒ‡å®š --url æˆ– --file å‚æ•°")

        # å¦‚æœæŒ‡å®šäº†--fileï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if args.file:
            file_path = Path(args.file)
            if not file_path.exists():
                errors.append(f"--file æŒ‡å®šçš„æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
            elif not file_path.suffix == '.json':
                print(f"[WARNING] --file æŒ‡å®šçš„æ–‡ä»¶ä¸æ˜¯JSONæ ¼å¼: {args.file}")

        # çº¿ç¨‹æ•°éªŒè¯
        if args.threads < 1 or args.threads > 100:
            errors.append("--threads å¿…é¡»åœ¨ 1-100 ä¹‹é—´")

        # æ¨¡å¼éªŒè¯
        if args.mode == 'cvae':
            # CVAEæ¨¡å¼éœ€è¦æ£€æŸ¥ç”Ÿæˆè½½è·æ˜¯å¦å­˜åœ¨
            cvae_payloads = Path("Data/processed/fuzzing/refined_payloads.txt")
            if not cvae_payloads.exists():
                print(f"[WARNING] CVAEæ¨¡å¼æœªæ‰¾åˆ°è½½è·æ–‡ä»¶: {cvae_payloads}")
                print(f"[INFO] è¯·å…ˆç”Ÿæˆè½½è·: python fuzzmain.py --generate --cluster")

    return errors


def print_banner():
    """æ‰“å°ç¨‹åºå¯åŠ¨æ¨ªå¹…"""
    banner = """
================================================================
å¯åŠ¨ï¼ï¼ï¼ï¼ï¼
================================================================
"""
    print(banner)


def execute_preprocess(args):
    """æ‰§è¡Œæ•°æ®é¢„å¤„ç†"""
    print("\n" + "=" * 60)
    print("é˜¶æ®µä¸€ï¼šæ•°æ®é¢„å¤„ç†")
    print("=" * 60)

    try:
        # ========== ğŸ”¥ ä¼˜åŒ–1ï¼šæ£€æŸ¥Data_processingç›®å½• ==========
        data_processing_dir = os.path.join(project_root, "Data_processing")
        if not os.path.exists(data_processing_dir):
            print(f"\n[ERROR] Data_processingç›®å½•ä¸å­˜åœ¨: {data_processing_dir}")
            print("[INFO] è¯·ç¡®ä¿é¡¹ç›®ç»“æ„å®Œæ•´")
            return False

        # ========== ğŸ”¥ ä¼˜åŒ–2ï¼šæ£€æŸ¥æ•°æ®ç›®å½•ä¸­çš„jsonlæ–‡ä»¶ ==========
        data_dir = Path(args.data_dir)
        if not data_dir.exists():
            print(f"\n[ERROR] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return False

        # æŸ¥æ‰¾jsonlæ–‡ä»¶
        jsonl_files = list(data_dir.glob("*.jsonl"))

        if not jsonl_files:
            print(f"\n[WARNING] æ•°æ®ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°.jsonlæ–‡ä»¶: {data_dir}")
            print(f"[INFO] å‘ç°çš„æ–‡ä»¶ç±»å‹: {list(data_dir.glob('*'))}")
            print("\n[æç¤º] æ•°æ®ç›®å½•ä¸ºç©ºï¼Œå¯èƒ½éœ€è¦å…ˆè¿è¡Œåˆ†ç±»å™¨å‡†å¤‡æ•°æ®ï¼š")
            print(f"        python Data_processing/categorize_fuzz_dicts.py")
            print(f"\n        æˆ–è€…æ‰‹åŠ¨å‡†å¤‡æ•°æ®åˆ°: {args.data_dir}")

            # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
            try:
                user_input = input("\næ˜¯å¦ä»è¦ç»§ç»­æ‰§è¡Œé¢„å¤„ç†ï¼Ÿ(y/N): ").strip().lower()
                if user_input != 'y':
                    print("[INFO] ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                    return False
            except (EOFError, KeyboardInterrupt):
                print("\n[INFO] ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return False

        # ========== ä¼˜åŒ–3ï¼šæ˜¾ç¤ºæ‰¾åˆ°çš„æ•°æ®æ–‡ä»¶ ==========
        if jsonl_files:
            print(f"[INFO] æ‰¾åˆ° {len(jsonl_files)} ä¸ªæ•°æ®æ–‡ä»¶:")
            for jsonl_file in sorted(jsonl_files):
                file_size = jsonl_file.stat().st_size
                print(f"  - {jsonl_file.name}: {file_size:,} å­—èŠ‚")

        # æ„å»ºpreprocessorå‚æ•°
        # ä¿®å¤ï¼šå¿…é¡»æ˜¾å¼æ·»åŠ '--preprocess'å‚æ•°ï¼Œè§¦å‘å®é™…å¤„ç†é€»è¾‘
        preprocess_args = [
            '--preprocess',  # å¿…éœ€å‚æ•°ï¼Œå‘Šè¯‰preprocessor.pyæ‰§è¡Œé¢„å¤„ç†
            '--data-dir', args.data_dir,
            '--output-dir', args.output_dir,
            '--max-length', str(args.max_length),
            '--vocab-size', str(args.vocab_size)
        ]

        # ä¿®å¤ï¼šç§»é™¤--verboseå‚æ•°ä¼ é€’ï¼Œpreprocessor.pyä¸æ”¯æŒæ­¤å‚æ•°
        # ä¸å†ä¼ é€’ --verbose ç»™ preprocessor.py

        print(f"\n[INFO] æ•°æ®ç›®å½•: {args.data_dir}")
        print(f"[INFO] è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"[INFO] æœ€å¤§åºåˆ—é•¿åº¦: {args.max_length}")
        print(f"[INFO] è¯è¡¨å¤§å°: {args.vocab_size}")

        # è°ƒç”¨preprocessorçš„mainå‡½æ•°
        import sys
        old_argv = sys.argv
        sys.argv = ['preprocessor.py'] + preprocess_args
        try:
            preprocess_main()
        finally:
            sys.argv = old_argv

        print("\n[SUCCESS] æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
        return True

    except Exception as e:
        print(f"\n[ERROR] æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return False


def execute_train(args):
    """æ‰§è¡ŒCVAEæ¨¡å‹è®­ç»ƒ"""
    print("\n" + "=" * 60)
    print("é˜¶æ®µäºŒï¼šCVAEæ¨¡å‹è®­ç»ƒ")
    print("=" * 60)

    try:
        # ========== ä¼˜åŒ–1ï¼šæ£€æŸ¥Stage 1è¾“å‡ºæ–‡ä»¶ ==========
        processed_data_path = os.path.join(args.output_dir, "processed_data.pt")
        vocab_path = os.path.join(args.output_dir, "vocab.json")

        if not os.path.exists(processed_data_path):
            print(f"\n[ERROR] æ‰¾ä¸åˆ°é¢„å¤„ç†æ•°æ®æ–‡ä»¶: {processed_data_path}")
            print("[INFO] è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†ï¼špython fuzzmain.py --preprocess")
            return False

        if not os.path.exists(vocab_path):
            print(f"\n[ERROR] æ‰¾ä¸åˆ°è¯è¡¨æ–‡ä»¶: {vocab_path}")
            print("[INFO] è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†ï¼špython fuzzmain.py --preprocess")
            return False

        print(f"[INFO] é¢„å¤„ç†æ•°æ®æ–‡ä»¶: {processed_data_path}")
        print(f"[INFO] è¯è¡¨æ–‡ä»¶: {vocab_path}")

        # ========== ä¼˜åŒ–2ï¼šæ£€æŸ¥traineræ¨¡å—æ˜¯å¦å­˜åœ¨ ==========
        trainer_module_path = os.path.join(project_root, "Data_processing", "trainer.py")
        if not os.path.exists(trainer_module_path):
            print(f"\n[ERROR] trainer.pyæ¨¡å—ä¸å­˜åœ¨: {trainer_module_path}")
            print("\n[æç¤º] CVAEè®­ç»ƒæ¨¡å—å°šæœªå®ç°ï¼Œéœ€è¦æ ¹æ®Doc/promptæŒ‡å¯¼.mdåˆ›å»ºï¼š")
            print("  - Seq2Seq CVAEæ¶æ„ï¼ˆEncoder: Bi-GRU, Decoder: GRUï¼‰")
            print("  - Gumbel-Softmaxé‡å‚æ•°åŒ–ï¼ˆè§£å†³ç¦»æ•£æ–‡æœ¬ç”Ÿæˆï¼‰")
            print("  - KLé€€ç«ç­–ç•¥ï¼ˆé˜²æ­¢Posterior Collapseï¼‰")
            print("  - æŸå¤±å‡½æ•°ï¼šReconstruction Loss + Î²Â·KL Divergence")
            print("\nå‚è€ƒå®ç°:")
            print(f"  æ•°å­¦å®šä¹‰: Doc/promptæŒ‡å¯¼.md ç¬¬2èŠ‚")
            print(f"  è¶…å‚æ•°: epochs={args.epochs}, batch_size={args.batch_size}")
            print(f"          embed_dim={args.embed_dim}, hidden_dim={args.hidden_dim}")
            print(f"          latent_dim={args.latent_dim}, num_layers={args.num_layers}")
            return False

        # ========== ä¼˜åŒ–3ï¼šå¯¼å…¥traineræ¨¡å— ==========
        try:
            from Data_processing.trainer import CVAETrainer
        except ImportError as e:
            print(f"\n[ERROR] å¯¼å…¥traineræ¨¡å—å¤±è´¥: {e}")
            print(f"[INFO] trainer.pyè·¯å¾„: {trainer_module_path}")
            return False

        # ========== ä¼˜åŒ–4ï¼šæ„å»ºè®­ç»ƒé…ç½®å­—å…¸ ==========
        config = {
            # æ•°æ®è·¯å¾„
            'data_path': processed_data_path,
            'vocab_path': vocab_path,
            'output_dir': args.output_dir,

            # æ¨¡å‹æ¶æ„å‚æ•°
            'vocab_size': args.vocab_size,
            'embed_dim': args.embed_dim,
            'hidden_dim': args.hidden_dim,
            'latent_dim': args.latent_dim,
            'num_layers': args.num_layers,
            'condition_dim': args.condition_dim,

            # è®­ç»ƒè¶…å‚æ•°
            'epochs': args.epochs,
            'batch_size': args.batch_size,
            'learning_rate': args.learning_rate,
            'weight_decay': args.weight_decay,
            'train_split': args.train_split,
            'oversample': args.oversample,

            # KLé€€ç«å‚æ•°
            'kl_cycles': args.kl_cycles,
            'beta_max': args.beta_max,
            'delay_epochs': args.delay_epochs,

            # Gumbel-Softmaxæ¸©åº¦å‚æ•°
            'tau_init': 1.0,
            'tau_min': 0.5,
            'tau_decay': 0.99995,
        }

        print("\n[INFO] è®­ç»ƒé…ç½®å‚æ•°:")
        print(f"  - æ•°æ®æ–‡ä»¶: {config['data_path']}")
        print(f"  - è¯è¡¨æ–‡ä»¶: {config['vocab_path']}")
        print(f"  - æ¨¡å‹æ¶æ„: vocab_size={config['vocab_size']}, embed_dim={config['embed_dim']}")
        print(f"              hidden_dim={config['hidden_dim']}, latent_dim={config['latent_dim']}")
        print(f"              num_layers={config['num_layers']}, condition_dim={config['condition_dim']}")
        print(f"  - è®­ç»ƒå‚æ•°: epochs={config['epochs']}, batch_size={config['batch_size']}")
        print(f"              learning_rate={config['learning_rate']}, weight_decay={config['weight_decay']}")
        print(f"              train_split={config['train_split']}, oversample={config['oversample']}")
        print(f"  - KLé€€ç«: cycles={config['kl_cycles']}, beta_max={config['beta_max']}, delay_epochs={config['delay_epochs']}")

        # ========== ğŸ”¥ è€ç‹ä¼˜åŒ–5ï¼šåˆå§‹åŒ–trainerå¹¶å¼€å§‹è®­ç»ƒ ==========
        print("\n[INFO] åˆå§‹åŒ–CVAEè®­ç»ƒå™¨...")
        trainer = CVAETrainer(config)

        print("[INFO] å¼€å§‹è®­ç»ƒCVAEæ¨¡å‹...")
        print("[æç¤º] è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§ä»¥ä¸‹æŒ‡æ ‡:")
        print("  - Reconstruction Loss (é‡æ„æŸå¤±ï¼Œåº”è¯¥ä¸‹é™)")
        print("  - KL Divergence (KLæ•£åº¦ï¼Œåº”è¯¥é€æ¸ä¸Šå‡)")
        print("  - Total Loss (æ€»æŸå¤±ï¼Œåº”è¯¥å¹³ç¨³ä¸‹é™)")
        print("  - Betaå€¼ (KLæƒé‡ï¼Œå‘¨æœŸæ€§å˜åŒ–)")

        # è°ƒç”¨è®­ç»ƒæ–¹æ³•
        history = trainer.train()

        # ========== ä¼˜åŒ–6ï¼šéªŒè¯è¾“å‡ºæ¨¡å‹æ–‡ä»¶ ==========
        model_path = os.path.join(args.output_dir, "cvae.pth")
        if not os.path.exists(model_path):
            print(f"\n[WARNING] è®­ç»ƒå®Œæˆä½†æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
            print("[INFO] è¯·æ£€æŸ¥trainer.pyæ˜¯å¦æ­£ç¡®ä¿å­˜äº†æ¨¡å‹")
        else:
            print(f"\n[SUCCESS] æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            print(f"[INFO] æ¨¡å‹æ–‡ä»¶: {model_path}")
            file_size = os.path.getsize(model_path)
            print(f"[INFO] æ¨¡å‹å¤§å°: {file_size:,} å­—èŠ‚ ({file_size/1024/1024:.2f} MB)")

        # æ˜¾ç¤ºè®­ç»ƒå†å²
        if history and 'final_loss' in history:
            print(f"\n[INFO] è®­ç»ƒæ€»ç»“:")
            print(f"  - æœ€ç»ˆé‡æ„æŸå¤±: {history.get('final_recon_loss', 'N/A'):.4f}")
            print(f"  - æœ€ç»ˆKLæ•£åº¦: {history.get('final_kl_loss', 'N/A'):.4f}")
            print(f"  - æœ€ç»ˆæ€»æŸå¤±: {history['final_loss']:.4f}")

        return True

    except Exception as e:
        print(f"\n[ERROR] CVAEè®­ç»ƒå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return False


def execute_generate(args):
    """æ‰§è¡Œè½½è·ç”Ÿæˆ"""
    print("\n" + "=" * 60)
    print("é˜¶æ®µä¸‰ï¼šè½½è·ç”Ÿæˆ")
    print("=" * 60)

    try:
        # ========== ä¼˜åŒ–1ï¼šæ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨ ==========
        model_path = os.path.join(args.output_dir, "cvae.pth")
        vocab_path = os.path.join(args.output_dir, "vocab.json")

        if not os.path.exists(model_path):
            print(f"\n[ERROR] æ‰¾ä¸åˆ°CVAEæ¨¡å‹æ–‡ä»¶: {model_path}")
            print("[INFO] è¯·å…ˆè®­ç»ƒæ¨¡å‹ï¼špython fuzzmain.py --train")
            return False

        if not os.path.exists(vocab_path):
            print(f"\n[ERROR] æ‰¾ä¸åˆ°è¯è¡¨æ–‡ä»¶: {vocab_path}")
            print("[INFO] è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†ï¼špython fuzzmain.py --preprocess")
            return False

        print(f"[INFO] CVAEæ¨¡å‹æ–‡ä»¶: {model_path}")
        print(f"[INFO] è¯è¡¨æ–‡ä»¶: {vocab_path}")

        # ========== ä¼˜åŒ–2ï¼šåˆå§‹åŒ–CVAEç”Ÿæˆå™¨ ==========
        print("\n[INFO] åˆå§‹åŒ–CVAEç”Ÿæˆå™¨...")
        generator = CVAEGenerator(
            model_path=model_path,
            vocab_path=vocab_path,
            device='auto'
        )

        # ========== ä¼˜åŒ–3ï¼šå¤„ç†æ”»å‡»ç±»å‹å‚æ•° ==========
        if ',' in args.attack_type:
            attack_types = [t.strip() for t in args.attack_type.split(',')]
        else:
            attack_types = args.attack_type

        print(f"[INFO] ç”Ÿæˆå‚æ•°:")
        print(f"  - æ”»å‡»ç±»å‹: {attack_types}")
        print(f"  - æ¯ç±»æ ·æœ¬æ•°: {args.num_samples}")
        print(f"  - æ¸©åº¦å‚æ•°: {args.temperature}")
        print(f"  - æ‰¹å¤„ç†å¤§å°: {args.generation_batch_size}")

        # ========== ä¼˜åŒ–4ï¼šç”Ÿæˆè½½è· ==========
        print("\n[INFO] å¼€å§‹ç”Ÿæˆæ”»å‡»è½½è·...")
        payloads, metadata = generator.generate_payloads(
            attack_types=attack_types,
            num_samples=args.num_samples,
            temperature=args.temperature,
            max_length=150,
            batch_size=args.generation_batch_size
        )

        print(f"\n[SUCCESS] è½½è·ç”Ÿæˆå®Œæˆï¼æ€»è®¡: {len(payloads)} ä¸ª")

        # ==========ä¼˜åŒ–5ï¼šæ¸…æ´—è½½è· ==========
        print("\n[INFO] æ¸…æ´—æ— æ•ˆè½½è·...")
        cleaned_payloads, cleaned_metadata = generator.clean_payloads(payloads, metadata)

        valid_ratio = len(cleaned_payloads) / len(payloads) * 100
        print(f"[INFO] æ¸…æ´—å®Œæˆï¼æœ‰æ•ˆè½½è·: {len(cleaned_payloads)}/{len(payloads)} ({valid_ratio:.1f}%)")

        # ========== ä¼˜åŒ–6ï¼šæå–éšç©ºé—´ç‰¹å¾ ==========
        print("\n[INFO] æå–éšç©ºé—´ç‰¹å¾...")
        embeddings, valid_mask = generator.get_embeddings(cleaned_payloads, cleaned_metadata)

        print(f"[INFO] éšç©ºé—´ç‰¹å¾: {embeddings.shape}")
        print(f"[INFO] æœ‰æ•ˆç‰¹å¾æ•°: {np.sum(valid_mask)}")

        # ========== ä¼˜åŒ–7ï¼šä¿å­˜ç”Ÿæˆæ•°æ® ==========
        generated_dir = os.path.join(args.output_dir, "generated")
        print(f"\n[INFO] ä¿å­˜ç”Ÿæˆæ•°æ®åˆ°: {generated_dir}")
        generator.save_generated_data(cleaned_payloads, cleaned_metadata, generated_dir)

        # ä¿å­˜éšç©ºé—´ç‰¹å¾å’Œæœ‰æ•ˆæ€§æ©ç 
        embeddings_file = os.path.join(generated_dir, "latent_embeddings.npy")
        valid_mask_file = os.path.join(generated_dir, "valid_mask.npy")

        np.save(embeddings_file, embeddings)
        np.save(valid_mask_file, valid_mask)

        print(f"[SUCCESS] éšç©ºé—´ç‰¹å¾å·²ä¿å­˜: {embeddings_file}")
        print(f"[SUCCESS] æœ‰æ•ˆæ€§æ©ç å·²ä¿å­˜: {valid_mask_file}")

        # ========== ä¼˜åŒ–8ï¼šæ˜¾ç¤ºç”Ÿæˆç»Ÿè®¡ ==========
        print("\n[INFO] ç”Ÿæˆç»Ÿè®¡:")
        type_counts = {}
        for meta in cleaned_metadata:
            attack_type = meta['type']
            type_counts[attack_type] = type_counts.get(attack_type, 0) + 1

        for attack_type, count in sorted(type_counts.items()):
            percentage = count / len(cleaned_metadata) * 100
            print(f"  - {attack_type:>8}: {count:>6} æ¡ ({percentage:>5.1f}%)")

        print("\n[SUCCESS] è½½è·ç”Ÿæˆé˜¶æ®µå®Œæˆï¼")
        return True

    except Exception as e:
        print(f"\n[ERROR] è½½è·ç”Ÿæˆå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return False


def execute_cluster(args):
    """æ‰§è¡ŒDBSCANèšç±»ä¼˜åŒ–"""
    print("\n" + "=" * 60)
    print("é˜¶æ®µä¸‰ç»­ï¼šDBSCANèšç±»ä¼˜åŒ–")
    print("=" * 60)

    try:
        # ========== ä¼˜åŒ–1ï¼šæ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨ ==========
        generated_dir = os.path.join(args.output_dir, "generated")

        embeddings_file = os.path.join(generated_dir, "latent_embeddings.npy")
        payloads_file = os.path.join(generated_dir, "raw_payloads.txt")
        metadata_file = os.path.join(generated_dir, "payload_metadata.json")
        valid_mask_file = os.path.join(generated_dir, "valid_mask.npy")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æç¤ºå…ˆè¿è¡Œç”Ÿæˆ
        missing_files = []
        if not os.path.exists(embeddings_file):
            missing_files.append("latent_embeddings.npy")
        if not os.path.exists(payloads_file):
            missing_files.append("raw_payloads.txt")
        if not os.path.exists(metadata_file):
            missing_files.append("payload_metadata.json")

        if missing_files:
            print(f"\n[ERROR] æ‰¾ä¸åˆ°å¿…è¦æ–‡ä»¶: {', '.join(missing_files)}")
            print(f"[INFO] è¯·å…ˆç”Ÿæˆè½½è·ï¼špython fuzzmain.py --generate")
            return False

        print(f"[INFO] éšç©ºé—´ç‰¹å¾æ–‡ä»¶: {embeddings_file}")
        print(f"[INFO] è½½è·æ–‡ä»¶: {payloads_file}")
        print(f"[INFO] å…ƒæ•°æ®æ–‡ä»¶: {metadata_file}")

        # ========== ä¼˜åŒ–2ï¼šåŠ è½½æ•°æ® ==========
        print("\n[INFO] åŠ è½½æ•°æ®æ–‡ä»¶...")
        embeddings = np.load(embeddings_file)
        print(f"[INFO] éšç©ºé—´ç‰¹å¾: {embeddings.shape}")

        with open(payloads_file, 'r', encoding='utf-8') as f:
            payloads = [line.strip() for line in f.readlines()]
        print(f"[INFO] è½½è·æ•°é‡: {len(payloads)}")

        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        print(f"[INFO] å…ƒæ•°æ®æ•°é‡: {len(metadata)}")

        # åŠ è½½æœ‰æ•ˆæ€§æ©ç ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        valid_mask = None
        if os.path.exists(valid_mask_file):
            valid_mask = np.load(valid_mask_file)
            print(f"[INFO] æœ‰æ•ˆæ€§æ©ç : {valid_mask.shape}")
            print(f"[INFO] æœ‰æ•ˆæ ·æœ¬: {np.sum(valid_mask)} ({np.sum(valid_mask)/len(valid_mask)*100:.1f}%)")

        # ========== ä¼˜åŒ–3ï¼šåˆå§‹åŒ–èšç±»å™¨ ==========
        print("\n[INFO] åˆå§‹åŒ–CVAEèšç±»å™¨...")
        clusterer = CVAEClusterer(
            embeddings=embeddings,
            payloads=payloads,
            metadata=metadata,
            valid_mask=valid_mask,
            label_weight=15.0  # ä½¿ç”¨å¼ºæ ‡ç­¾æƒé‡è¿›è¡Œç±»å‹éš”ç¦»
        )

        # ========== ä¼˜åŒ–4ï¼šæ‰§è¡ŒDBSCANèšç±» ==========
        print("\n[INFO] å¼€å§‹DBSCANèšç±»åˆ†æ...")
        print(f"[INFO] èšç±»å‚æ•°:")
        print(f"  - eps: {args.eps if args.eps else 'è‡ªåŠ¨å¯»æ‰¾'}")
        print(f"  - min_samples: {args.min_samples if args.min_samples else '3 (é»˜è®¤)'}")

        clustering_results = clusterer.perform_clustering(
            eps=args.eps,
            min_samples=args.min_samples
        )

        # æ˜¾ç¤ºèšç±»ç»“æœ
        print(f"\n[SUCCESS] èšç±»å®Œæˆï¼")
        print(f"[INFO] èšç±»ç»“æœ:")
        print(f"  - ç°‡æ•°é‡: {clustering_results['n_clusters']}")
        print(f"  - å™ªå£°ç‚¹: {clustering_results['n_noise']} ({clustering_results['n_noise']/len(payloads)*100:.1f}%)")

        if clustering_results.get('silhouette_score'):
            print(f"  - è½®å»“ç³»æ•°: {clustering_results['silhouette_score']:.3f}")

        # ========== ä¼˜åŒ–5ï¼šé™ç»´å¤„ç†ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰ ==========
        if args.visualize:
            print(f"\n[INFO] æ‰§è¡Œé™ç»´å¤„ç† ({args.reduction_method})...")
            clusterer.reduce_dimensions(method=args.reduction_method)

        # ========== ä¼˜åŒ–6ï¼šç­›é€‰ç²¾é”è½½è· ==========
        print("\n[INFO] ç­›é€‰ç²¾é”è½½è·...")
        print(f"[INFO] ç­›é€‰å‚æ•°:")
        print(f"  - æ¯ç°‡æ ·æœ¬æ•°: {args.samples_per_cluster}")
        print(f"  - ä¿ç•™å™ªå£°ç‚¹: {args.keep_noise}")

        refined_payloads = clusterer.select_refined_payloads(
            samples_per_cluster=args.samples_per_cluster,
            keep_all_noise=args.keep_noise
        )

        # è®¡ç®—å‹ç¼©æ¯”ä¾‹
        reduction_ratio = (len(payloads) - len(refined_payloads)) / len(payloads) * 100
        print(f"\n[SUCCESS] ç²¾é”è½½è·ç­›é€‰å®Œæˆï¼")
        print(f"[INFO] ç­›é€‰ç»“æœ:")
        print(f"  - åŸå§‹æ ·æœ¬: {len(payloads)}")
        print(f"  - ç²¾é”æ ·æœ¬: {len(refined_payloads)}")
        print(f"  - å‹ç¼©æ¯”ä¾‹: {reduction_ratio:.1f}%")

        # ========== ä¼˜åŒ–7ï¼šä¿å­˜èšç±»ç»“æœ ==========
        clustered_dir = os.path.join(args.output_dir, "clustered")
        print(f"\n[INFO] ä¿å­˜èšç±»ç»“æœåˆ°: {clustered_dir}")
        clusterer.save_clustering_results(clustered_dir)

        # ========== ä¼˜åŒ–8ï¼šä¿å­˜ç²¾é”è½½è· ==========
        fuzzing_dir = os.path.join(args.output_dir, "fuzzing")
        os.makedirs(fuzzing_dir, exist_ok=True)

        refined_payloads_file = os.path.join(fuzzing_dir, "refined_payloads.txt")
        clusterer.save_refined_payloads(refined_payloads_file)
        print(f"[SUCCESS] ç²¾é”è½½è·å·²ä¿å­˜: {refined_payloads_file}")

        # ========== ä¼˜åŒ–9ï¼šç”Ÿæˆå¯è§†åŒ–å›¾åƒï¼ˆå¯é€‰ï¼‰ ==========
        if args.visualize:
            print("\n[INFO] ç”Ÿæˆèšç±»å¯è§†åŒ–å›¾åƒ...")
            viz_path = os.path.join(clustered_dir, "clustering_visualization.png")
            clusterer.visualize_clusters(
                method=args.reduction_method,
                save_path=viz_path
            )
            print(f"[SUCCESS] å¯è§†åŒ–å›¾åƒå·²ä¿å­˜: {viz_path}")

        # ========== ä¼˜åŒ–10ï¼šæ˜¾ç¤ºç°‡ç»Ÿè®¡ä¿¡æ¯ ==========
        print("\n[INFO] ç°‡ç»Ÿè®¡ä¿¡æ¯:")
        for cluster_id, cluster_info in clustering_results['cluster_info'].items():
            print(f"  - ç°‡ {cluster_id}: {cluster_info['size']} ä¸ªæ ·æœ¬, "
                  f"å¹³å‡è·ç¦»: {cluster_info['avg_distance_to_centroid']:.3f}")

        print("\n[SUCCESS] DBSCANèšç±»ä¼˜åŒ–é˜¶æ®µå®Œæˆï¼")
        print(f"[INFO] ç²¾é”è½½è·å·²å‡†å¤‡å¥½ç”¨äºFuzzæµ‹è¯•")
        print(f"[INFO] è½½è·æ–‡ä»¶: {refined_payloads_file}")

        return True

    except Exception as e:
        print(f"\n[ERROR] DBSCANèšç±»å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return False


def execute_scan_init(args):
    """
    ç¬¬å››é˜¶æ®µï¼šé»‘ç›’æ¨¡ç³Šæµ‹è¯•è°ƒåº¦å‡½æ•°

    è€ç‹æ³¨é‡Šï¼šè¿™ä¸ªSBå‡½æ•°è´Ÿè´£ï¼š
    1. ç«™ç‚¹ç¯å¢ƒåˆå§‹åŒ–ï¼ˆç›®å½•åˆ›å»ºï¼‰
    2. çˆ¬è™«æ‰§è¡Œæˆ–ç¼“å­˜åŠ è½½
    3. ä»»åŠ¡ç»Ÿè®¡å’Œå¯è§†åŒ–
    4. å‡†å¤‡è¿›å…¥Fuzzå¼•æ“é˜¶æ®µ

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡

    Returns:
        æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
    """
    print("\n" + "=" * 60)
    print("é˜¶æ®µå››ï¼šé»‘ç›’æ¨¡ç³Šæµ‹è¯•")
    print("=" * 60)

    try:
        # ========== ä¼˜åŒ–1ï¼šå¯¼å…¥çˆ¬è™«æ¨¡å— ==========
        try:
            sys.path.insert(0, os.path.join(project_root, "Fuzz"))
            from spider import CVDBSpider, extract_site_name
        except ImportError as e:
            print(f"\n[ERROR] å¯¼å…¥çˆ¬è™«æ¨¡å—å¤±è´¥: {e}")
            print(f"[INFO] è¯·ç¡®ä¿ Fuzz/spider.py æ–‡ä»¶å­˜åœ¨")
            return False

        # ========== ä¼˜åŒ–2ï¼šç«™ç‚¹ç¯å¢ƒåˆå§‹åŒ– ==========
        site_name = extract_site_name(args.url)
        print(f"\n[INFO] ç›®æ ‡ç«™ç‚¹: {site_name}")
        print(f"[INFO] åŸºç¡€URL: {args.url}")

        # åˆ›å»ºç›®å½•ç»“æ„
        results_dir = Path("Data/scan_results") / site_name
        results_dir.mkdir(parents=True, exist_ok=True)
        print(f"[INFO] ç»“æœç›®å½•: {results_dir}")

        # ========== ä¼˜åŒ–3ï¼šæ™ºèƒ½æ¨¡å¼åˆ¤æ–­ ==========
        # è‰¹ï¼æå‰å®šä¹‰å‚æ•°è¿‡æ»¤åˆ—è¡¨ï¼ˆåŒ…å«HTTPå¤´ï¼‰ï¼Œç”¨äºEngineåˆå§‹åŒ–
        param_filter_for_engine = []

        if not args.crawl:
            # ========== çº¯æ‰«ææ¨¡å¼ï¼ˆä¸çˆ¬è™«ï¼‰ ==========
            print(f"\n[INFO] çº¯æ‰«ææ¨¡å¼ï¼šä»…æµ‹è¯•æŒ‡å®šURL")
            print(f"[INFO] ç›®æ ‡URL: {args.url}")
            print(f"[INFO] è¯·æ±‚æ–¹æ³•: {args.method}")

            # å¯¼å…¥FuzzTarget
            from Fuzz.spider import FuzzTarget

            # æ‰‹åŠ¨æ„é€ å•ä¸ªFuzzTarget
            from urllib.parse import urlparse, parse_qs

            parsed = urlparse(args.url)

            params = {}
            data = {}


            # ========== æ–°å¢ï¼šæ”¯æŒ--paramsæ‰‹åŠ¨æŒ‡å®šå‚æ•° ==========
            if args.params:
                # ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šäº†æµ‹è¯•å‚æ•°
                # æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
                # 1. name1,name2ï¼ˆæµ‹è¯•è¿™äº›å‚æ•°ï¼Œå€¼è®¾ä¸ºFuzzï¼‰
                # 2. name1=value1,name2ï¼ˆname1å›ºå®šä¸ºvalue1ï¼Œname2æµ‹è¯•è®¾ä¸ºFuzzï¼‰
                manual_params = [p.strip() for p in args.params.split(',')]
                print(f"[INFO] æ‰‹åŠ¨æŒ‡å®šæµ‹è¯•å‚æ•°: {manual_params}")

                # æ ¹æ®è¯·æ±‚æ–¹æ³•è®¾ç½®å‚æ•°
                for param_def in manual_params:
                    if '=' in param_def:
                        # æœ‰=å·ï¼Œè§£æåç§°å’Œå€¼
                        param_name, param_value = param_def.split('=', 1)
                        param_name = param_name.strip()
                        param_value = param_value.strip()
                    else:
                        # æ²¡æœ‰=å·ï¼Œé»˜è®¤æµ‹è¯•å‚æ•°ï¼ˆå€¼ä¸ºFuzzï¼‰
                        param_name = param_def
                        param_value = 'Fuzz'

                    # æ ¹æ®è¯·æ±‚æ–¹æ³•æ”¾åˆ°ä¸åŒçš„å­—å…¸
                    if args.method == 'GET':
                        params[param_name] = param_value
                    else:  # POST
                        data[param_name] = param_value
            else:
                # æ²¡æœ‰æ‰‹åŠ¨æŒ‡å®šï¼Œè‡ªåŠ¨è§£æ
                if args.method == 'GET':
                    # GETè¯·æ±‚ï¼šä»URLä¸­è§£æå‚æ•°
                    if parsed.query:
                        params = {k: v[0] if v else '' for k, v in parse_qs(parsed.query).items()}
                else:  # POST
                    # POSTè¯·æ±‚ï¼šæ²¡æœ‰æ‰‹åŠ¨æŒ‡å®šå‚æ•°å°±è­¦å‘Š
                    print(f"[WARNING] POSTè¯·æ±‚éœ€è¦æ‰‹åŠ¨æŒ‡å®šå‚æ•°ï¼Œå¦‚: --params id,name")
                    return False

            # è‰¹ï¼æ–°å¢ï¼šè§£æHTTPå¤´æ³¨å…¥åˆ—è¡¨
            injectable_headers = {}
            # è‰¹ï¼åŒæ—¶æ”¶é›†è¦æµ‹è¯•çš„å‚æ•°åï¼ˆç”¨äºparam_filterï¼‰
            param_names_to_test = []
            if args.params:
                # è§£æ--paramså‚æ•°ï¼Œæå–å‚æ•°å
                for param_def in args.params.split(','):
                    param_def = param_def.strip()
                    if '=' in param_def:
                        param_name = param_def.split('=', 1)[0].strip()
                    else:
                        param_name = param_def
                    param_names_to_test.append(param_name)

            # è‰¹ï¼è°ƒè¯•æ—¥å¿—
            print(f"[DEBUG] args.headers = '{args.headers}'")
            print(f"[DEBUG] args.headersç±»å‹ = {type(args.headers)}")

            if args.headers and args.headers.strip():
                header_list = [h.strip() for h in args.headers.split(',')]
                print(f"[INFO] æŒ‡å®šHTTPå¤´æ³¨å…¥: {header_list}")

                # ä»Requesterè·å–é»˜è®¤å¤´å€¼
                from Fuzz.BaseFuzz.requester import Requester
                temp_requester = Requester(timeout=args.timeout)

                for header_name in header_list:
                    default_value = temp_requester.headers.get(header_name, '')
                    injectable_headers[header_name] = default_value
                    # è‰¹ï¼æŠŠHTTPå¤´ä¹ŸåŠ å…¥åˆ°è¦æµ‹è¯•çš„å‚æ•°åˆ—è¡¨ä¸­
                    param_names_to_test.append(header_name)
                    print(f"  - æ·»åŠ HTTPå¤´æ³¨å…¥ç‚¹: {header_name}={default_value[:30]}")
            else:
                print(f"[INFO] æœªæŒ‡å®šHTTPå¤´æ³¨å…¥")

            # è‰¹ï¼ä¿å­˜åˆ°å¤–éƒ¨å˜é‡ï¼ˆç”¨äºEngineåˆå§‹åŒ–ï¼‰
            param_filter_for_engine = param_names_to_test

            # è‰¹ï¼æ˜¾ç¤ºå®Œæ•´æµ‹è¯•å‚æ•°åˆ—è¡¨
            if param_names_to_test:
                print(f"[INFO] å®Œæ•´æµ‹è¯•å‚æ•°åˆ—è¡¨: {param_names_to_test}")
            else:
                print(f"[INFO] å®Œæ•´æµ‹è¯•å‚æ•°åˆ—è¡¨: è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å‚æ•°")

            # æ„é€ target
            target = FuzzTarget(
                url=args.url,
                method=args.method,
                params=params,
                data=data,
                injectable_headers=injectable_headers,  # æ–°å¢
                depth=0
            )

            targets = [target]

            print(f"[INFO] å·²æ„é€ æ‰«æç›®æ ‡: {len(targets)} ä¸ª")
            print(f"  - [{target.method}] {target.url}")
            if target.params:
                print(f"    GETå‚æ•°: {list(target.params.keys())}")
            if target.data:
                print(f"    POSTå‚æ•°: {list(target.data.keys())}")
            if target.injectable_headers:
                print(f"    HTTPå¤´æ³¨å…¥: {list(target.injectable_headers.keys())}")

        else:
            # ========== çˆ¬è™«+æ‰«ææ¨¡å¼ ==========
            print(f"[INFO] çˆ¬è™«+æ‰«ææ¨¡å¼ï¼šé€’å½’çˆ¬å–åæ‰«æ")
            print(f"[INFO] çˆ¬å–æ·±åº¦: {args.depth}")

            # åˆ›å»ºç¼“å­˜ç›®å½•
            cache_dir = Path("Data/cache") / site_name
            cache_dir.mkdir(parents=True, exist_ok=True)
            print(f"[INFO] ç¼“å­˜ç›®å½•: {cache_dir}")

            # ========== ä¼˜åŒ–4ï¼šç¼“å­˜åŠ è½½é€»è¾‘ ==========
            cache_file = cache_dir / "spider_cache.json"

            if args.use_cache and cache_file.exists():
                # å¤ç”¨ç¼“å­˜
                print(f"\n[INFO] æ£€æµ‹åˆ°å·²æœ‰ç¼“å­˜ï¼Œæ­£åœ¨åŠ è½½...")
                print(f"[INFO] ç¼“å­˜æ–‡ä»¶: {cache_file}")

                try:
                    spider = CVDBSpider.load_cache(str(cache_file))
                    print(f"[SUCCESS] ç¼“å­˜åŠ è½½æˆåŠŸï¼")
                    print(f"[INFO] ç¼“å­˜æ—¶é—´: {spider.stats.get('timestamp', 'N/A')}")

                except Exception as e:
                    print(f"[ERROR] ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
                    print(f"[INFO] å°†é‡æ–°æ‰§è¡Œçˆ¬å–...")
                    args.use_cache = False

            # ========== ä¼˜åŒ–5ï¼šæ‰§è¡Œçˆ¬è™«ï¼ˆå¦‚æœéœ€è¦ï¼‰ ==========
            if not args.use_cache or not cache_file.exists():
                # åˆå§‹åŒ–çˆ¬è™«
                print(f"\n[INFO] åˆå§‹åŒ–CVDBSpiderçˆ¬è™«...")
                spider = CVDBSpider(
                    base_url=args.url,
                    max_depth=args.depth,
                    timeout=10,
                    cookie=args.cookie
                )

                # æ‰§è¡Œçˆ¬å–
                print(f"[INFO] å¼€å§‹é€’å½’çˆ¬å–...")
                targets = spider.crawl()

                # ä¿å­˜ç¼“å­˜
                print(f"\n[INFO] ä¿å­˜çˆ¬è™«ç¼“å­˜...")
                cache_path = spider.save_cache(str(cache_dir))
                print(f"[SUCCESS] ç¼“å­˜å·²ä¿å­˜: {cache_path}")
            else:
                targets = spider.targets

        # ========== ä¼˜åŒ–6ï¼šä»»åŠ¡ç»Ÿè®¡è¡¨æ ¼ ==========
        # æ³¨æ„ï¼štargetsåœ¨ä¸¤ç§æ¨¡å¼ä¸‹éƒ½å·²ç»å®šä¹‰å¥½äº†

        print(f"\n{'='*60}")
        print("ä»»åŠ¡ç»Ÿè®¡è¡¨")
        print(f"{'='*60}")

        # ç»Ÿè®¡GETå’ŒPOSTä»»åŠ¡
        get_targets = [t for t in targets if t.method == 'GET']
        post_targets = [t for t in targets if t.method == 'POST']

        # æŒ‰æ·±åº¦ç»Ÿè®¡
        depth_stats = {}
        for target in targets:
            depth = target.depth
            depth_stats[depth] = depth_stats.get(depth, 0) + 1

        # æ‰“å°ç»Ÿè®¡è¡¨æ ¼
        print(f"\nä»»åŠ¡ç±»å‹ç»Ÿè®¡:")
        print(f"  - GET ä»»åŠ¡:  {len(get_targets)} ä¸ª")
        print(f"  - POST ä»»åŠ¡: {len(post_targets)} ä¸ª")
        print(f"  - æ€»ä»»åŠ¡æ•°:  {len(targets)} ä¸ª")

        print(f"\næ·±åº¦åˆ†å¸ƒç»Ÿè®¡:")
        for depth in sorted(depth_stats.keys()):
            count = depth_stats[depth]
            percentage = count / len(targets) * 100
            print(f"  - æ·±åº¦ {depth}:  {count} ä¸ª ({percentage:.1f}%)")

        # æ˜¾ç¤ºå‰5ä¸ªç›®æ ‡ç¤ºä¾‹
        if targets:
            print(f"\nç›®æ ‡ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
            for i, target in enumerate(targets[:5], 1):
                params_info = f"å‚æ•°: {len(target.params)}ä¸ª" if target.params else f"å­—æ®µ: {len(target.data)}ä¸ª"
                print(f"  {i}. [{target.method}] {target.url[:60]}... ({params_info})")

        # ========== ä¼˜åŒ–6ï¼šä¿å­˜ä»»åŠ¡åˆ—è¡¨åˆ°æ–‡ä»¶ ==========
        targets_file = results_dir / "fuzz_targets.json"
        print(f"\n[INFO] ä¿å­˜ä»»åŠ¡åˆ—è¡¨åˆ°: {targets_file}")

        targets_data = {
            'site_name': site_name,
            'base_url': args.url,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_targets': len(targets),
            'get_targets': len(get_targets),
            'post_targets': len(post_targets),
            'depth_stats': depth_stats,
            'targets': [target.to_dict() for target in targets]
        }

        with open(targets_file, 'w', encoding='utf-8') as f:
            json.dump(targets_data, f, indent=2, ensure_ascii=False)

        print(f"[SUCCESS] ä»»åŠ¡åˆ—è¡¨å·²ä¿å­˜")

        # ========== ä¼˜åŒ–7ï¼šæ¨¡å¼åˆ¤æ–­ ==========
        # åªæœ‰--crawlï¼ˆæ²¡æœ‰--scanï¼‰ï¼šçº¯çˆ¬è™«æ¨¡å¼ï¼Œç›´æ¥é€€å‡º
        if args.crawl and not args.scan:
            print(f"\n{'='*60}")
            print(f"[SUCCESS] çº¯çˆ¬è™«æ¨¡å¼å®Œæˆï¼")
            print(f"[INFO] å·²å‘ç° {len(targets)} ä¸ªFuzzç›®æ ‡")
            print(f"[INFO] ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›å…¥æ‰«ææ¨¡å¼ï¼š")
            print(f"        python fuzzmain.py --scan --url {args.url} --use-cache")
            print(f"{'='*60}\n")
            return True

        # æœ‰--scanå‚æ•°ï¼šè¿›å…¥Fuzzå¼•æ“
        if args.scan:
            # æ‰«ææ¨¡å¼ï¼ˆå¯èƒ½å¸¦çˆ¬è™«ï¼Œä¹Ÿå¯èƒ½ä¸å¸¦ï¼‰
            if not args.crawl:
                print(f"\n{'='*60}")
                print(f"[INFO] çº¯æ‰«ææ¨¡å¼ï¼šç›´æ¥æµ‹è¯•æŒ‡å®šURL")
                print(f"[INFO] å¾…æ³¨å…¥ç›®æ ‡: {len(targets)} ä¸ª")
                print(f"{'='*60}\n")
            else:
                print(f"\n{'='*60}")
                print(f"[SUCCESS] çˆ¬è™«é˜¶æ®µå®Œæˆï¼")
                print(f"[INFO] ç›®æ ‡å·²ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶")
                print(f"[INFO] å¾…æ³¨å…¥ç›®æ ‡: {len(targets)} ä¸ª")
                print(f"{'='*60}\n")

            return True

    except Exception as e:
        print(f"\n[ERROR] ç¬¬å››é˜¶æ®µæ‰§è¡Œå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return False


def execute_basefuzz(args):
    """
    BaseFuzzå¼•æ“æ‰§è¡Œå‡½æ•°

    è´Ÿè´£å®Œæ•´çš„BaseFuzzæµç¨‹ï¼š
    1. åŠ è½½ç›®æ ‡åˆ—è¡¨ï¼ˆä»spiderç¼“å­˜æˆ–--urlçˆ¬å–ï¼‰
    2. åˆå§‹åŒ–BaseFuzz Engine
    3. æ‰§è¡Œæ¨¡ç³Šæµ‹è¯•
    4. ç”Ÿæˆåˆ†ææŠ¥å‘Š

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡

    Returns:
        æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
    """
    print("\n" + "=" * 70)
    print("BaseFuzzå¼•æ“ - æ™ºèƒ½æ¨¡ç³Šæµ‹è¯•")
    print("=" * 70)

    try:
        # ========== æ­¥éª¤1ï¼šå¯¼å…¥BaseFuzzæ¨¡å— ==========
        print("\n[INFO] å¯¼å…¥BaseFuzzæ¨¡å—...")

        try:
            sys.path.insert(0, os.path.join(project_root, "Fuzz/BaseFuzz"))
            from Fuzz.BaseFuzz.engine import Engine
            from Fuzz.BaseFuzz.analysis import Analyzer, Reporter
        except ImportError as e:
            print(f"\n[ERROR] å¯¼å…¥BaseFuzzæ¨¡å—å¤±è´¥: {e}")
            print(f"[INFO] è¯·ç¡®ä¿ Fuzz/BaseFuzz/engine.py å­˜åœ¨")
            return False

        # ========== æ­¥éª¤2ï¼šè·å–ç›®æ ‡åˆ—è¡¨ ==========
        print("\n[æ­¥éª¤1] å‡†å¤‡æµ‹è¯•ç›®æ ‡")
        print("-" * 70)

        targets = []

        if args.file:
            # ä»æ–‡ä»¶åŠ è½½spiderç¼“å­˜
            print(f"[INFO] ä»ç¼“å­˜æ–‡ä»¶åŠ è½½ç›®æ ‡: {args.file}")

            try:
                with open(args.file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)

                # å¯¼å…¥FuzzTarget
                from Fuzz.spider import FuzzTarget

                # ä»ç¼“å­˜ä¸­æå–targets
                targets_data = cache_data.get('targets', [])
                targets = [FuzzTarget.from_dict(t) for t in targets_data]

                print(f"[SUCCESS] å·²åŠ è½½ {len(targets)} ä¸ªç›®æ ‡")

            except Exception as e:
                print(f"[ERROR] ç¼“å­˜æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
                return False

        elif args.url:
            # å…ˆå°è¯•åŠ è½½ç¼“å­˜
            print(f"[INFO] ç›®æ ‡URL: {args.url}")

            # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰ç¼“å­˜
            from Fuzz.spider import extract_site_name
            from urllib.parse import urlparse, parse_qs
            from Fuzz.spider import FuzzTarget

            site_name = extract_site_name(args.url)
            cache_dir = Path("Data/cache") / site_name
            cache_file = cache_dir / "spider_cache.json"

            if cache_file.exists():
                print(f"[INFO] å‘ç°ç°æœ‰ç¼“å­˜: {cache_file}")

                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)

                    targets_data = cache_data.get('targets', [])
                    targets = [FuzzTarget.from_dict(t) for t in targets_data]

                    print(f"[SUCCESS] å·²ä»ç¼“å­˜åŠ è½½ {len(targets)} ä¸ªç›®æ ‡")

                except Exception as e:
                    print(f"[WARNING] ç¼“å­˜åŠ è½½å¤±è´¥: {e}ï¼Œå°†ç›´æ¥ä»URLåˆ›å»ºç›®æ ‡")
                    targets = []
            else:
                print(f"[INFO] æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œå°†ç›´æ¥ä»URLåˆ›å»ºç›®æ ‡")
                targets = []

            # å¦‚æœç¼“å­˜ä¸ºç©ºï¼Œç›´æ¥ä»URLåˆ›å»ºFuzzTarget
            if not targets:
                print(f"[INFO] ç›´æ¥ä»URLåˆ›å»ºæµ‹è¯•ç›®æ ‡...")

                try:
                    # è§£æURL
                    parsed = urlparse(args.url)
                    query_params = parse_qs(parsed.query)

                    # parse_qsè¿”å›çš„å€¼æ˜¯åˆ—è¡¨ï¼Œéœ€è¦æå–ç¬¬ä¸€ä¸ªå€¼
                    # ä¾‹å¦‚ï¼š{'id': ['1'], 'Submit': ['æäº¤']}
                    # éœ€è¦è½¬æ¢ä¸ºï¼š{'id': '1', 'Submit': 'æäº¤'}
                    params = {k: v[0] if v else '' for k, v in query_params.items()}

                    # æ–°å¢ï¼šæ”¯æŒ--paramsæ‰‹åŠ¨æŒ‡å®šå‚æ•°ï¼
                    # æ”¯æŒè¯­æ³•ï¼š
                    # - name=value â†’ æµ‹è¯•nameå‚æ•°ï¼Œåˆå§‹å€¼ä¸ºvalue
                    # - name=@value â†’ å›ºå®šnameå‚æ•°ä¸ºvalueï¼Œä¸æµ‹è¯•ï¼ˆ@å‰ç¼€è¡¨ç¤ºå›ºå®šå€¼ï¼‰
                    # - name â†’ æµ‹è¯•nameå‚æ•°ï¼Œåˆå§‹å€¼ä¸ºFuzz
                    if args.params:
                        manual_params = [p.strip() for p in args.params.split(',')]
                        print(f"[INFO] æ‰‹åŠ¨æŒ‡å®šæµ‹è¯•å‚æ•°: {manual_params}")

                        # æ ¹æ®è¯·æ±‚æ–¹æ³•è®¾ç½®å‚æ•°
                        manual_data = {}
                        manual_params_dict = {}
                        for param_def in manual_params:
                            if '=' in param_def:
                                # æœ‰=å·ï¼Œè§£æåç§°å’Œå€¼
                                param_name, param_value = param_def.split('=', 1)
                                param_name = param_name.strip()
                                param_value = param_value.strip()

                                # æ£€æŸ¥@å‰ç¼€ï¼ˆå›ºå®šå€¼ï¼Œä¸æµ‹è¯•ï¼‰
                                if param_value.startswith('@'):
                                    # å»æ‰@å‰ç¼€ï¼Œä¿æŒåŸå€¼
                                    param_value = param_value[1:]
                                    print(f"[INFO]   - {param_name} = {param_value} (å›ºå®šå€¼ï¼Œä¸æµ‹è¯•)")
                                else:
                                    print(f"[INFO]   - {param_name} = {param_value} (æµ‹è¯•)")
                            else:
                                # æ²¡æœ‰=å·ï¼Œé»˜è®¤æµ‹è¯•å‚æ•°ï¼ˆå€¼ä¸ºFuzzï¼‰
                                param_name = param_def
                                param_value = 'Fuzz'
                                print(f"[INFO]   - {param_name} = {param_value} (æµ‹è¯•)")

                            # æ ¹æ®è¯·æ±‚æ–¹æ³•æ”¾åˆ°ä¸åŒçš„å­—å…¸
                            if args.method == 'GET':
                                manual_params_dict[param_name] = param_value
                            else:  # POST
                                manual_data[param_name] = param_value

                        # æ‰‹åŠ¨å‚æ•°ä¼˜å…ˆï¼Œè¦†ç›–URLä¸­çš„å‚æ•°
                        if args.method == 'GET':
                            params = manual_params_dict
                        else:
                            params = {}  # POSTè¯·æ±‚ï¼Œparamsåº”è¯¥ä¸ºç©º
                            data = manual_data
                    else:
                        # æ²¡æœ‰æ‰‹åŠ¨æŒ‡å®šï¼Œä½¿ç”¨URLä¸­çš„å‚æ•°
                        data = {}

                    # åˆ¤æ–­è¯·æ±‚æ–¹æ³•
                    method = args.method.upper()

                    # æ–°å¢ï¼šè§£æHTTPå¤´æ³¨å…¥åˆ—è¡¨
                    injectable_headers = {}
                    if args.headers and args.headers.strip():
                        header_list = [h.strip() for h in args.headers.split(',')]
                        print(f"[INFO] æŒ‡å®šHTTPå¤´æ³¨å…¥: {header_list}")

                        # ä»Requesterè·å–é»˜è®¤å¤´å€¼
                        from Fuzz.BaseFuzz.requester import Requester
                        temp_requester = Requester(timeout=args.timeout)

                        for header_name in header_list:
                            default_value = temp_requester.headers.get(header_name, '')
                            injectable_headers[header_name] = default_value
                            print(f"[INFO]   - æ·»åŠ HTTPå¤´æ³¨å…¥ç‚¹: {header_name}={default_value[:30]}")
                    else:
                        print(f"[INFO] æœªæŒ‡å®šHTTPå¤´æ³¨å…¥")

                    # æ„å»ºFuzzTarget
                    if method == 'GET':
                        target = FuzzTarget(
                            url=args.url,
                            method=method,
                            params=params,
                            data={},
                            injectable_headers=injectable_headers,  # æ–°å¢HTTPå¤´æ³¨å…¥
                            depth=0  # ç›´æ¥URLçš„æ·±åº¦è®¾ä¸º0
                        )
                    else:  # POST
                        target = FuzzTarget(
                            url=args.url,
                            method=method,
                            params={},
                            data=data if args.params else {},
                            injectable_headers=injectable_headers,  # æ–°å¢HTTPå¤´æ³¨å…¥
                            depth=0  # ç›´æ¥URLçš„æ·±åº¦è®¾ä¸º0
                        )

                    targets = [target]
                    print(f"[SUCCESS] å·²åˆ›å»ºæµ‹è¯•ç›®æ ‡: {args.url}")
                    print(f"[INFO] è¯·æ±‚æ–¹æ³•: {method}")

                    #æ˜¾ç¤ºå‚æ•°ä¿¡æ¯ï¼ˆGETå’ŒPOSTåˆ†å¼€å¤„ç†ï¼‰
                    if method == 'GET':
                        print(f"[INFO] GETå‚æ•°æ•°é‡: {len(params)}")
                        if params:
                            print(f"[INFO] GETå‚æ•°åˆ—è¡¨: {', '.join(params.keys())}")
                    else:  # POST
                        print(f"[INFO] POSTå‚æ•°æ•°é‡: {len(data)}")
                        if data:
                            print(f"[INFO] POSTå‚æ•°åˆ—è¡¨: {', '.join(data.keys())}")

                    #æ˜¾ç¤ºHTTPå¤´æ³¨å…¥ä¿¡æ¯
                    if injectable_headers:
                        print(f"[INFO] HTTPå¤´æ³¨å…¥: {', '.join(injectable_headers.keys())}")

                except Exception as e:
                    print(f"[ERROR] URLè§£æå¤±è´¥: {e}")
                    return False

        else:
            print("[ERROR] å¿…é¡»æŒ‡å®š --file å‚æ•°ï¼ˆåŠ è½½çˆ¬è™«ç¼“å­˜ï¼‰")
            print("[HINT] æ­£ç¡®ç”¨æ³•:")
            print("  1. å…ˆè¿è¡Œçˆ¬è™«: python fuzzmain.py --crawl --url <url> --cookie <cookie>")
            print("  2. å†è¿è¡ŒBaseFuzz: python fuzzmain.py --fuzz --engine base --mode common --file <cache_file>")
            return False

        # éªŒè¯ç›®æ ‡åˆ—è¡¨
        if not targets:
            print("[ERROR] æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç›®æ ‡")
            return False

        # æ˜¾ç¤ºç›®æ ‡ç»Ÿè®¡
        print(f"\n[INFO] ç›®æ ‡ç»Ÿè®¡:")
        print(f"  - æ€»ç›®æ ‡æ•°: {len(targets)}")
        get_count = sum(1 for t in targets if t.method == 'GET')
        post_count = sum(1 for t in targets if t.method == 'POST')
        print(f"  - GETä»»åŠ¡: {get_count}")
        print(f"  - POSTä»»åŠ¡: {post_count}")

        # ========== æ­¥éª¤3ï¼šåˆå§‹åŒ–BaseFuzz Engine ==========
        print("\n[æ­¥éª¤2] åˆå§‹åŒ–BaseFuzzå¼•æ“")
        print("-" * 70)

        # ç¡®å®šä½¿ç”¨çš„å¼•æ“
        engine_names = ['sqli', 'xss']  # ç›®å‰æ”¯æŒSQLiå’ŒXSS

        # è§£æå‚æ•°è¿‡æ»¤åˆ—è¡¨
        param_filter = None
        if args.params:
            # ä¿®å¤ï¼šæå–å‚æ•°åï¼ˆå¿½ç•¥å€¼éƒ¨åˆ†ï¼‰
            # æ”¯æŒè¯­æ³•ï¼š
            # - id=1 â†’ æµ‹è¯•id
            # - id=@1 â†’ ä¸æµ‹è¯•idï¼ˆ@å‰ç¼€è¡¨ç¤ºå›ºå®šå€¼ï¼‰
            # - id â†’ æµ‹è¯•id
            param_filter = []
            for p in args.params.split(','):
                p = p.strip()
                if '=' in p:
                    # æœ‰=å·ï¼Œè§£æåç§°å’Œå€¼
                    param_name, param_value = p.split('=', 1)
                    param_name = param_name.strip()
                    param_value = param_value.strip()

                    # æ£€æŸ¥@å‰ç¼€ï¼ˆå›ºå®šå€¼ï¼Œä¸æµ‹è¯•ï¼‰
                    if param_value.startswith('@'):
                        # è·³è¿‡å›ºå®šå€¼å‚æ•°
                        continue
                    else:
                        # æ·»åŠ åˆ°æµ‹è¯•åˆ—è¡¨
                        param_filter.append(param_name)
                else:
                    # æ²¡æœ‰=å·ï¼Œå°±æ˜¯å‚æ•°åï¼ˆæµ‹è¯•ï¼‰
                    param_filter.append(p)

            # è‰¹ï¼æ–°å¢ï¼šæŠŠHTTPå¤´ä¹ŸåŠ å…¥åˆ°å‚æ•°è¿‡æ»¤åˆ—è¡¨ä¸­
            if args.headers:
                header_list = [h.strip() for h in args.headers.split(',')]
                param_filter.extend(header_list)

            if param_filter:
                print(f"[INFO] å‚æ•°è¿‡æ»¤: åªæµ‹è¯•æŒ‡å®šçš„å‚æ•° -> {param_filter}")
            else:
                print(f"[INFO] å‚æ•°è¿‡æ»¤: æ‰€æœ‰å‚æ•°éƒ½æ˜¯å›ºå®šå€¼ï¼Œæ— å¯æµ‹è¯•å‚æ•°")
                print(f"[WARNING] è­¦å‘Šï¼šæ²¡æœ‰ä»»ä½•å‚æ•°ä¼šè¢«æµ‹è¯•ï¼")

        print(f"[INFO] å¼•æ“é…ç½®:")
        print(f"  - æ£€æµ‹å¼•æ“: {', '.join(engine_names)}")
        print(f"  - è½½è·æ¨¡å¼: {args.mode}")
        print(f"  - å¹¶å‘çº¿ç¨‹: {args.threads}")
        print(f"  - è¶…æ—¶æ—¶é—´: {args.timeout}ç§’")

        try:
            # åˆ›å»ºEngineå®ä¾‹
            engine = Engine(
                engine_names=engine_names,
                mode=args.mode,
                timeout=args.timeout,
                cookie=args.cookie,
                max_workers=args.threads,
                concurrent_params=10,  # å‚æ•°çº§å¹¶å‘æ•°
                param_filter=param_filter  # å·²åŒ…å«HTTPå¤´çš„å‚æ•°è¿‡æ»¤åˆ—è¡¨
            )

            print("[SUCCESS] BaseFuzzå¼•æ“åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            print(f"[ERROR] å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()
            return False

        # ========== æ­¥éª¤4ï¼šæ‰§è¡Œæ¨¡ç³Šæµ‹è¯• ==========
        print("\n[æ­¥éª¤3] æ‰§è¡Œæ¨¡ç³Šæµ‹è¯•")
        print("-" * 70)
        print("[INFO] å¼€å§‹æ‰«æ...")
        print("[æç¤º] æŒ‰Ctrl+Cå¯éšæ—¶ä¸­æ–­æ‰«æ")

        import time
        start_time = time.time()

        try:
            # æ‰§è¡Œæ‰«æ
            results = engine.run(targets)

            elapsed_time = time.time() - start_time

            print(f"\n[SUCCESS] æ‰«æå®Œæˆï¼")
            print(f"[INFO] æ‰«æè€—æ—¶: {elapsed_time:.2f}ç§’")
            print(f"[INFO] å‘ç°æ¼æ´: {len(results)} ä¸ª")

        except KeyboardInterrupt:
            print("\n\n[WARNING] ç”¨æˆ·ä¸­æ–­æ‰«æ")
            print("[INFO] æ­£åœ¨ä¿å­˜å·²å‘ç°çš„ç»“æœ...")

            # Engineä¼šè‡ªåŠ¨ä¿å­˜å·²å‘ç°çš„ç»“æœ
            elapsed_time = time.time() - start_time

            return True

        except Exception as e:
            print(f"\n[ERROR] æ‰«ææ‰§è¡Œå¤±è´¥: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()
            return False

        # ========== æ­¥éª¤5ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Š ==========
        if results:
            print("\n[æ­¥éª¤4] ç”Ÿæˆåˆ†ææŠ¥å‘Š")
            print("-" * 70)

            try:
                # åˆ†æç»“æœï¼ˆåŸºäºè¿‡æ»¤åçš„vulnerabilities.jsonï¼‰
                analyzer = Analyzer()
                analyzed_results, stats = analyzer.analyze(
                    engine.results_file  # Engineä¿å­˜çš„è¿‡æ»¤åç»“æœï¼ˆæ— Error-Basedï¼‰
                )

                print(f"[INFO] åˆ†æå®Œæˆ:")
                print(f"  - æœ‰æ•ˆæ¼æ´: {len(analyzed_results)} æ¡ï¼ˆå·²è¿‡æ»¤Error-Basedï¼‰")
                print(f"  - é«˜å±æ¼æ´: {stats.get('high_risk_count', 0)} ä¸ª")
                print(f"  - ä¸­å±æ¼æ´: {stats.get('medium_risk_count', 0)} ä¸ª")
                print(f"  - ä½å±æ¼æ´: {stats.get('low_risk_count', 0)} ä¸ª")
                print(f"  - é£é™©æŒ‡æ•°: {stats.get('risk_index', 0):.2f}")

                # ç”ŸæˆæŠ¥å‘Š
                print(f"\n[INFO] ç”ŸæˆæŠ¥å‘Š...")

                reporter = Reporter(output_dir=engine.output_dir)

                # æ‰«æä¿¡æ¯
                scan_info = {
                    'start_time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time)),
                    'end_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'total_targets': len(targets),
                    'total_params_tested': sum(len(t.params) + len(t.data) for t in targets),
                    'engines_used': engine_names,
                    'mode': args.mode,
                    'engine_type': args.engine,
                }

                # è‰¹ï¼è¯»å–æ‰€æœ‰æ¼æ´ï¼ˆåŒ…æ‹¬Error-Basedï¼‰
                all_results_file = engine.output_dir / "vulnerabilities_all.json"
                if all_results_file.exists():
                    import json
                    with open(all_results_file, 'r', encoding='utf-8') as f:
                        all_vulns = json.load(f)
                    print(f"  - æ€»æ£€æµ‹æ•°: {len(all_vulns)} æ¡ï¼ˆå«Error-Basedï¼‰")
                else:
                    all_vulns = results  # é™çº§ï¼šä½¿ç”¨åŸå§‹results

                # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šï¼ˆåŸºäºè¿‡æ»¤åçš„ç»“æœï¼‰
                reporter.generate_summary(analyzed_results, stats, scan_info)

                #ä¿®æ”¹Reporterï¼šç”Ÿæˆè¯¦ç»†æŠ¥å‘Šæ—¶ä½¿ç”¨æ‰€æœ‰æ¼æ´
                reporter.detail_file = engine.output_dir / "vulnerabilities_detail.json"
                reporter._generate_json_report(all_vulns, stats, scan_info)

                # æ‰“å°ç»ˆç«¯æ±‡æ€»
                reporter.print_console_summary(stats, scan_info)

                # æ‰“å°æ¼æ´è¡¨æ ¼ï¼ˆTop 20ï¼‰
                reporter.print_vulnerability_table(analyzed_results, top_n=20)

                # è·å–æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
                report_files = reporter.get_report_files()

                print(f"\n[SUCCESS] æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
                print(f"[INFO] æŠ¥å‘Šç›®å½•: {report_files['directory']}")
                print(f"[INFO] æ±‡æ€»æŠ¥å‘Š: {report_files['summary']}")
                print(f"[INFO] è¯¦ç»†æŠ¥å‘Š: {report_files['detail']}")

            except Exception as e:
                print(f"[ERROR] æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
                if args.verbose:
                    import traceback
                    traceback.print_exc()
        else:
            print("\n[INFO] æœªå‘ç°æ¼æ´")

        print("\n" + "=" * 70)
        print("[SUCCESS] BaseFuzzæ‰«æå®Œæˆï¼")
        print("=" * 70)

        return True

    except Exception as e:
        print(f"\n[ERROR] BaseFuzzæ‰§è¡Œå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    args = parse_arguments()

    # éªŒè¯å‚æ•°
    errors = validate_arguments(args)
    if errors:
        for error in errors:
            print(f"[ERROR] {error}")
        return 1

    # æ‰“å°æ¨ªå¹…
    print_banner()

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    # æ‰§è¡Œå„ä¸ªé˜¶æ®µ
    success = True

    if args.preprocess:
        success = execute_preprocess(args) and success

    if args.train:
        success = execute_train(args) and success

    if args.generate:
        success = execute_generate(args) and success

    if args.cluster:
        success = execute_cluster(args) and success

    # ========== æ–°å¢ï¼šç¬¬å››é˜¶æ®µè°ƒåº¦ ==========
    if args.crawl or args.scan:
        success = execute_scan_init(args) and success

    # ========== æ–°å¢ï¼šBaseFuzzè°ƒåº¦ ==========
    if args.fuzz:
        success = execute_basefuzz(args) and success

    # ç»Ÿè®¡æ€»è€—æ—¶
    elapsed_time = time.time() - start_time

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    if success:
        print("[SUCCESS] æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼")
        print(f"[TIME] æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
    else:
        print("[FAILED] éƒ¨åˆ†ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    print("=" * 60)

    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
