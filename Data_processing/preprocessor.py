#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†æ¨¡å—
================================

æ¨¡å—åŠŸèƒ½ï¼šå°†åŸå§‹Webæ”»å‡»è½½è·è½¬æ¢ä¸ºç¥ç»ç½‘ç»œå¯å¤„ç†çš„æ•°å€¼å¼ é‡
æŠ€æœ¯è·¯å¾„ï¼šå­—ç¬¦çº§åˆ†è¯ + åºåˆ—æ ‡å‡†åŒ– + è¯è¡¨æ„å»º + è´¨é‡åˆ†æ

"""

import json
import os
import sys
import io
import argparse
from typing import Dict, List, Tuple, Set, Optional
from collections import Counter, defaultdict
import pickle
import torch
import numpy as np
from pathlib import Path

# è§£å†³Windowsä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
if sys.platform.startswith('win'):
    # è®¾ç½®UTF-8ç¼–ç è¾“å‡º
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


class AdvancedCharTokenizer:
    """é«˜çº§å­—ç¬¦çº§åˆ†è¯å™¨

    ä¸“é—¨å¤„ç†Webæ”»å‡»è½½è·çš„åˆ†è¯å™¨ï¼Œæ”¯æŒæ‰©å±•å­—ç¬¦é›†ä»¥æå‡è¦†ç›–ç‡
    è§£å†³ä¼ ç»ŸWord-levelåˆ†è¯åœ¨ä»£ç ç±»æ•°æ®ä¸Šçš„OOVé—®é¢˜
    """

    def __init__(self, vocab_size: int = 256, extended_chars: bool = True):
        """åˆå§‹åŒ–åˆ†è¯å™¨

        Args:
            vocab_size: è¯è¡¨å¤§å°ï¼Œé»˜è®¤256ä¸ªå­—ç¬¦
            extended_chars: æ˜¯å¦å¯ç”¨æ‰©å±•å­—ç¬¦é›†ï¼ŒåŒ…å«å¸¸ç”¨Unicodeå­—ç¬¦
        """
        self.vocab_size = vocab_size
        self.extended_chars = extended_chars
        self.char_to_idx = {}
        self.idx_to_char = {}
        self.special_tokens = {
            '<SOS>': 0,    # åºåˆ—å¼€å§‹æ ‡è®°
            '<EOS>': 1,    # åºåˆ—ç»“æŸæ ‡è®°
            '<PAD>': 2,    # å¡«å……æ ‡è®°
            '<UNK>': 3     # æœªçŸ¥å­—ç¬¦æ ‡è®°
        }

        # å­—ç¬¦ç»Ÿè®¡
        self.char_frequency = Counter()
        self.uncovered_chars = set()

        # åˆå§‹åŒ–è¯è¡¨
        self._build_vocab()

    def _build_vocab(self) -> None:
        """æ„å»ºæ‰©å±•å­—ç¬¦é›†è¯è¡¨

        åŒ…å«ASCIIå¯æ‰“å°å­—ç¬¦ + å¸¸ç”¨Unicodeå­—ç¬¦ + Webæ”»å‡»å¸¸è§ç‰¹æ®Šå­—ç¬¦
        """
        # åŸºç¡€ASCIIå¯æ‰“å°å­—ç¬¦ (32-126)
        base_chars = [chr(i) for i in range(32, 127)]

        # æ§åˆ¶å­—ç¬¦
        control_chars = ['\t', '\n', '\r', '\f', '\v']

        # æ‰©å±•å­—ç¬¦é›†ï¼Œé’ˆå¯¹Webæ”»å‡»è½½è·ä¸­çš„å¸¸è§å­—ç¬¦
        extended_chars = []
        if self.extended_chars:
            extended_chars = [
                # æ‰©å±•ASCIIå­—ç¬¦ (128-255) - æ¬§æ´²è¯­è¨€å­—ç¬¦
                'Â¡', 'Â¢', 'Â£', 'Â¤', 'Â¥', 'Â¦', 'Â§', 'Â¨', 'Â©', 'Âª', 'Â«', 'Â¬', 'Â®', 'Â¯',
                'Â°', 'Â±', 'Â²', 'Â³', 'Â´', 'Âµ', 'Â¶', 'Â·', 'Â¸', 'Â¹', 'Âº', 'Â»', 'Â¼', 'Â½',
                'Â¾', 'Â¿', 'Ã€', 'Ã', 'Ã‚', 'Ãƒ', 'Ã„', 'Ã…', 'Ã†', 'Ã‡', 'Ãˆ', 'Ã‰', 'ÃŠ', 'Ã‹',
                'ÃŒ', 'Ã', 'Ã', 'Ã', 'Ã', 'Ã‘', 'Ã’', 'Ã“', 'Ã”', 'Ã•', 'Ã–', 'Ã—', 'Ã˜', 'Ã™',
                'Ãš', 'Ã›', 'Ãœ', 'Ã', 'Ã', 'ÃŸ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', 'Ã¦', 'Ã§',
                'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ',
                'Ã¶', 'Ã·', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ã½', 'Ã¾', 'Ã¿',

                # Unicodeç‰¹æ®Šå­—ç¬¦ (Webæ”»å‡»ä¸­å¸¸è§)
                '\u00a0',  # ä¸æ¢è¡Œç©ºæ ¼
                '\u200b',  # é›¶å®½ç©ºæ ¼
                '\u200c',  # é›¶å®½éè¿å­—ç¬¦
                '\u200d',  # é›¶å®½è¿å­—ç¬¦
                '\ufeff',  # é›¶å®½éæ–­ç©ºæ ¼
                '\u2060',  # å•è¯è¿æ¥ç¬¦

                # æ•°å­¦ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
                'â€¦', 'â€“', 'â€”', ''', ''', '"', '"', 'â€¢', 'â€°', 'â€¹', 'â€º',

                # å…¶ä»–Webæ”»å‡»å¸¸è§å­—ç¬¦
                'Å ', 'Å¡', 'Å½', 'Å¾', 'Å’', 'Å“', 'Å¸', 'Æ’', 'Ë†', 'Ëœ',
                'â‚¬', 'â„¢', 'âˆ', 'â‰ ', 'â‰¤', 'â‰¥', 'âˆ‚', 'âˆ†', 'âˆ‡', 'âˆ',
                'âˆ‘', 'âˆ«', 'Ï€', 'Î©', 'Î±', 'Î²', 'Î³', 'Î´', 'Îµ', 'Î¶',
                'Î·', 'Î¸', 'Î»', 'Î¼', 'Î¾', 'Ï', 'Ïƒ', 'Ï„', 'Ï†', 'Ï‡', 'Ïˆ', 'Ï‰',

                # ä¸­æ–‡å­—ç¬¦ (ä¸­æ–‡æ”»å‡»è½½è·)
                'çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–', 'è¿™',
                'ä¸ª', 'ä¸€', 'ä¸', 'ä¼š', 'å°±', 'è¯´', 'è¦', 'å¯', 'ä»¥', 'æ¥',

                # æ—¥æ–‡å­—ç¬¦ (æ—¥æœ¬æ”»å‡»è½½è·)
                'ã‚', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‹', 'ã', 'ã', 'ã‘', 'ã“',
                'ã‚¢', 'ã‚¤', 'ã‚¦', 'ã‚¨', 'ã‚ª', 'ã‚«', 'ã‚­', 'ã‚¯', 'ã‚±', 'ã‚³',

                # éŸ©æ–‡å­—ç¬¦ (éŸ©å›½æ”»å‡»è½½è·)
                'ê°€', 'ë‚˜', 'ë‹¤', 'ë¼', 'ë§ˆ', 'ë°”', 'ì‚¬', 'ì•„', 'ì', 'ì°¨',

                # é˜¿æ‹‰ä¼¯å­—ç¬¦
                'Ø§', 'Ø¨', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±',

                # è¥¿é‡Œå°”å­—ç¬¦ (ä¿„è¯­)
                'Ğ', 'Ğ‘', 'Ğ’', 'Ğ“', 'Ğ”', 'Ğ•', 'Ğ–', 'Ğ—', 'Ğ˜', 'Ğš',
                'Ğ°', 'Ğ±', 'Ğ²', 'Ğ³', 'Ğ´', 'Ğµ', 'Ğ¶', 'Ğ·', 'Ğ¸', 'Ğº',
            ]

        # åˆå¹¶æ‰€æœ‰å­—ç¬¦
        all_chars = base_chars + control_chars + extended_chars

        # æ·»åŠ ç‰¹æ®ŠTokenåˆ°è¯è¡¨å¼€å¤´
        vocab_list = list(self.special_tokens.keys()) + all_chars

        # é™åˆ¶è¯è¡¨å¤§å°
        if len(vocab_list) > self.vocab_size:
            vocab_list = vocab_list[:self.vocab_size]
            print(f"âš ï¸ è¯è¡¨è¿‡å¤§ï¼Œæˆªæ–­åˆ° {self.vocab_size} ä¸ªå­—ç¬¦")

        # æ„å»ºåŒå‘æ˜ å°„
        for idx, char in enumerate(vocab_list):
            self.char_to_idx[char] = idx
            self.idx_to_char[idx] = char

        print(f"âœ… è¯è¡¨æ„å»ºå®Œæˆï¼Œæ€»å¤§å°ï¼š{len(self.char_to_idx)}")
        print(f"ğŸ“ ç‰¹æ®ŠTokenæ•°é‡ï¼š{len(self.special_tokens)}")
        print(f"ğŸ”¤ ASCIIå¯æ‰“å°å­—ç¬¦ï¼š{len(base_chars)}")
        print(f"âŒ¨ï¸  æ§åˆ¶å­—ç¬¦ï¼š{len(control_chars)}")
        print(f"ğŸŒ æ‰©å±•å­—ç¬¦æ•°é‡ï¼š{len(extended_chars)}")
        print(f"ğŸ¯ å®é™…ä½¿ç”¨å­—ç¬¦ï¼š{len(all_chars)}")

    def analyze_text(self, texts: List[str]) -> Dict[str, any]:
        """åˆ†ææ–‡æœ¬ä¸­çš„å­—ç¬¦åˆ†å¸ƒ

        Args:
            texts: å¾…åˆ†æçš„æ–‡æœ¬åˆ—è¡¨

        Returns:
            å­—ç¬¦åˆ†å¸ƒåˆ†æç»“æœ
        """
        print(f"\nğŸ” å¼€å§‹å­—ç¬¦åˆ†å¸ƒåˆ†æ...")

        # ç»Ÿè®¡å­—ç¬¦é¢‘ç‡
        self.char_frequency.clear()
        self.uncovered_chars.clear()

        total_chars = 0
        covered_chars = 0

        for text in texts:
            for char in text:
                total_chars += 1
                self.char_frequency[char] += 1
                if char in self.char_to_idx:
                    covered_chars += 1
                else:
                    self.uncovered_chars.add(char)

        coverage = (covered_chars / total_chars * 100) if total_chars > 0 else 0

        print(f"ğŸ“Š å­—ç¬¦åˆ†å¸ƒåˆ†æç»“æœï¼š")
        print(f"   æ€»å­—ç¬¦æ•°ï¼š{total_chars}")
        print(f"   è¯è¡¨è¦†ç›–å­—ç¬¦æ•°ï¼š{covered_chars}")
        print(f"   æœªè¦†ç›–å­—ç¬¦æ•°ï¼š{len(self.uncovered_chars)}")
        print(f"   è¯è¡¨è¦†ç›–ç‡ï¼š{coverage:.3f}%")

        # æ˜¾ç¤ºæœ€å¸¸è§çš„å­—ç¬¦
        most_common = self.char_frequency.most_common(20)
        print(f"\nğŸ“ˆ æœ€å¸¸è§çš„20ä¸ªå­—ç¬¦ï¼š")
        for char, freq in most_common:
            char_repr = repr(char)
            print(f"   {char_repr:>6}: {freq:>6} æ¬¡")

        # æ˜¾ç¤ºæœªè¦†ç›–çš„å­—ç¬¦
        if self.uncovered_chars:
            print(f"\nâš ï¸ æœªè¦†ç›–çš„å­—ç¬¦ï¼ˆ{len(self.uncovered_chars)}ä¸ªï¼‰ï¼š")
            for char in sorted(self.uncovered_chars):
                print(f"   {repr(char)} (U+{ord(char):04X})")

        return {
            'total_chars': total_chars,
            'covered_chars': covered_chars,
            'uncovered_chars': len(self.uncovered_chars),
            'coverage_rate': coverage,
            'most_common': most_common,
            'uncovered_list': list(self.uncovered_chars)
        }

    def encode(self, text: str, max_length: int = 150) -> List[int]:
        """å°†æ–‡æœ¬ç¼–ç ä¸ºç´¢å¼•åºåˆ—

        Args:
            text: å¾…ç¼–ç çš„æ–‡æœ¬
            max_length: æœ€å¤§åºåˆ—é•¿åº¦

        Returns:
            ç¼–ç åçš„ç´¢å¼•åºåˆ—
        """
        # æ·»åŠ èµ·å§‹å’Œç»“æŸæ ‡è®°
        encoded = [self.special_tokens['<SOS>']]

        # ç¼–ç æ–‡æœ¬å†…å®¹
        for char in text[:max_length-2]:  # ä¸ºSOSå’ŒEOSé¢„ç•™ä½ç½®
            if char in self.char_to_idx:
                encoded.append(self.char_to_idx[char])
            else:
                encoded.append(self.special_tokens['<UNK>'])

        # æ·»åŠ ç»“æŸæ ‡è®°
        encoded.append(self.special_tokens['<EOS>'])

        # åç¼€å¡«å……åˆ°max_length
        while len(encoded) < max_length:
            encoded.append(self.special_tokens['<PAD>'])

        return encoded[:max_length]

    def decode(self, indices: List[int]) -> str:
        """å°†ç´¢å¼•åºåˆ—è§£ç ä¸ºæ–‡æœ¬

        Args:
            indices: ç´¢å¼•åºåˆ—

        Returns:
            è§£ç åçš„æ–‡æœ¬
        """
        chars = []
        for idx in indices:
            if idx in self.idx_to_char:
                char = self.idx_to_char[idx]
                # è·³è¿‡ç‰¹æ®ŠTokenï¼ˆé™¤äº†UNKï¼‰
                if char not in ['<SOS>', '<EOS>', '<PAD>']:
                    if char == '<UNK>':
                        chars.append('ï¿½')  # æœªçŸ¥å­—ç¬¦ç”¨æ›¿æ¢ç¬¦å·è¡¨ç¤º
                    else:
                        chars.append(char)
            else:
                chars.append('ï¿½')  # æ— æ•ˆç´¢å¼•ä¹Ÿç”¨æ›¿æ¢ç¬¦å·è¡¨ç¤º

        return ''.join(chars)

    def get_vocab_stats(self) -> Dict[str, any]:
        """è·å–è¯è¡¨ç»Ÿè®¡ä¿¡æ¯

        Returns:
            è¯è¡¨ç»Ÿè®¡ä¿¡æ¯
        """
        return {
            'vocab_size': len(self.char_to_idx),
            'special_tokens': self.special_tokens,
            'extended_mode': self.extended_chars,
            'char_frequency': dict(self.char_frequency.most_common(50)),
            'uncovered_count': len(self.uncovered_chars)
        }


class PayloadDataset:
    """Webæ”»å‡»è½½è·æ•°æ®é›†å¤„ç†å™¨

    æ”¯æŒ6ç§æ”»å‡»ç±»å‹çš„åŠ è½½å’Œé¢„å¤„ç†ï¼ŒåŒ…å«æ•°æ®è´¨é‡åˆ†æ
    """

    # æ”¯æŒçš„æ”»å‡»ç±»å‹æ˜ å°„
    ATTACK_TYPES = {
        'SQLi': 0,
        'XSS': 1,
        'CMDi': 2,
        'Overflow': 3,
        'XXE': 4,
        'SSI': 5,
        'XML': 4,  # XMLç±»å‹æ˜ å°„åˆ°XXE
    }

    # æ–‡ä»¶ååˆ°æ”»å‡»ç±»å‹çš„æ˜ å°„
    FILE_MAPPING = {
        'sqli.jsonl': 'SQLi',
        'xss.jsonl': 'XSS',
        'cmdi.jsonl': 'CMDi',
        'overflow.jsonl': 'Overflow',
        'xml.jsonl': 'XXE',      # xml.jsonlåŒ…å«XXEæ”»å‡»
        'ssi.jsonl': 'SSI'
    }

    def __init__(self, data_dir: str, max_length: int = 150, vocab_size: int = 256):
        """åˆå§‹åŒ–æ•°æ®é›†å¤„ç†å™¨

        Args:
            data_dir: è®­ç»ƒæ•°æ®ç›®å½•è·¯å¾„
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            vocab_size: è¯è¡¨å¤§å°
        """
        self.data_dir = Path(data_dir)
        self.max_length = max_length
        self.tokenizer = AdvancedCharTokenizer(vocab_size=vocab_size, extended_chars=True)
        self.payloads = []      # å­˜å‚¨è½½è·æ–‡æœ¬
        self.labels = []        # å­˜å‚¨è½½è·æ ‡ç­¾
        self.attack_counts = {}  # å„ç±»å‹æ”»å‡»æ•°é‡ç»Ÿè®¡

        # æ•°æ®è´¨é‡ç»Ÿè®¡
        self.quality_stats = {
            'empty_payloads': 0,
            'duplicate_payloads': 0,
            'avg_payload_length': 0,
            'max_payload_length': 0,
            'min_payload_length': float('inf'),
            'total_chars': 0
        }

        print(f"ğŸ¯ åˆå§‹åŒ–æ•°æ®é›†å¤„ç†å™¨ (å¢å¼ºç‰ˆ)")
        print(f"ğŸ“ æ•°æ®ç›®å½•ï¼š{self.data_dir}")
        print(f"ğŸ“ æœ€å¤§åºåˆ—é•¿åº¦ï¼š{self.max_length}")
        print(f"ğŸ“š è¯è¡¨å¤§å°ï¼š{vocab_size}")

    def _quality_check(self, payload: str) -> bool:
        """æ•°æ®è´¨é‡æ£€æŸ¥

        Args:
            payload: å¾…æ£€æŸ¥çš„è½½è·

        Returns:
            æ˜¯å¦é€šè¿‡è´¨é‡æ£€æŸ¥
        """
        # æ£€æŸ¥ç©ºè½½è·
        if not payload or not payload.strip():
            self.quality_stats['empty_payloads'] += 1
            return False

        # æ£€æŸ¥è½½è·é•¿åº¦
        payload_len = len(payload)
        self.quality_stats['total_chars'] += payload_len
        self.quality_stats['max_payload_length'] = max(self.quality_stats['max_payload_length'], payload_len)
        self.quality_stats['min_payload_length'] = min(self.quality_stats['min_payload_length'], payload_len)

        return True

    def load_data(self) -> Tuple[List[str], List[int]]:
        """åŠ è½½æ‰€æœ‰è®­ç»ƒæ•°æ®

        Returns:
            (payloads, labels): è½½è·åˆ—è¡¨å’Œå¯¹åº”æ ‡ç­¾åˆ—è¡¨
        """
        print(f"\nğŸš€ å¼€å§‹åŠ è½½è®­ç»ƒæ•°æ® (å¢å¼ºç‰ˆ)...")

        total_samples = 0
        seen_payloads = set()  # ç”¨äºå»é‡

        # éå†æ‰€æœ‰è®­ç»ƒæ–‡ä»¶
        for filename, attack_type in self.FILE_MAPPING.items():
            file_path = self.data_dir / filename

            if not file_path.exists():
                print(f"âš ï¸ è­¦å‘Šï¼šæ–‡ä»¶ä¸å­˜åœ¨ {file_path}")
                continue

            print(f"ğŸ“– æ­£åœ¨è¯»å–ï¼š{filename} -> {attack_type}")

            samples_in_file = 0
            duplicates_in_file = 0
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        if not line:
                            continue

                        try:
                            data = json.loads(line)
                            payload = data.get('payload', '').strip()
                            data_type = data.get('type', '')

                            # æ•°æ®è´¨é‡æ£€æŸ¥
                            if not self._quality_check(payload):
                                continue

                            # éªŒè¯æ”»å‡»ç±»å‹åŒ¹é…
                            if data_type != attack_type and data_type not in self.ATTACK_TYPES:
                                print(f"âš ï¸ {filename}:{line_num} æœªçŸ¥æ”»å‡»ç±»å‹ï¼š{data_type}")
                                continue

                            # å»é‡æ£€æŸ¥
                            if payload in seen_payloads:
                                duplicates_in_file += 1
                                self.quality_stats['duplicate_payloads'] += 1
                                continue

                            seen_payloads.add(payload)

                            # æ·»åŠ åˆ°æ•°æ®é›†
                            self.payloads.append(payload)
                            self.labels.append(self.ATTACK_TYPES[attack_type])
                            samples_in_file += 1

                        except json.JSONDecodeError as e:
                            print(f"âš ï¸ {filename}:{line_num} JSONè§£æé”™è¯¯ï¼š{e}")
                            continue

            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}ï¼š{e}")
                continue

            # è®°å½•è¯¥ç±»å‹æ”»å‡»æ•°é‡
            self.attack_counts[attack_type] = samples_in_file
            total_samples += samples_in_file

            print(f"âœ… {filename} åŠ è½½å®Œæˆï¼š{samples_in_file} æ¡è½½è· (å»é‡å)")
            if duplicates_in_file > 0:
                print(f"   è·³è¿‡é‡å¤è½½è·ï¼š{duplicates_in_file} æ¡")

        # è®¡ç®—å¹³å‡é•¿åº¦
        if self.payloads:
            total_length = sum(len(p) for p in self.payloads)
            self.quality_stats['avg_payload_length'] = total_length / len(self.payloads)
        else:
            self.quality_stats['avg_payload_length'] = 0
            self.quality_stats['min_payload_length'] = 0

        print(f"\nğŸŠ æ•°æ®åŠ è½½å®Œæˆï¼")
        print(f"ğŸ“Š æœ‰æ•ˆæ ·æœ¬æ•°ï¼š{total_samples}")
        print(f"ğŸ“ˆ å„ç±»å‹åˆ†å¸ƒï¼š")
        for attack_type, count in self.attack_counts.items():
            percentage = (count / total_samples) * 100 if total_samples > 0 else 0
            print(f"   {attack_type:>8}: {count:>6} æ¡ ({percentage:>5.1f}%)")

        # æ˜¾ç¤ºè´¨é‡ç»Ÿè®¡
        print(f"\nğŸ“‹ æ•°æ®è´¨é‡ç»Ÿè®¡ï¼š")
        print(f"   ç©ºè½½è·ï¼š{self.quality_stats['empty_payloads']} æ¡")
        print(f"   é‡å¤è½½è·ï¼š{self.quality_stats['duplicate_payloads']} æ¡")
        print(f"   å¹³å‡é•¿åº¦ï¼š{self.quality_stats['avg_payload_length']:.1f} å­—ç¬¦")
        print(f"   æœ€å¤§é•¿åº¦ï¼š{self.quality_stats['max_payload_length']} å­—ç¬¦")
        print(f"   æœ€å°é•¿åº¦ï¼š{self.quality_stats['min_payload_length']} å­—ç¬¦")

        return self.payloads, self.labels

    def preprocess(self) -> torch.Tensor:
        """é¢„å¤„ç†æ‰€æœ‰æ•°æ®

        å°†æ–‡æœ¬è½½è·è½¬æ¢ä¸ºæ•°å€¼å¼ é‡çŸ©é˜µ X âˆˆ R^(N Ã— L_max)

        Returns:
            å¤„ç†åçš„æ•°æ®å¼ é‡
        """
        print(f"\nğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç† (å¢å¼ºç‰ˆ)...")

        if not self.payloads:
            raise ValueError("âŒ æ²¡æœ‰åŠ è½½ä»»ä½•æ•°æ®ï¼Œè¯·å…ˆè°ƒç”¨ load_data()")

        # é¦–å…ˆè¿›è¡Œå­—ç¬¦åˆ†å¸ƒåˆ†æ
        char_analysis = self.tokenizer.analyze_text(self.payloads)

        # ç¼–ç æ‰€æœ‰è½½è·
        encoded_data = []
        print(f"ğŸ”¤ æ­£åœ¨ç¼–ç  {len(self.payloads)} æ¡è½½è·...")

        # æ‰¹é‡å¤„ç†ä¼˜åŒ–
        batch_size = 1000
        for batch_start in range(0, len(self.payloads), batch_size):
            batch_end = min(batch_start + batch_size, len(self.payloads))
            batch_payloads = self.payloads[batch_start:batch_end]

            for i, payload in enumerate(batch_payloads):
                if (batch_start + i + 1) % 1000 == 0 or (batch_start + i) == 0:
                    print(f"   è¿›åº¦ï¼š{batch_start + i + 1}/{len(self.payloads)} ({(batch_start + i + 1)/len(self.payloads)*100:.1f}%)")

                encoded = self.tokenizer.encode(payload, self.max_length)
                encoded_data.append(encoded)

        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        print("ğŸ”¥ æ­£åœ¨è½¬æ¢ä¸ºPyTorchå¼ é‡...")
        data_tensor = torch.tensor(encoded_data, dtype=torch.long)

        print(f"âœ… é¢„å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºå¼ é‡å½¢çŠ¶ï¼š{data_tensor.shape}")
        print(f"ğŸ”¢ æ•°æ®ç±»å‹ï¼š{data_tensor.dtype}")

        return data_tensor

    def calculate_vocabulary_coverage(self) -> float:
        """è®¡ç®—è¯è¡¨è¦†ç›–ç‡

        Returns:
            è¯è¡¨è¦†ç›–ç‡ç™¾åˆ†æ¯”ï¼ˆ0-100ï¼‰
        """
        print(f"\nğŸ” è®¡ç®—è¯è¡¨è¦†ç›–ç‡ (å¢å¼ºç‰ˆ)...")

        # ä½¿ç”¨tokenizerçš„åˆ†æç»“æœ
        if not self.tokenizer.char_frequency:
            self.tokenizer.analyze_text(self.payloads)

        total_chars = sum(self.tokenizer.char_frequency.values())
        covered_chars = sum(freq for char, freq in self.tokenizer.char_frequency.items()
                          if char in self.tokenizer.char_to_idx)
        uncovered_chars = total_chars - covered_chars

        coverage = (covered_chars / total_chars * 100) if total_chars > 0 else 0

        print(f"ğŸ“Š è¯è¡¨è¦†ç›–ç‡åˆ†æç»“æœï¼š")
        print(f"   æ€»å­—ç¬¦æ•°ï¼š{total_chars}")
        print(f"   è¦†ç›–å­—ç¬¦æ•°ï¼š{covered_chars}")
        print(f"   æœªè¦†ç›–å­—ç¬¦æ•°ï¼š{uncovered_chars}")
        print(f"   è¦†ç›–ç‡ï¼š{coverage:.3f}%")

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡è¦†ç›–ç‡
        target_coverage = 99.9
        if coverage >= target_coverage:
            print(f"ğŸ¯ è¾¾æ ‡ï¼è¯è¡¨è¦†ç›–ç‡ {coverage:.3f}% >= {target_coverage}%")
        else:
            print(f"âš ï¸ æœªè¾¾æ ‡ï¼è¯è¡¨è¦†ç›–ç‡ {coverage:.3f}% < {target_coverage}%")
            print("ğŸ’¡ å»ºè®®ï¼š")
            if self.tokenizer.uncovered_chars:
                print("   1. å°†æœªè¦†ç›–å­—ç¬¦æ·»åŠ åˆ°æ‰©å±•å­—ç¬¦é›†")
                print("   2. å¢åŠ è¯è¡¨å¤§å°")
                print("   3. è€ƒè™‘ä½¿ç”¨å­—ç¬¦æ›¿æ¢ç­–ç•¥")

        return coverage

    def save_processed_data(self, output_dir: str, data_tensor: torch.Tensor) -> None:
        """ä¿å­˜å¤„ç†åçš„æ•°æ® (å¢å¼ºç‰ˆ)

        Args:
            output_dir: è¾“å‡ºç›®å½•
            data_tensor: å¤„ç†åçš„æ•°æ®å¼ é‡
        """
        print(f"\nğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ® (å¢å¼ºç‰ˆ)...")

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜æ•°æ®å¼ é‡
        data_file = output_path / "processed_data.pt"
        torch.save(data_tensor, data_file)
        print(f"âœ… æ•°æ®å¼ é‡å·²ä¿å­˜ï¼š{data_file}")

        # ä¿å­˜è¯è¡¨
        vocab_file = output_path / "vocab.json"
        vocab_data = {
            'char_to_idx': self.tokenizer.char_to_idx,
            'idx_to_char': {str(k): v for k, v in self.tokenizer.idx_to_char.items()},
            'special_tokens': self.tokenizer.special_tokens,
            'vocab_size': len(self.tokenizer.char_to_idx),
            'max_length': self.max_length,
            'stats': self.tokenizer.get_vocab_stats()
        }

        with open(vocab_file, 'w', encoding='utf-8') as f:
            json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… è¯è¡¨å·²ä¿å­˜ï¼š{vocab_file}")

        # ä¿å­˜æ ‡ç­¾æ˜ å°„
        label_file = output_path / "label_mapping.json"
        with open(label_file, 'w', encoding='utf-8') as f:
            json.dump(self.ATTACK_TYPES, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ ‡ç­¾æ˜ å°„å·²ä¿å­˜ï¼š{label_file}")

        # ä¿å­˜æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ (å¢å¼ºç‰ˆ)
        stats_file = output_path / "dataset_stats.json"
        stats = {
            'total_samples': len(self.payloads),
            'attack_distribution': self.attack_counts,
            'vocabulary_coverage': self.calculate_vocabulary_coverage(),
            'tensor_shape': list(data_tensor.shape),
            'max_length': self.max_length,
            'quality_stats': self.quality_stats,
            'char_analysis': {
                'total_chars': self.quality_stats['total_chars'],
                'uncovered_chars': len(self.tokenizer.uncovered_chars),
                'uncovered_list': list(self.tokenizer.uncovered_chars)[:20]  # åªä¿å­˜å‰20ä¸ª
            }
        }

        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜ï¼š{stats_file}")

        # ä¿å­˜æ ·æœ¬æ•°æ® (ç”¨äºè°ƒè¯•)
        samples_file = output_path / "sample_payloads.json"
        sample_data = []
        for i in range(min(20, len(self.payloads))):
            attack_type = list(self.ATTACK_TYPES.keys())[list(self.ATTACK_TYPES.values()).index(self.labels[i])]
            sample_data.append({
                'id': i,
                'type': attack_type,
                'label': self.labels[i],
                'payload': self.payloads[i],
                'encoded': data_tensor[i].tolist()[:20],  # åªä¿å­˜å‰20ä¸ªtoken
                'length': len(self.payloads[i])
            })

        with open(samples_file, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ ·æœ¬æ•°æ®å·²ä¿å­˜ï¼š{samples_file}")

        print(f"\nğŸ‰ æ‰€æœ‰æ•°æ®æ–‡ä»¶å·²ä¿å­˜åˆ°ï¼š{output_path}")


def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œè°ƒç”¨"""
    parser = argparse.ArgumentParser(
        description="CVDBFuzz æ•°æ®é¢„å¤„ç†å·¥å…· (å¢å¼ºç‰ˆ)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹ï¼š
  python main.py --preprocess
  python main.py --preprocess --data-dir ./custom_data --output-dir ./output --vocab-size 512
        """
    )

    parser.add_argument(
        '--preprocess',
        action='store_true',
        help='æ‰§è¡Œæ•°æ®é¢„å¤„ç†'
    )

    parser.add_argument(
        '--data-dir',
        type=str,
        default='Data/payload/train',
        help='è®­ç»ƒæ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: Data/payload/train)'
    )

    parser.add_argument(
        '--output-dir',
        type=str,
        default='Data/processed',
        help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: Data/processed)'
    )

    parser.add_argument(
        '--max-length',
        type=int,
        default=150,
        help='æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 150)'
    )

    parser.add_argument(
        '--vocab-size',
        type=int,
        default=256,
        help='è¯è¡¨å¤§å° (é»˜è®¤: 256)'
    )

    args = parser.parse_args()

    if not args.preprocess:
        parser.print_help()
        return

    print("=" * 80)
    print("ğŸ¯ CVDBFuzz æ•°æ®é¢„å¤„ç†å·¥å…· (å¢å¼ºç‰ˆ)")
    print("=" * 80)

    try:
        # åˆå§‹åŒ–æ•°æ®é›†å¤„ç†å™¨
        dataset = PayloadDataset(
            data_dir=args.data_dir,
            max_length=args.max_length,
            vocab_size=args.vocab_size
        )

        # åŠ è½½åŸå§‹æ•°æ®
        payloads, labels = dataset.load_data()

        # æ•°æ®é¢„å¤„ç†
        data_tensor = dataset.preprocess()

        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        dataset.save_processed_data(args.output_dir, data_tensor)

        print("\n" + "=" * 80)
        print("ğŸ‰ æ•°æ®é¢„å¤„ç†ä»»åŠ¡å®Œæˆï¼")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ é¢„å¤„ç†å¤±è´¥ï¼š{e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()